From fa2989cebdc8510b2a818f6d26517bcc1e2101d1 Mon Sep 17 00:00:00 2001
From: joohyun kyong <joohyun0115@gmail.com>
Date: Wed, 19 Oct 2016 11:45:08 +0900
Subject: [PATCH] LDU: enable global queue version of the LDU

---
 fs/exec.c                |   3 +-
 fs/hugetlbfs/inode.c     |   3 +
 fs/inode.c               |   7 ++
 include/linux/fs.h       |  17 +++
 include/linux/ldu.h      |  56 +++++++++
 include/linux/mm.h       |   1 +
 include/linux/mm_types.h |   4 +
 include/linux/rmap.h     |  17 +--
 kernel/events/uprobes.c  |   7 +-
 kernel/fork.c            |  17 ++-
 mm/huge_memory.c         |   2 +-
 mm/hugetlb.c             |   4 +
 mm/ksm.c                 |  10 +-
 mm/memory-failure.c      |   5 +-
 mm/memory.c              |   5 +-
 mm/migrate.c             |   9 +-
 mm/mlock.c               |   2 +-
 mm/mmap.c                | 228 ++++++++++++++++++++++++++++++------
 mm/mremap.c              |   3 +-
 mm/rmap.c                | 294 ++++++++++++++++++++++++++++++++++++-----------
 20 files changed, 564 insertions(+), 130 deletions(-)
 create mode 100644 include/linux/ldu.h

diff --git a/fs/exec.c b/fs/exec.c
index dcd4ac7..dd5a46a 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -277,6 +277,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 	vma->vm_flags = VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP;
 	vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	memset(&vma->dnode, 0, sizeof(vma->dnode));
 
 	err = insert_vm_struct(mm, vma);
 	if (err)
@@ -290,7 +291,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 err:
 	up_write(&mm->mmap_sem);
 	bprm->vma = NULL;
-	kmem_cache_free(vm_area_cachep, vma);
+	free_vma(vma);
 	return err;
 }
 
diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c
index e1f465a..24b7f40 100644
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@ -446,6 +446,7 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,
 				BUG_ON(truncate_op);
 
 				i_mmap_lock_write(mapping);
+				synchronize_ldu_i_mmap(mapping);
 				hugetlb_vmdelete_list(&mapping->i_mmap,
 					next * pages_per_huge_page(h),
 					(next + 1) * pages_per_huge_page(h));
@@ -507,6 +508,7 @@ static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)
 
 	i_size_write(inode, offset);
 	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	if (!RB_EMPTY_ROOT(&mapping->i_mmap))
 		hugetlb_vmdelete_list(&mapping->i_mmap, pgoff, 0);
 	i_mmap_unlock_write(mapping);
@@ -532,6 +534,7 @@ static long hugetlbfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 		inode_lock(inode);
 		i_mmap_lock_write(mapping);
+		synchronize_ldu_i_mmap(mapping);
 		if (!RB_EMPTY_ROOT(&mapping->i_mmap))
 			hugetlb_vmdelete_list(&mapping->i_mmap,
 						hole_start >> PAGE_SHIFT,
diff --git a/fs/inode.c b/fs/inode.c
index 69b8b52..1603781 100644
--- a/fs/inode.c
+++ b/fs/inode.c
@@ -256,6 +256,10 @@ static void i_callback(struct rcu_head *head)
 static void destroy_inode(struct inode *inode)
 {
 	BUG_ON(!list_empty(&inode->i_lru));
+	if (inode->i_mapping && !llist_empty(&inode->i_mapping->lduh.ll_head)) {
+		flush_delayed_work(&inode->i_mapping->lduh.sync);
+	}
+
 	__destroy_inode(inode);
 	if (inode->i_sb->s_op->destroy_inode)
 		inode->i_sb->s_op->destroy_inode(inode);
@@ -341,6 +345,8 @@ void inc_nlink(struct inode *inode)
 }
 EXPORT_SYMBOL(inc_nlink);
 
+extern void i_mmap_free_work_func(struct work_struct *w);
+
 void address_space_init_once(struct address_space *mapping)
 {
 	memset(mapping, 0, sizeof(*mapping));
@@ -350,6 +356,7 @@ void address_space_init_once(struct address_space *mapping)
 	INIT_LIST_HEAD(&mapping->private_list);
 	spin_lock_init(&mapping->private_lock);
 	mapping->i_mmap = RB_ROOT;
+	i_mmap_init_ldu_head(&mapping->lduh);
 }
 EXPORT_SYMBOL(address_space_init_once);
 
diff --git a/include/linux/fs.h b/include/linux/fs.h
index ae68100..9036d08 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -35,6 +35,7 @@
 
 #include <asm/byteorder.h>
 #include <uapi/linux/fs.h>
+#include <linux/ldu.h>
 
 struct backing_dev_info;
 struct bdi_writeback;
@@ -430,6 +431,8 @@ struct address_space {
 	spinlock_t		tree_lock;	/* and lock protecting it */
 	atomic_t		i_mmap_writable;/* count VM_SHARED mappings */
 	struct rb_root		i_mmap;		/* tree of private and shared mappings */
+	struct ldu_head  lduh;
+	struct llist_head llclean;
 	struct rw_semaphore	i_mmap_rwsem;	/* protect tree, count, list */
 	/* Protected by tree_lock together with the radix tree */
 	unsigned long		nrpages;	/* number of total pages */
@@ -516,11 +519,21 @@ static inline void i_mmap_unlock_read(struct address_space *mapping)
 	up_read(&mapping->i_mmap_rwsem);
 }
 
+void synchronize_ldu_i_mmap(struct address_space *mapping);
+bool ldu_logical_update(struct address_space *mapping, struct ldu_node *dnode);
+bool ldu_logical_insert(struct vm_area_struct *vma, struct address_space *mapping);
+bool ldu_logical_remove(struct vm_area_struct *vma, struct address_space *mapping);
+
+
 /*
  * Might pages of this file be mapped into userspace?
  */
 static inline int mapping_mapped(struct address_space *mapping)
 {
+	down_write(&mapping->i_mmap_rwsem);
+	synchronize_ldu_i_mmap(mapping);
+	up_write(&mapping->i_mmap_rwsem);
+
 	return	!RB_EMPTY_ROOT(&mapping->i_mmap);
 }
 
@@ -2533,6 +2546,10 @@ static inline void file_end_write(struct file *file)
  */
 static inline int get_write_access(struct inode *inode)
 {
+	down_write(&inode->i_mapping->i_mmap_rwsem);
+	synchronize_ldu_i_mmap(inode->i_mapping);
+	up_write(&inode->i_mapping->i_mmap_rwsem);
+
 	return atomic_inc_unless_negative(&inode->i_writecount) ? 0 : -ETXTBSY;
 }
 static inline int deny_write_access(struct file *file)
diff --git a/include/linux/ldu.h b/include/linux/ldu.h
new file mode 100644
index 0000000..2eea4e2
--- /dev/null
+++ b/include/linux/ldu.h
@@ -0,0 +1,56 @@
+#ifndef __LINUX_DEFERABLE_UPDATE
+#define __LINUX_DEFERABLE_UPDATE
+
+#include <linux/llist.h>
+#include <linux/mutex.h>
+
+#define LDU_LINKED_LIST     0
+#define LDU_INTERVAL_TREE   1
+
+#define LDU_OP_ADD 1
+#define LDU_OP_DEL 2
+
+struct ldu_head {
+	struct delayed_work sync;
+	struct llist_head ll_head;
+};
+
+struct ldu_node {
+	void *key;
+	int mark;
+	int op_num;
+	struct rb_root *root;
+	struct llist_node ll_node;
+};
+
+struct ldu_anon_node {
+	unsigned long used;
+	struct ldu_node node[2]; /* 0 : add op, 1 : del op */
+};
+
+struct ldu_i_mmap_node {
+	unsigned long used;
+	struct ldu_node node[2]; /* 0 : add op, 1 : del op */
+};
+
+
+void avc_free_work_func(struct work_struct *work);
+
+
+static inline void anon_vma_init_ldu_head(struct ldu_head *dp)
+{
+	init_llist_head(&dp->ll_head);
+	INIT_DELAYED_WORK(&dp->sync, avc_free_work_func);
+}
+
+void i_mmap_free_work_func(struct work_struct *work);
+
+void i_mmap_ldu_physical_update(int op, struct vm_area_struct *vma, struct rb_root *root);
+static inline void i_mmap_init_ldu_head(struct ldu_head *dp)
+{
+	init_llist_head(&dp->ll_head);
+	INIT_DELAYED_WORK(&dp->sync, i_mmap_free_work_func);
+}
+
+
+#endif /* __LINUX_DEFERABLE_UPDATE */
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 516e149..d743269 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -112,6 +112,7 @@ extern int overcommit_kbytes_handler(struct ctl_table *, int, void __user *,
  */
 
 extern struct kmem_cache *vm_area_cachep;
+extern void free_vma(struct vm_area_struct *vma);
 
 #ifndef CONFIG_MMU
 extern struct rb_root nommu_region_tree;
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 624b78b..fc88178 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -14,6 +14,7 @@
 #include <linux/page-flags-layout.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
+#include <linux/ldu.h>
 
 #ifndef AT_VECTOR_SIZE_ARCH
 #define AT_VECTOR_SIZE_ARCH 0
@@ -326,6 +327,9 @@ struct vm_area_struct {
 		unsigned long rb_subtree_last;
 	} shared;
 
+	struct ldu_i_mmap_node dnode;
+	struct llist_node llist;
+
 	/*
 	 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
 	 * list, after a COW of one of the file pages.	A MAP_SHARED vma
diff --git a/include/linux/rmap.h b/include/linux/rmap.h
index a07f42b..7404e2d 100644
--- a/include/linux/rmap.h
+++ b/include/linux/rmap.h
@@ -9,6 +9,7 @@
 #include <linux/mm.h>
 #include <linux/rwsem.h>
 #include <linux/memcontrol.h>
+#include <linux/ldu.h>
 
 /*
  * The anon_vma heads a list of private "related" vmas, to scan if
@@ -35,14 +36,7 @@ struct anon_vma {
 	 * anon_vma if they are the last user on release
 	 */
 	atomic_t refcount;
-
-	/*
-	 * Count of child anon_vmas and VMAs which points to this anon_vma.
-	 *
-	 * This counter is used for making decision about reusing anon_vma
-	 * instead of forking new one. See comments in function anon_vma_clone.
-	 */
-	unsigned degree;
+	atomic_t refcount_free;
 
 	struct anon_vma *parent;	/* Parent of this anon_vma */
 
@@ -55,6 +49,9 @@ struct anon_vma {
 	 * mm_take_all_locks() (mm_all_locks_mutex).
 	 */
 	struct rb_root rb_root;	/* Interval tree of private "related" vmas */
+	struct ldu_head  lduh;
+	struct llist_node llist;
+	struct llist_head llclean;
 };
 
 /*
@@ -75,6 +72,8 @@ struct anon_vma_chain {
 	struct anon_vma *anon_vma;
 	struct list_head same_vma;   /* locked by mmap_sem & page_table_lock */
 	struct rb_node rb;			/* locked by anon_vma->rwsem */
+	struct ldu_anon_node dnode;
+	struct llist_node llist;
 	unsigned long rb_subtree_last;
 #ifdef CONFIG_DEBUG_VM_RB
 	unsigned long cached_vma_start, cached_vma_last;
@@ -95,6 +94,8 @@ enum ttu_flags {
 					 * do a final flush if necessary */
 };
 
+void synchronize_ldu_anon(struct anon_vma *anon);
+
 #ifdef CONFIG_MMU
 static inline void get_anon_vma(struct anon_vma *anon_vma)
 {
diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c
index 0167679..1bc4ea4 100644
--- a/kernel/events/uprobes.c
+++ b/kernel/events/uprobes.c
@@ -37,6 +37,7 @@
 #include <linux/percpu-rwsem.h>
 #include <linux/task_work.h>
 #include <linux/shmem_fs.h>
+#include <linux/ldu.h>
 
 #include <linux/uprobes.h>
 
@@ -719,7 +720,9 @@ build_map_info(struct address_space *mapping, loff_t offset, bool is_register)
 	int more = 0;
 
  again:
-	i_mmap_lock_read(mapping);
+	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
+
 	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
 		if (!valid_vma(vma, is_register))
 			continue;
@@ -750,7 +753,7 @@ build_map_info(struct address_space *mapping, loff_t offset, bool is_register)
 		info->mm = vma->vm_mm;
 		info->vaddr = offset_to_vaddr(vma, offset);
 	}
-	i_mmap_unlock_read(mapping);
+	i_mmap_unlock_write(mapping);
 
 	if (!more)
 		goto out;
diff --git a/kernel/fork.c b/kernel/fork.c
index 2e391c7..b9bb53d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -394,6 +394,10 @@ free_tsk:
 }
 
 #ifdef CONFIG_MMU
+extern void free_vma(struct vm_area_struct *vma);
+bool i_mmap_ldu_logical_insert(struct vm_area_struct *vma,
+		struct address_space *mapping);
+
 static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 {
 	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
@@ -460,6 +464,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		tmp->vm_next = tmp->vm_prev = NULL;
 		tmp->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
 		file = tmp->vm_file;
+		memset(&tmp->dnode, 0, sizeof(tmp->dnode));
 		if (file) {
 			struct inode *inode = file_inode(file);
 			struct address_space *mapping = file->f_mapping;
@@ -467,15 +472,17 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			get_file(file);
 			if (tmp->vm_flags & VM_DENYWRITE)
 				atomic_dec(&inode->i_writecount);
-			i_mmap_lock_write(mapping);
+			//i_mmap_lock_write(mapping);
 			if (tmp->vm_flags & VM_SHARED)
 				atomic_inc(&mapping->i_mmap_writable);
 			flush_dcache_mmap_lock(mapping);
 			/* insert tmp into the share list, just after mpnt */
-			vma_interval_tree_insert_after(tmp, mpnt,
-					&mapping->i_mmap);
+			//vma_interval_tree_insert_after(tmp, mpnt,
+			//      &mapping->i_mmap);
+			i_mmap_ldu_logical_insert(tmp, mapping);
+
 			flush_dcache_mmap_unlock(mapping);
-			i_mmap_unlock_write(mapping);
+			//i_mmap_unlock_write(mapping);
 		}
 
 		/*
@@ -519,7 +526,7 @@ out:
 fail_nomem_anon_vma_fork:
 	mpol_put(vma_policy(tmp));
 fail_nomem_policy:
-	kmem_cache_free(vm_area_cachep, tmp);
+	free_vma(tmp);
 fail_nomem:
 	retval = -ENOMEM;
 	vm_unacct_memory(charge);
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index e10a4fe..db1f7f5 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -3126,7 +3126,6 @@ static void freeze_page(struct anon_vma *anon_vma, struct page *page)
 	pgoff_t pgoff = page_to_pgoff(page);
 
 	VM_BUG_ON_PAGE(!PageHead(page), page);
-
 	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff,
 			pgoff + HPAGE_PMD_NR - 1) {
 		unsigned long address = __vma_address(page, avc->vma);
@@ -3386,6 +3385,7 @@ int split_huge_page_to_list(struct page *page, struct list_head *list)
 		goto out;
 	}
 	anon_vma_lock_write(anon_vma);
+	synchronize_ldu_anon(anon_vma->root);
 
 	/*
 	 * Racy check if we can split the page, before freeze_page() will
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 01f2b48..d49c77c 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -23,6 +23,7 @@
 #include <linux/swapops.h>
 #include <linux/page-isolation.h>
 #include <linux/jhash.h>
+#include <linux/ldu.h>
 
 #include <asm/page.h>
 #include <asm/pgtable.h>
@@ -3280,6 +3281,7 @@ static void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,
 	 * __unmap_hugepage_range() is called as the lock is already held
 	 */
 	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	vma_interval_tree_foreach(iter_vma, &mapping->i_mmap, pgoff, pgoff) {
 		/* Do not unmap the current VMA */
 		if (iter_vma == vma)
@@ -3909,6 +3911,7 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 
 	mmu_notifier_invalidate_range_start(mm, start, end);
 	i_mmap_lock_write(vma->vm_file->f_mapping);
+	//synchronize_ldu_i_mmap(vma->vm_file->f_mapping);
 	for (; address < end; address += huge_page_size(h)) {
 		spinlock_t *ptl;
 		ptep = huge_pte_offset(mm, address);
@@ -4167,6 +4170,7 @@ pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)
 		return (pte_t *)pmd_alloc(mm, pud, addr);
 
 	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	vma_interval_tree_foreach(svma, &mapping->i_mmap, idx, idx) {
 		if (svma == vma)
 			continue;
diff --git a/mm/ksm.c b/mm/ksm.c
index ca6d2a0..e59e5e4 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -37,6 +37,7 @@
 #include <linux/freezer.h>
 #include <linux/oom.h>
 #include <linux/numa.h>
+#include <linux/ldu.h>
 
 #include <asm/tlbflush.h>
 #include "internal.h"
@@ -1899,7 +1900,8 @@ again:
 		struct vm_area_struct *vma;
 
 		cond_resched();
-		anon_vma_lock_read(anon_vma);
+		anon_vma_lock_write(anon_vma);
+		synchronize_ldu_anon(anon_vma->root);
 		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
 					       0, ULONG_MAX) {
 			cond_resched();
@@ -1922,15 +1924,15 @@ again:
 			ret = rwc->rmap_one(page, vma,
 					rmap_item->address, rwc->arg);
 			if (ret != SWAP_AGAIN) {
-				anon_vma_unlock_read(anon_vma);
+				anon_vma_unlock_write(anon_vma);
 				goto out;
 			}
 			if (rwc->done && rwc->done(page)) {
-				anon_vma_unlock_read(anon_vma);
+				anon_vma_unlock_write(anon_vma);
 				goto out;
 			}
 		}
-		anon_vma_unlock_read(anon_vma);
+		anon_vma_unlock_write(anon_vma);
 	}
 	if (!search_new_forks++)
 		goto again;
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index ac595e7..fb5b005 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -445,8 +445,9 @@ static void collect_procs_file(struct page *page, struct list_head *to_kill,
 	struct task_struct *tsk;
 	struct address_space *mapping = page->mapping;
 
-	i_mmap_lock_read(mapping);
+	i_mmap_lock_write(mapping);
 	read_lock(&tasklist_lock);
+	synchronize_ldu_i_mmap(mapping);
 	for_each_process(tsk) {
 		pgoff_t pgoff = page_to_pgoff(page);
 		struct task_struct *t = task_early_kill(tsk, force_early);
@@ -467,7 +468,7 @@ static void collect_procs_file(struct page *page, struct list_head *to_kill,
 		}
 	}
 	read_unlock(&tasklist_lock);
-	i_mmap_unlock_read(mapping);
+	i_mmap_unlock_write(mapping);
 }
 
 /*
diff --git a/mm/memory.c b/mm/memory.c
index 8132787..6003ddf 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1288,6 +1288,7 @@ static void unmap_single_vma(struct mmu_gather *tlb,
 			 */
 			if (vma->vm_file) {
 				i_mmap_lock_write(vma->vm_file->f_mapping);
+				//synchronize_ldu_i_mmap(vma->vm_file->f_mapping);
 				__unmap_hugepage_range_final(tlb, vma, start, end, NULL);
 				i_mmap_unlock_write(vma->vm_file->f_mapping);
 			}
@@ -1321,8 +1322,9 @@ void unmap_vmas(struct mmu_gather *tlb,
 	struct mm_struct *mm = vma->vm_mm;
 
 	mmu_notifier_invalidate_range_start(mm, start_addr, end_addr);
-	for ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next)
+	for ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next) {
 		unmap_single_vma(tlb, vma, start_addr, end_addr, NULL);
+	}
 	mmu_notifier_invalidate_range_end(mm, start_addr, end_addr);
 }
 
@@ -2433,6 +2435,7 @@ void unmap_mapping_range(struct address_space *mapping,
 
 	/* DAX uses i_mmap_lock to serialise file truncate vs page fault */
 	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	if (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap)))
 		unmap_mapping_range_tree(&mapping->i_mmap, &details);
 	i_mmap_unlock_write(mapping);
diff --git a/mm/migrate.c b/mm/migrate.c
index 3ad0fea..867a8ba 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -839,8 +839,9 @@ static int __unmap_and_move(struct page *page, struct page *newpage,
 	 * because that implies that the anon page is no longer mapped
 	 * (and cannot be remapped so long as we hold the page lock).
 	 */
-	if (PageAnon(page) && !PageKsm(page))
+	if (PageAnon(page) && !PageKsm(page)) {
 		anon_vma = page_get_anon_vma(page);
+	}
 
 	/*
 	 * Block others from accessing the new page when we get around to
@@ -889,6 +890,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,
 				page);
 		try_to_unmap(page,
 			TTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);
+
 		page_was_mapped = 1;
 	}
 
@@ -1048,8 +1050,9 @@ static int unmap_and_move_huge_page(new_page_t get_new_page,
 		lock_page(hpage);
 	}
 
-	if (PageAnon(hpage))
+	if (PageAnon(hpage)) {
 		anon_vma = page_get_anon_vma(hpage);
+	}
 
 	if (unlikely(!trylock_page(new_hpage)))
 		goto put_anon;
@@ -1684,6 +1687,8 @@ int migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,
 	int nr_remaining;
 	LIST_HEAD(migratepages);
 
+	if (PageAnon(page))
+		goto out;
 	/*
 	 * Don't migrate file pages that are mapped in multiple processes
 	 * with execute permissions as they are probably shared libraries.
diff --git a/mm/mlock.c b/mm/mlock.c
index 96f0010..184c655 100644
--- a/mm/mlock.c
+++ b/mm/mlock.c
@@ -128,7 +128,7 @@ static void __munlock_isolated_page(struct page *page)
 	 * Optimization: if the page was mapped just once, that's our mapping
 	 * and we don't need to check all the other vmas.
 	 */
-	if (page_mapcount(page) > 1)
+	if (page_mapcount(page) > 1 && (PageAnon(page) || PageKsm(page)))
 		ret = try_to_munlock(page);
 
 	/* Did try_to_unlock() succeed or punt? */
diff --git a/mm/mmap.c b/mm/mmap.c
index 76d1ec2..035a212 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -41,6 +41,7 @@
 #include <linux/notifier.h>
 #include <linux/memory.h>
 #include <linux/printk.h>
+#include <linux/kthread.h>
 #include <linux/userfaultfd_k.h>
 #include <linux/moduleparam.h>
 
@@ -77,6 +78,136 @@ static void unmap_region(struct mm_struct *mm,
 		struct vm_area_struct *vma, struct vm_area_struct *prev,
 		unsigned long start, unsigned long end);
 
+#define LDU_UPDATE_RATE 1 /* 1 sec */
+LLIST_HEAD(vma_cleanup_list);
+static struct workqueue_struct *i_mmap_wq;
+
+void i_mmap_free_work_func(struct work_struct *work);
+
+bool i_mmap_ldu_logical_update(struct address_space *mapping,
+		struct ldu_node *dnode)
+{
+	if (llist_add(&dnode->ll_node, &mapping->lduh.ll_head)) {
+		queue_delayed_work(i_mmap_wq, &mapping->lduh.sync,
+				round_jiffies_relative(HZ / 10));
+	}
+	return true;
+}
+
+bool i_mmap_ldu_logical_insert(struct vm_area_struct *obj,
+		struct address_space *head)
+{
+	struct ldu_node *add_dnode = &obj->dnode.node[0];
+	struct ldu_node *del_dnode = &obj->dnode.node[1];
+
+	/* Phase 1 : update-side removing logs */
+	if (!xchg(&del_dnode->mark, 0)) {
+		BUG_ON(add_dnode->mark);
+		WRITE_ONCE(add_dnode->mark, 1);
+		/* Phase 2 : reusing garbage log */
+		if (!test_and_set_bit(LDU_OP_ADD, &obj->dnode.used)) {
+			add_dnode->op_num = LDU_OP_ADD;
+			add_dnode->key = obj;
+			add_dnode->root = &head->i_mmap;
+			/* Phase 3(slow-path): insert log to queue */
+			i_mmap_ldu_logical_update(head, add_dnode);
+		}
+	}
+
+	return true;
+}
+
+bool i_mmap_ldu_logical_remove(struct vm_area_struct *obj,
+		struct address_space *head)
+{
+	struct ldu_node *add_dnode = &obj->dnode.node[0];
+	struct ldu_node *del_dnode = &obj->dnode.node[1];
+
+	/* Phase 1 : update-side removing logs */
+	if (!xchg(&add_dnode->mark, 0)) {
+		BUG_ON(del_dnode->mark);
+		WRITE_ONCE(del_dnode->mark, 1);
+		/* Phase 2 : reusing garbage log */
+		if (!test_and_set_bit(LDU_OP_DEL, &obj->dnode.used)) {
+			del_dnode->op_num = LDU_OP_DEL;
+			del_dnode->key = obj;
+			del_dnode->root = &head->i_mmap;
+			/* Phase 3(slow-path): insert log to queue */
+			i_mmap_ldu_logical_update(head, del_dnode);
+		}
+	}
+
+	return true;
+}
+
+void i_mmap_ldu_physical_update(int op, struct vm_area_struct *vma,
+		struct rb_root *root)
+{
+	if (op == LDU_OP_ADD)
+		vma_interval_tree_insert(vma, root);
+	else
+		vma_interval_tree_remove(vma, root);
+}
+
+void synchronize_ldu_i_mmap(struct address_space *mapping)
+{
+	struct llist_node *entry;
+	struct ldu_node *dnode, *next;
+	struct ldu_head *lduh = &mapping->lduh;
+
+	entry = llist_del_all(&lduh->ll_head);
+	entry = llist_reverse_order(entry);
+	/* iteration all logs */
+	llist_for_each_entry_safe(dnode, next, entry, ll_node) {
+		struct vm_area_struct *vma = ACCESS_ONCE(dnode->key);
+		/* atomic swap due to update-side removing */
+		if (xchg(&dnode->mark, 0)) {
+			i_mmap_ldu_physical_update(dnode->op_num, vma,
+					ACCESS_ONCE(dnode->root));
+		}
+		clear_bit(dnode->op_num, &vma->dnode.used);
+		/* once again check due to reusing garbage logs */
+		if (xchg(&dnode->mark, 0)) {
+			i_mmap_ldu_physical_update(dnode->op_num, vma,
+					ACCESS_ONCE(dnode->root));
+		}
+	}
+}
+EXPORT_SYMBOL_GPL(synchronize_ldu_i_mmap);
+
+void free_vma(struct vm_area_struct *vma)
+{
+	struct file *file = vma->vm_file;
+
+	if (file && READ_ONCE(vma->dnode.used)) {
+		struct address_space *mapping = file->f_mapping;
+		llist_add(&vma->llist, &mapping->llclean);
+		return;
+	}
+	kmem_cache_free(vm_area_cachep, vma);
+}
+
+/* i_mmap_sync_and_free_work */
+void i_mmap_free_work_func(struct work_struct *work)
+{
+	struct address_space *mapping = container_of(work, struct address_space,
+										lduh.sync.work);
+	struct llist_node *entry;
+	struct vm_area_struct *vnode, *vnext;
+
+	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
+	i_mmap_unlock_write(mapping);
+
+	entry = llist_del_all(&mapping->llclean);
+	llist_for_each_entry_safe(vnode, vnext, entry, llist) {
+		if (!vnode->dnode.used)
+			kmem_cache_free(vm_area_cachep, vnode);
+		else
+			llist_add(&vnode->llist, &mapping->llclean);
+	}
+}
+
 /* description of effects of mapping type and prot in current implementation.
  * this is due to the limited x86 page protection hardware.  The expected
  * behavior is in parens:
@@ -253,14 +384,15 @@ error:
 static void __remove_shared_vm_struct(struct vm_area_struct *vma,
 		struct file *file, struct address_space *mapping)
 {
+	flush_dcache_mmap_lock(mapping);
+	i_mmap_ldu_logical_remove(vma, mapping);
+	//vma_interval_tree_remove(vma, &mapping->i_mmap);
+	flush_dcache_mmap_unlock(mapping);
+
 	if (vma->vm_flags & VM_DENYWRITE)
 		atomic_inc(&file_inode(file)->i_writecount);
 	if (vma->vm_flags & VM_SHARED)
 		mapping_unmap_writable(mapping);
-
-	flush_dcache_mmap_lock(mapping);
-	vma_interval_tree_remove(vma, &mapping->i_mmap);
-	flush_dcache_mmap_unlock(mapping);
 }
 
 /*
@@ -273,9 +405,9 @@ void unlink_file_vma(struct vm_area_struct *vma)
 
 	if (file) {
 		struct address_space *mapping = file->f_mapping;
-		i_mmap_lock_write(mapping);
+		//i_mmap_lock_write(mapping);
 		__remove_shared_vm_struct(vma, file, mapping);
-		i_mmap_unlock_write(mapping);
+		//i_mmap_unlock_write(mapping);
 	}
 }
 
@@ -292,7 +424,7 @@ static struct vm_area_struct *remove_vma(struct vm_area_struct *vma)
 	if (vma->vm_file)
 		fput(vma->vm_file);
 	mpol_put(vma_policy(vma));
-	kmem_cache_free(vm_area_cachep, vma);
+	free_vma(vma);
 	return next;
 }
 
@@ -463,10 +595,11 @@ static void validate_mm(struct mm_struct *mm)
 		struct anon_vma_chain *avc;
 
 		if (anon_vma) {
-			anon_vma_lock_read(anon_vma);
+			anon_vma_lock_write(anon_vma);
+			synchronize_ldu_anon(anon_vma);
 			list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
 				anon_vma_interval_tree_verify(avc);
-			anon_vma_unlock_read(anon_vma);
+			anon_vma_unlock_write(anon_vma);
 		}
 
 		highest_address = vma->vm_end;
@@ -537,6 +670,10 @@ static void vma_rb_erase(struct vm_area_struct *vma, struct rb_root *root)
 	rb_erase_augmented(&vma->vm_rb, root, &vma_gap_callbacks);
 }
 
+
+#include <linux/ldu.h>
+bool anon_vma_ldu_logical_insert(struct anon_vma_chain *avc, struct anon_vma *anon);
+bool anon_vma_ldu_logical_remove(struct anon_vma_chain *avc, struct anon_vma *anon);
 /*
  * vma has some anon_vma assigned, and is already inserted on that
  * anon_vma's interval trees.
@@ -557,7 +694,8 @@ anon_vma_interval_tree_pre_update_vma(struct vm_area_struct *vma)
 	struct anon_vma_chain *avc;
 
 	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
-		anon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);
+		anon_vma_ldu_logical_remove(avc, avc->anon_vma);
+//		anon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);
 }
 
 static inline void
@@ -566,7 +704,8 @@ anon_vma_interval_tree_post_update_vma(struct vm_area_struct *vma)
 	struct anon_vma_chain *avc;
 
 	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
-		anon_vma_interval_tree_insert(avc, &avc->anon_vma->rb_root);
+		anon_vma_ldu_logical_insert(avc, avc->anon_vma);
+//		anon_vma_interval_tree_insert(avc, &avc->anon_vma->rb_root);
 }
 
 static int find_vma_links(struct mm_struct *mm, unsigned long addr,
@@ -663,14 +802,16 @@ static void __vma_link_file(struct vm_area_struct *vma)
 	if (file) {
 		struct address_space *mapping = file->f_mapping;
 
+		flush_dcache_mmap_lock(mapping);
+		i_mmap_ldu_logical_insert(vma, mapping);
+		//vma_interval_tree_insert(vma, &mapping->i_mmap);
+		flush_dcache_mmap_unlock(mapping);
+
 		if (vma->vm_flags & VM_DENYWRITE)
 			atomic_dec(&file_inode(file)->i_writecount);
 		if (vma->vm_flags & VM_SHARED)
 			atomic_inc(&mapping->i_mmap_writable);
 
-		flush_dcache_mmap_lock(mapping);
-		vma_interval_tree_insert(vma, &mapping->i_mmap);
-		flush_dcache_mmap_unlock(mapping);
 	}
 }
 
@@ -689,16 +830,13 @@ static void vma_link(struct mm_struct *mm, struct vm_area_struct *vma,
 {
 	struct address_space *mapping = NULL;
 
-	if (vma->vm_file) {
+	if (vma->vm_file)
 		mapping = vma->vm_file->f_mapping;
-		i_mmap_lock_write(mapping);
-	}
 
 	__vma_link(mm, vma, prev, rb_link, rb_parent);
-	__vma_link_file(vma);
 
 	if (mapping)
-		i_mmap_unlock_write(mapping);
+		__vma_link_file(vma);
 
 	mm->map_count++;
 	validate_mm(mm);
@@ -735,6 +873,7 @@ __vma_unlink(struct mm_struct *mm, struct vm_area_struct *vma,
 	vmacache_invalidate(mm);
 }
 
+
 /*
  * We cannot adjust vm_start, vm_end, vm_pgoff fields of a vma that
  * is already present in an i_mmap tree without adjusting the tree.
@@ -810,7 +949,6 @@ again:			remove_next = 1 + (end > next->vm_end);
 		if (adjust_next)
 			uprobe_munmap(next, next->vm_start, next->vm_end);
 
-		i_mmap_lock_write(mapping);
 		if (insert) {
 			/*
 			 * Put into interval tree now, so instantiated pages
@@ -831,16 +969,20 @@ again:			remove_next = 1 + (end > next->vm_end);
 		VM_BUG_ON_VMA(adjust_next && next->anon_vma &&
 			  anon_vma != next->anon_vma, next);
 		anon_vma_lock_write(anon_vma);
+#if 1
 		anon_vma_interval_tree_pre_update_vma(vma);
 		if (adjust_next)
 			anon_vma_interval_tree_pre_update_vma(next);
+#endif
 	}
 
 	if (root) {
 		flush_dcache_mmap_lock(mapping);
-		vma_interval_tree_remove(vma, root);
+		i_mmap_ldu_logical_remove(vma, mapping);
+		//vma_interval_tree_remove(vma, root);
 		if (adjust_next)
-			vma_interval_tree_remove(next, root);
+			i_mmap_ldu_logical_remove(next, mapping);
+			//vma_interval_tree_remove(next, root);
 	}
 
 	if (start != vma->vm_start) {
@@ -859,8 +1001,10 @@ again:			remove_next = 1 + (end > next->vm_end);
 
 	if (root) {
 		if (adjust_next)
-			vma_interval_tree_insert(next, root);
-		vma_interval_tree_insert(vma, root);
+			i_mmap_ldu_logical_insert(next, mapping);
+			//vma_interval_tree_insert(next, root);
+		i_mmap_ldu_logical_insert(vma, mapping);
+		//vma_interval_tree_insert(vma, root);
 		flush_dcache_mmap_unlock(mapping);
 	}
 
@@ -891,13 +1035,13 @@ again:			remove_next = 1 + (end > next->vm_end);
 	}
 
 	if (anon_vma) {
+#if 1
 		anon_vma_interval_tree_post_update_vma(vma);
 		if (adjust_next)
 			anon_vma_interval_tree_post_update_vma(next);
+#endif
 		anon_vma_unlock_write(anon_vma);
 	}
-	if (mapping)
-		i_mmap_unlock_write(mapping);
 
 	if (root) {
 		uprobe_mmap(vma);
@@ -915,7 +1059,7 @@ again:			remove_next = 1 + (end > next->vm_end);
 			anon_vma_merge(vma, next);
 		mm->map_count--;
 		mpol_put(vma_policy(next));
-		kmem_cache_free(vm_area_cachep, next);
+		free_vma(next);
 		/*
 		 * In mprotect's case 6 (see comments on vma_merge),
 		 * we must remove another next too. It would clutter
@@ -1605,6 +1749,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 	vma->vm_page_prot = vm_get_page_prot(vm_flags);
 	vma->vm_pgoff = pgoff;
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	memset(&vma->dnode, 0, sizeof(vma->dnode));
 
 	if (file) {
 		if (vm_flags & VM_DENYWRITE) {
@@ -1695,7 +1840,7 @@ allow_write_and_free_vma:
 	if (vm_flags & VM_DENYWRITE)
 		allow_write_access(file);
 free_vma:
-	kmem_cache_free(vm_area_cachep, vma);
+	free_vma(vma);
 unacct_error:
 	if (charged)
 		vm_unacct_memory(charged);
@@ -2170,6 +2315,7 @@ int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 	 * anon_vma lock to serialize against concurrent expand_stacks.
 	 */
 	anon_vma_lock_write(vma->anon_vma);
+	//spin_lock(&mm->page_table_lock);
 
 	/* Somebody else might have raced and expanded it already */
 	if (address > vma->vm_end) {
@@ -2211,6 +2357,7 @@ int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 		}
 	}
 	anon_vma_unlock_write(vma->anon_vma);
+	//spin_unlock(&mm->page_table_lock);
 	khugepaged_enter_vma_merge(vma, vma->vm_flags);
 	validate_mm(mm);
 	return error;
@@ -2454,7 +2601,7 @@ static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 					~(huge_page_mask(hstate_vma(vma)))))
 		return -EINVAL;
 
-	new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+	new = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 	if (!new)
 		return -ENOMEM;
 
@@ -2462,6 +2609,7 @@ static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 	*new = *vma;
 
 	INIT_LIST_HEAD(&new->anon_vma_chain);
+	memset(&new->dnode, 0, sizeof(new->dnode));
 
 	if (new_below)
 		new->vm_end = addr;
@@ -2503,7 +2651,7 @@ static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
  out_free_mpol:
 	mpol_put(vma_policy(new));
  out_free_vma:
-	kmem_cache_free(vm_area_cachep, new);
+	free_vma(new);
 	return err;
 }
 
@@ -2800,6 +2948,7 @@ static unsigned long do_brk(unsigned long addr, unsigned long len)
 	}
 
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	memset(&vma->dnode, 0, sizeof(vma->dnode));
 	vma->vm_mm = mm;
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
@@ -2969,7 +3118,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 		}
 		*need_rmap_locks = (new_vma->vm_pgoff <= vma->vm_pgoff);
 	} else {
-		new_vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+		new_vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 		if (!new_vma)
 			goto out;
 		*new_vma = *vma;
@@ -2979,6 +3128,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 		if (vma_dup_policy(vma, new_vma))
 			goto out_free_vma;
 		INIT_LIST_HEAD(&new_vma->anon_vma_chain);
+		memset(&new_vma->dnode, 0, sizeof(new_vma->dnode));
 		if (anon_vma_clone(new_vma, vma))
 			goto out_free_mempol;
 		if (new_vma->vm_file)
@@ -2993,7 +3143,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 out_free_mempol:
 	mpol_put(vma_policy(new_vma));
 out_free_vma:
-	kmem_cache_free(vm_area_cachep, new_vma);
+	free_vma(new_vma);
 out:
 	return NULL;
 }
@@ -3099,6 +3249,7 @@ static struct vm_area_struct *__install_special_mapping(
 		return ERR_PTR(-ENOMEM);
 
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	memset(&vma->dnode, 0, sizeof(vma->dnode));
 	vma->vm_mm = mm;
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
@@ -3120,7 +3271,7 @@ static struct vm_area_struct *__install_special_mapping(
 	return vma;
 
 out:
-	kmem_cache_free(vm_area_cachep, vma);
+	free_vma(vma);
 	return ERR_PTR(ret);
 }
 
@@ -3454,3 +3605,14 @@ static int __meminit init_reserve_notifier(void)
 	return 0;
 }
 subsys_initcall(init_reserve_notifier);
+
+static int __init i_mmap_init_wq(void)
+{
+	i_mmap_wq = create_singlethread_workqueue("i_mmap");
+	WARN(!i_mmap_wq, "failed to create i_mmap workqueue\n");
+
+	pr_info("i_mmap work queue initialize");
+
+	return 0;
+}
+__initcall(i_mmap_init_wq);
diff --git a/mm/mremap.c b/mm/mremap.c
index 8eeba02..cc27f84 100644
--- a/mm/mremap.c
+++ b/mm/mremap.c
@@ -197,8 +197,9 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
 				VM_BUG_ON_VMA(vma->vm_file || !vma->anon_vma,
 					      vma);
 				/* See comment in move_ptes() */
-				if (need_rmap_locks)
+				if (need_rmap_locks) {
 					anon_vma_lock_write(vma->anon_vma);
+				}
 				moved = move_huge_pmd(vma, new_vma, old_addr,
 						    new_addr, old_end,
 						    old_pmd, new_pmd);
diff --git a/mm/rmap.c b/mm/rmap.c
index 79f3bf0..4c4652e 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -61,6 +61,8 @@
 #include <linux/hugetlb.h>
 #include <linux/backing-dev.h>
 #include <linux/page_idle.h>
+#include <linux/ldu.h>
+#include <linux/kthread.h>
 
 #include <asm/tlbflush.h>
 
@@ -68,9 +70,12 @@
 
 #include "internal.h"
 
+
 static struct kmem_cache *anon_vma_cachep;
 static struct kmem_cache *anon_vma_chain_cachep;
 
+static struct workqueue_struct *avc_wq;
+
 static inline struct anon_vma *anon_vma_alloc(void)
 {
 	struct anon_vma *anon_vma;
@@ -78,13 +83,16 @@ static inline struct anon_vma *anon_vma_alloc(void)
 	anon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);
 	if (anon_vma) {
 		atomic_set(&anon_vma->refcount, 1);
-		anon_vma->degree = 1;	/* Reference for first vma */
+		atomic_set(&anon_vma->refcount_free, 0);
+
 		anon_vma->parent = anon_vma;
 		/*
 		 * Initialise the anon_vma root to point to itself. If called
 		 * from fork, the root will be reset to the parents anon_vma.
 		 */
 		anon_vma->root = anon_vma;
+		init_llist_head(&anon_vma->llclean);
+		anon_vma_init_ldu_head(&anon_vma->lduh);
 	}
 
 	return anon_vma;
@@ -116,18 +124,175 @@ static inline void anon_vma_free(struct anon_vma *anon_vma)
 		anon_vma_lock_write(anon_vma);
 		anon_vma_unlock_write(anon_vma);
 	}
-
 	kmem_cache_free(anon_vma_cachep, anon_vma);
 }
 
 static inline struct anon_vma_chain *anon_vma_chain_alloc(gfp_t gfp)
 {
-	return kmem_cache_alloc(anon_vma_chain_cachep, gfp);
+	return kmem_cache_zalloc(anon_vma_chain_cachep, gfp);
 }
 
 static void anon_vma_chain_free(struct anon_vma_chain *anon_vma_chain)
 {
+	struct anon_vma *anon_vma = anon_vma_chain->anon_vma;
+
+	if (READ_ONCE(anon_vma_chain->dnode.used)) {
+		llist_add(&anon_vma_chain->llist, &anon_vma->root->llclean);
+		return;
+	}
+
 	kmem_cache_free(anon_vma_chain_cachep, anon_vma_chain);
+	if (anon_vma)
+		atomic_dec(&anon_vma->refcount_free);
+}
+
+bool anon_vma_ldu_logical_update(struct anon_vma *anon, struct ldu_node *dnode)
+{
+	BUG_ON(!anon);
+	if (llist_add(&dnode->ll_node, &anon->root->lduh.ll_head)) {
+		queue_delayed_work(avc_wq, &anon->root->lduh.sync,
+				round_jiffies_relative(HZ / 10));
+	}
+	return true;
+}
+
+bool anon_vma_ldu_logical_insert(struct anon_vma_chain *obj, struct anon_vma *head)
+{
+	struct ldu_node *add_dnode = &obj->dnode.node[0];
+	struct ldu_node *del_dnode = &obj->dnode.node[1];
+
+	BUG_ON(!head);
+	BUG_ON(!head->root);
+
+	/* Phase 1 : update-side removing logs */
+	if (!xchg(&del_dnode->mark, 0)) {
+		BUG_ON(add_dnode->mark);
+		WRITE_ONCE(add_dnode->mark, 1);
+		/* Phase 2 : reusing garbage log */
+		if (!test_and_set_bit(LDU_OP_ADD, &obj->dnode.used)) {
+			add_dnode->op_num = LDU_OP_ADD;
+			add_dnode->key = obj;
+			add_dnode->root = &head->rb_root;
+			/* Phase 3(slow-path): insert log to queue */
+			anon_vma_ldu_logical_update(head, add_dnode);
+		}
+	}
+
+	return true;
+}
+
+bool anon_vma_ldu_logical_remove(struct anon_vma_chain *obj, struct anon_vma *head)
+{
+	struct ldu_node *add_dnode = &obj->dnode.node[0];
+	struct ldu_node *del_dnode = &obj->dnode.node[1];
+
+	BUG_ON(!head);
+	BUG_ON(!head->root);
+	/* Phase 1 : update-side removing logs */
+	if (!xchg(&add_dnode->mark, 0)) {
+		WRITE_ONCE(del_dnode->mark, 1);
+		/* Phase 2 : reusing garbage log */
+		if (!test_and_set_bit(LDU_OP_DEL, &obj->dnode.used)) {
+			del_dnode->op_num = LDU_OP_DEL;
+			del_dnode->key = obj;
+			del_dnode->root = &head->rb_root;
+			/* Phase 3(slow-path): insert log to queue */
+			anon_vma_ldu_logical_update(head, del_dnode);
+		}
+	}
+
+	return true;
+}
+
+void anon_vma_ldu_physical_update(int op, struct anon_vma_chain *avc,
+		struct rb_root *root)
+{
+	BUG_ON(!root);
+	BUG_ON(!avc);
+	if (op == LDU_OP_ADD)
+		anon_vma_interval_tree_insert(avc, root);
+	else {
+		anon_vma_interval_tree_remove(avc, root);
+	}
+}
+
+void synchronize_ldu_anon(struct anon_vma *anon)
+{
+	struct llist_node *entry;
+	struct ldu_node *dnode, *next;
+	struct ldu_head *lduh = &anon->lduh;
+
+	entry = llist_del_all(&lduh->ll_head);
+	entry = llist_reverse_order(entry);
+	/* iteration all logs */
+	llist_for_each_entry_safe(dnode, next, entry, ll_node) {
+		struct anon_vma_chain *avc = READ_ONCE(dnode->key);
+		/* atomic swap due to update-side removing */
+		if (xchg(&dnode->mark, 0)) {
+			anon_vma_ldu_physical_update(dnode->op_num, avc,
+					READ_ONCE(dnode->root));
+		}
+		clear_bit(dnode->op_num, &avc->dnode.used);
+		/* once again check due to reusing garbage logs */
+		if (xchg(&dnode->mark, 0)) {
+			anon_vma_ldu_physical_update(dnode->op_num, avc,
+					READ_ONCE(dnode->root));
+		}
+	}
+}
+EXPORT_SYMBOL_GPL(synchronize_ldu_anon);
+
+/* avc_sync_and_free_work */
+void avc_free_work_func(struct work_struct *work)
+{
+	struct anon_vma *anon_vma = container_of(work, struct anon_vma,
+			lduh.sync.work);
+	struct llist_node *entry;
+	struct anon_vma_chain *anode, *anext;
+
+	down_write(&anon_vma->rwsem);
+	synchronize_ldu_anon(anon_vma);
+	up_write(&anon_vma->rwsem);
+
+	entry = llist_del_all(&anon_vma->llclean);
+	llist_for_each_entry_safe(anode, anext, entry, llist) {
+		if (!READ_ONCE(anode->dnode.used)) {
+			struct anon_vma *anon = anode->anon_vma;
+			kmem_cache_free(anon_vma_chain_cachep, anode);
+			if (anon == anon_vma)
+				continue;
+			if (atomic_dec_and_test(&anon->refcount_free) &&
+					RB_EMPTY_ROOT(&anon->rb_root) &&
+					llist_empty(&anon->llclean) &&
+					 llist_empty(&anon->lduh.ll_head) &&
+					!delayed_work_pending(&anon->lduh.sync)) {
+				struct anon_vma *root = anon->root;
+				anon_vma_free(anon);
+				if (root != anon && atomic_dec_and_test(&root->refcount) &&
+						llist_empty(&root->llclean) &&
+						llist_empty(&root->lduh.ll_head) &&
+						!delayed_work_pending(&root->lduh.sync)) {
+					anon_vma_free(root);
+				}
+			}
+		} else {
+			llist_add(&anode->llist, &anon_vma->llclean);
+		}
+	}
+
+	if (RB_EMPTY_ROOT(&anon_vma->rb_root) &&
+			llist_empty(&anon_vma->llclean) &&
+			llist_empty(&anon_vma->lduh.ll_head) &&
+			!delayed_work_pending(&anon_vma->lduh.sync)) {
+		struct anon_vma *root = anon_vma->root;
+		anon_vma_free(anon_vma);
+		if (root != anon_vma && atomic_dec_and_test(&root->refcount) &&
+				llist_empty(&root->llclean) &&
+				llist_empty(&root->lduh.ll_head) &&
+				!delayed_work_pending(&root->lduh.sync)) {
+			anon_vma_free(root);
+		}
+	}
 }
 
 static void anon_vma_chain_link(struct vm_area_struct *vma,
@@ -136,8 +301,10 @@ static void anon_vma_chain_link(struct vm_area_struct *vma,
 {
 	avc->vma = vma;
 	avc->anon_vma = anon_vma;
+	atomic_inc(&anon_vma->refcount_free);
 	list_add(&avc->same_vma, &vma->anon_vma_chain);
-	anon_vma_interval_tree_insert(avc, &anon_vma->rb_root);
+	//anon_vma_interval_tree_insert(avc, &anon_vma->rb_root);
+	anon_vma_ldu_logical_insert(avc, anon_vma);
 }
 
 /**
@@ -190,19 +357,18 @@ int anon_vma_prepare(struct vm_area_struct *vma)
 			allocated = anon_vma;
 		}
 
-		anon_vma_lock_write(anon_vma);
+		//anon_vma_lock_write(anon_vma);
 		/* page_table_lock to protect against threads */
 		spin_lock(&mm->page_table_lock);
 		if (likely(!vma->anon_vma)) {
 			vma->anon_vma = anon_vma;
 			anon_vma_chain_link(vma, avc, anon_vma);
 			/* vma reference or self-parent link for new root */
-			anon_vma->degree++;
 			allocated = NULL;
 			avc = NULL;
 		}
 		spin_unlock(&mm->page_table_lock);
-		anon_vma_unlock_write(anon_vma);
+		//anon_vma_unlock_write(anon_vma);
 
 		if (unlikely(allocated))
 			put_anon_vma(allocated);
@@ -265,31 +431,19 @@ int anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)
 
 		avc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);
 		if (unlikely(!avc)) {
-			unlock_anon_vma_root(root);
+			//unlock_anon_vma_root(root);
 			root = NULL;
 			avc = anon_vma_chain_alloc(GFP_KERNEL);
 			if (!avc)
 				goto enomem_failure;
 		}
 		anon_vma = pavc->anon_vma;
-		root = lock_anon_vma_root(root, anon_vma);
+		//root = lock_anon_vma_root(root, anon_vma);
 		anon_vma_chain_link(dst, avc, anon_vma);
 
-		/*
-		 * Reuse existing anon_vma if its degree lower than two,
-		 * that means it has no vma and only one anon_vma child.
-		 *
-		 * Do not chose parent anon_vma, otherwise first child
-		 * will always reuse it. Root anon_vma is never reused:
-		 * it has self-parent reference and at least one child.
-		 */
-		if (!dst->anon_vma && anon_vma != src->anon_vma &&
-				anon_vma->degree < 2)
-			dst->anon_vma = anon_vma;
 	}
-	if (dst->anon_vma)
-		dst->anon_vma->degree++;
-	unlock_anon_vma_root(root);
+
+	//unlock_anon_vma_root(root);
 	return 0;
 
  enomem_failure:
@@ -354,12 +508,12 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 	 * this anon_vma is freed, because the lock lives in the root.
 	 */
 	get_anon_vma(anon_vma->root);
+
 	/* Mark this anon_vma as the one where our new (COWed) pages go. */
 	vma->anon_vma = anon_vma;
-	anon_vma_lock_write(anon_vma);
+	//anon_vma_lock_write(anon_vma);
 	anon_vma_chain_link(vma, avc, anon_vma);
-	anon_vma->parent->degree++;
-	anon_vma_unlock_write(anon_vma);
+	//anon_vma_unlock_write(anon_vma);
 
 	return 0;
 
@@ -373,7 +527,6 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 void unlink_anon_vmas(struct vm_area_struct *vma)
 {
 	struct anon_vma_chain *avc, *next;
-	struct anon_vma *root = NULL;
 
 	/*
 	 * Unlink each anon_vma chained to the VMA.  This list is ordered
@@ -382,39 +535,13 @@ void unlink_anon_vmas(struct vm_area_struct *vma)
 	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
 		struct anon_vma *anon_vma = avc->anon_vma;
 
-		root = lock_anon_vma_root(root, anon_vma);
-		anon_vma_interval_tree_remove(avc, &anon_vma->rb_root);
-
-		/*
-		 * Leave empty anon_vmas on the list - we'll need
-		 * to free them outside the lock.
-		 */
-		if (RB_EMPTY_ROOT(&anon_vma->rb_root)) {
-			anon_vma->parent->degree--;
-			continue;
-		}
-
-		list_del(&avc->same_vma);
-		anon_vma_chain_free(avc);
-	}
-	if (vma->anon_vma)
-		vma->anon_vma->degree--;
-	unlock_anon_vma_root(root);
-
-	/*
-	 * Iterate the list once more, it now only contains empty and unlinked
-	 * anon_vmas, destroy them. Could not do before due to __put_anon_vma()
-	 * needing to write-acquire the anon_vma->root->rwsem.
-	 */
-	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
-		struct anon_vma *anon_vma = avc->anon_vma;
-
-		BUG_ON(anon_vma->degree);
-		put_anon_vma(anon_vma);
-
+		//root = lock_anon_vma_root(root, anon_vma);
+		//anon_vma_interval_tree_remove(avc, &anon_vma->rb_root);
+		anon_vma_ldu_logical_remove(avc, anon_vma);
 		list_del(&avc->same_vma);
 		anon_vma_chain_free(avc);
 	}
+	//unlock_anon_vma_root(root);
 }
 
 static void anon_vma_ctor(void *data)
@@ -423,6 +550,9 @@ static void anon_vma_ctor(void *data)
 
 	init_rwsem(&anon_vma->rwsem);
 	atomic_set(&anon_vma->refcount, 0);
+	atomic_set(&anon_vma->refcount_free, 0);
+	init_llist_head(&anon_vma->llclean);
+	anon_vma_init_ldu_head(&anon_vma->lduh);
 	anon_vma->rb_root = RB_ROOT;
 }
 
@@ -516,14 +646,14 @@ struct anon_vma *page_lock_anon_vma_read(struct page *page)
 
 	anon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);
 	root_anon_vma = READ_ONCE(anon_vma->root);
-	if (down_read_trylock(&root_anon_vma->rwsem)) {
+	if (down_write_trylock(&root_anon_vma->rwsem)) {
 		/*
 		 * If the page is still mapped, then this anon_vma is still
 		 * its anon_vma, and holding the mutex ensures that it will
 		 * not go away, see anon_vma_free().
 		 */
 		if (!page_mapped(page)) {
-			up_read(&root_anon_vma->rwsem);
+			up_write(&root_anon_vma->rwsem);
 			anon_vma = NULL;
 		}
 		goto out;
@@ -543,7 +673,7 @@ struct anon_vma *page_lock_anon_vma_read(struct page *page)
 
 	/* we pinned the anon_vma, its safe to sleep */
 	rcu_read_unlock();
-	anon_vma_lock_read(anon_vma);
+	anon_vma_lock_write(anon_vma);
 
 	if (atomic_dec_and_test(&anon_vma->refcount)) {
 		/*
@@ -551,7 +681,7 @@ struct anon_vma *page_lock_anon_vma_read(struct page *page)
 		 * and bail -- can't simply use put_anon_vma() because
 		 * we'll deadlock on the anon_vma_lock_write() recursion.
 		 */
-		anon_vma_unlock_read(anon_vma);
+		anon_vma_unlock_write(anon_vma);
 		__put_anon_vma(anon_vma);
 		anon_vma = NULL;
 	}
@@ -565,7 +695,7 @@ out:
 
 void page_unlock_anon_vma_read(struct anon_vma *anon_vma)
 {
-	anon_vma_unlock_read(anon_vma);
+	anon_vma_unlock_write(anon_vma);
 }
 
 #ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
@@ -1151,6 +1281,8 @@ static void __page_set_anon_rmap(struct page *page,
 	anon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;
 	page->mapping = (struct address_space *) anon_vma;
 	page->index = linear_page_index(vma, address);
+
+
 }
 
 /**
@@ -1569,8 +1701,9 @@ bool is_vma_temporary_stack(struct vm_area_struct *vma)
 		return false;
 
 	if ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==
-						VM_STACK_INCOMPLETE_SETUP)
+						VM_STACK_INCOMPLETE_SETUP) {
 		return true;
+	}
 
 	return false;
 }
@@ -1678,9 +1811,16 @@ void __put_anon_vma(struct anon_vma *anon_vma)
 {
 	struct anon_vma *root = anon_vma->root;
 
+	if (!llist_empty(&anon_vma->lduh.ll_head)) {
+		flush_delayed_work(&anon_vma->lduh.sync);
+	}
 	anon_vma_free(anon_vma);
-	if (root != anon_vma && atomic_dec_and_test(&root->refcount))
+	if (root != anon_vma && atomic_dec_and_test(&root->refcount)) {
+		if (!llist_empty(&root->lduh.ll_head)) {
+			flush_delayed_work(&root->lduh.sync);
+		}
 		anon_vma_free(root);
+	}
 }
 
 static struct anon_vma *rmap_walk_anon_lock(struct page *page,
@@ -1701,7 +1841,7 @@ static struct anon_vma *rmap_walk_anon_lock(struct page *page,
 	if (!anon_vma)
 		return NULL;
 
-	anon_vma_lock_read(anon_vma);
+	anon_vma_lock_write(anon_vma);
 	return anon_vma;
 }
 
@@ -1730,6 +1870,7 @@ static int rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc)
 	if (!anon_vma)
 		return ret;
 
+	synchronize_ldu_anon(anon_vma->root);
 	pgoff = page_to_pgoff(page);
 	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
 		struct vm_area_struct *vma = avc->vma;
@@ -1746,7 +1887,7 @@ static int rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc)
 		if (rwc->done && rwc->done(page))
 			break;
 	}
-	anon_vma_unlock_read(anon_vma);
+	anon_vma_unlock_write(anon_vma);
 	return ret;
 }
 
@@ -1781,13 +1922,15 @@ static int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc)
 	if (!mapping)
 		return ret;
 
+	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	pgoff = page_to_pgoff(page);
-	i_mmap_lock_read(mapping);
 	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
 		unsigned long address = vma_address(page, vma);
 
 		cond_resched();
 
+
 		if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
 			continue;
 
@@ -1799,7 +1942,7 @@ static int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc)
 	}
 
 done:
-	i_mmap_unlock_read(mapping);
+	i_mmap_unlock_write(mapping);
 	return ret;
 }
 
@@ -1846,8 +1989,9 @@ void hugepage_add_anon_rmap(struct page *page,
 	BUG_ON(!anon_vma);
 	/* address might be in next vma when migration races vma_adjust */
 	first = atomic_inc_and_test(compound_mapcount_ptr(page));
-	if (first)
+	if (first) {
 		__hugepage_set_anon_rmap(page, vma, address, 0);
+	}
 }
 
 void hugepage_add_new_anon_rmap(struct page *page,
@@ -1858,3 +2002,15 @@ void hugepage_add_new_anon_rmap(struct page *page,
 	__hugepage_set_anon_rmap(page, vma, address, 1);
 }
 #endif /* CONFIG_HUGETLB_PAGE */
+
+
+static int __init avc_init_wq(void)
+{
+	avc_wq = create_singlethread_workqueue("avc");
+	WARN(!avc_wq, "failed to create avc workqueue\n");
+	pr_info("avc work queue initialize");
+
+	return 0;
+}
+__initcall(avc_init_wq);
+
-- 
2.7.4

