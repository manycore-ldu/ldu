From aecd536461b89315752ca183c4b4493986b62473 Mon Sep 17 00:00:00 2001
From: joohyun kyong <joohyun0115@gmail.com>
Date: Mon, 8 Aug 2016 10:15:09 -0700
Subject: [PATCH] harris: enable harris lock-free list

---
 fs/exec.c                     |   4 +-
 fs/hugetlbfs/inode.c          |  15 ++-
 fs/inode.c                    |   5 +-
 include/linux/fs.h            |  10 +-
 include/linux/lockfree_list.h | 114 +++++++++++++++++++++
 include/linux/mm.h            |  43 ++------
 include/linux/mm_types.h      |  17 +++-
 include/linux/rmap.h          |  15 ++-
 kernel/events/uprobes.c       |  15 ++-
 kernel/fork.c                 |  13 ++-
 lib/Kconfig                   |   4 +
 lib/Makefile                  |   1 +
 lib/lockfree_list.c           | 160 +++++++++++++++++++++++++++++
 mm/huge_memory.c              |  28 +++--
 mm/hugetlb.c                  |  22 +++-
 mm/interval_tree.c            |  89 ----------------
 mm/ksm.c                      |  12 ++-
 mm/memory-failure.c           |  27 +++--
 mm/memory.c                   |  19 +++-
 mm/mmap.c                     | 231 +++++++++++++++++++++++-------------------
 mm/rmap.c                     | 173 +++++++++++++++++++++----------
 21 files changed, 685 insertions(+), 332 deletions(-)
 create mode 100644 include/linux/lockfree_list.h
 create mode 100644 lib/lockfree_list.c

diff --git a/fs/exec.c b/fs/exec.c
index dcd4ac7..ec1b610 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -56,6 +56,7 @@
 #include <linux/pipe_fs_i.h>
 #include <linux/oom.h>
 #include <linux/compat.h>
+#include <linux/lockfree_list.h>
 
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
@@ -276,7 +277,8 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 	vma->vm_start = vma->vm_end - PAGE_SIZE;
 	vma->vm_flags = VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP;
 	vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
-	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	init_lockfree_list_head(&vma->anon_vma_chain, &vma->anon_vma_chain_head_node,
+			&vma->anon_vma_chain_tail_node);
 
 	err = insert_vm_struct(mm, vma);
 	if (err)
diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c
index e1f465a..6f505ae 100644
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@ -36,6 +36,7 @@
 #include <linux/magic.h>
 #include <linux/migrate.h>
 #include <linux/uio.h>
+#include <linux/lockfree_list.h>
 
 #include <asm/uaccess.h>
 
@@ -325,17 +326,23 @@ static void remove_huge_page(struct page *page)
 }
 
 static void
-hugetlb_vmdelete_list(struct rb_root *root, pgoff_t start, pgoff_t end)
+hugetlb_vmdelete_list(struct lockfree_list_head *head, pgoff_t start, pgoff_t end)
 {
 	struct vm_area_struct *vma;
+	struct lockfree_list_node *node = (struct lockfree_list_node *)get_unmarked_ref((long)head->head->next);
+	struct lockfree_list_node *onode = head->head->next;
 
 	/*
 	 * end == 0 indicates that the entire range after
 	 * start should be unmapped.
 	 */
-	vma_interval_tree_foreach(vma, root, start, end ? end : ULONG_MAX) {
+	lockfree_list_for_each_entry(vma, node, shared.linear, onode) {
 		unsigned long v_offset;
 		unsigned long v_end;
+		if (&vma->shared.linear == head->tail)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
 
 		/*
 		 * Can the expression below overflow on 32-bit arches?
@@ -507,7 +514,7 @@ static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)
 
 	i_size_write(inode, offset);
 	i_mmap_lock_write(mapping);
-	if (!RB_EMPTY_ROOT(&mapping->i_mmap))
+	if (!lockfree_list_empty(&mapping->i_mmap))
 		hugetlb_vmdelete_list(&mapping->i_mmap, pgoff, 0);
 	i_mmap_unlock_write(mapping);
 	remove_inode_hugepages(inode, offset, LLONG_MAX);
@@ -532,7 +539,7 @@ static long hugetlbfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 		inode_lock(inode);
 		i_mmap_lock_write(mapping);
-		if (!RB_EMPTY_ROOT(&mapping->i_mmap))
+		if (!lockfree_list_empty(&mapping->i_mmap))
 			hugetlb_vmdelete_list(&mapping->i_mmap,
 						hole_start >> PAGE_SHIFT,
 						hole_end  >> PAGE_SHIFT);
diff --git a/fs/inode.c b/fs/inode.c
index 69b8b52..d07c722 100644
--- a/fs/inode.c
+++ b/fs/inode.c
@@ -19,6 +19,7 @@
 #include <linux/ratelimit.h>
 #include <linux/list_lru.h>
 #include <trace/events/writeback.h>
+#include <linux/lockfree_list.h>
 #include "internal.h"
 
 /*
@@ -349,7 +350,9 @@ void address_space_init_once(struct address_space *mapping)
 	init_rwsem(&mapping->i_mmap_rwsem);
 	INIT_LIST_HEAD(&mapping->private_list);
 	spin_lock_init(&mapping->private_lock);
-	mapping->i_mmap = RB_ROOT;
+	init_lockfree_list_head(&mapping->i_mmap, &mapping->i_mmap_head_node,
+			&mapping->i_mmap_tail_node);
+
 }
 EXPORT_SYMBOL(address_space_init_once);
 
diff --git a/include/linux/fs.h b/include/linux/fs.h
index ae68100..f00cd1a 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -11,6 +11,7 @@
 #include <linux/list.h>
 #include <linux/list_lru.h>
 #include <linux/llist.h>
+#include <linux/lockfree_list.h>
 #include <linux/radix-tree.h>
 #include <linux/rbtree.h>
 #include <linux/init.h>
@@ -429,8 +430,11 @@ struct address_space {
 	struct radix_tree_root	page_tree;	/* radix tree of all pages */
 	spinlock_t		tree_lock;	/* and lock protecting it */
 	atomic_t		i_mmap_writable;/* count VM_SHARED mappings */
-	struct rb_root		i_mmap;		/* tree of private and shared mappings */
-	struct rw_semaphore	i_mmap_rwsem;	/* protect tree, count, list */
+	struct lockfree_list_head   i_mmap;
+	struct lockfree_list_node   i_mmap_head_node;
+	struct lockfree_list_node   i_mmap_tail_node;
+	struct rw_semaphore i_mmap_rwsem;;  /* protect tree, count, list */
+
 	/* Protected by tree_lock together with the radix tree */
 	unsigned long		nrpages;	/* number of total pages */
 	/* number of shadow or DAX exceptional entries */
@@ -521,7 +525,7 @@ static inline void i_mmap_unlock_read(struct address_space *mapping)
  */
 static inline int mapping_mapped(struct address_space *mapping)
 {
-	return	!RB_EMPTY_ROOT(&mapping->i_mmap);
+	return  !lockfree_list_empty(&mapping->i_mmap);
 }
 
 /*
diff --git a/include/linux/lockfree_list.h b/include/linux/lockfree_list.h
new file mode 100644
index 0000000..2d8121e
--- /dev/null
+++ b/include/linux/lockfree_list.h
@@ -0,0 +1,114 @@
+#ifndef _LOCKFREE_LIST_H_
+#define _LOCKFREE_LIST_H_
+
+#include <linux/kernel.h>
+#include <asm/cmpxchg.h>
+
+struct lockfree_list_head {
+	struct lockfree_list_node *head, *tail;
+};
+
+struct lockfree_list_node {
+	struct lockfree_list_node *next;
+	void *key;
+	int garbage;
+};
+
+#define LOCKFREE_LIST_HEAD_INIT(name)   { NULL, NULL }
+#define LOCKFREE_LIST_HEAD(name) \
+		struct lockfree_list_head name = LOCKFREE_LIST_HEAD_INIT(name)
+
+#define LOCKFREE_LIST_HEAD_NODE_INIT(name)   { NULL, NULL }
+#define LOCKFREE_LIST_HEAD_NODE(name) \
+		struct lockfree_list_node name = LOCKFREE_LIST_HEAD_NODE_INIT(name)
+
+#define LOCKFREE_LIST_TAIL_NODE_INIT(name)   { NULL, NULL }
+#define LOCKFREE_LIST_TAIL_NODE(name) \
+		struct lockfree_list_node name = LOCKFREE_LIST_TAIL_NODE_INIT(name)
+
+#define LOCKFREE_LIST_SAVE_KEY(name, member) \
+		(name)->member.key = (void *)(name)
+
+#define LOCKFREE_LIST_CLEAR_GC(name, member) \
+		(name)->member.garbage = 0
+
+static inline void init_lockfree_list_head(struct lockfree_list_head *list,
+		struct lockfree_list_node *head, struct lockfree_list_node *tail)
+{
+	tail->key = (void *)LONG_MAX;
+	tail->next = NULL;
+	head->key = 0;
+	head->next = tail;
+	list->head = head;
+	list->tail = tail;
+}
+
+static inline int is_marked_ref(long i)
+{
+	return (i & 0x1L);
+}
+
+static inline long get_unmarked_ref(long w)
+{
+	return (w & ~0x1L);
+}
+
+static inline long get_marked_ref(long w)
+{
+	return (w | 0x1L);
+}
+
+#define lockfree_list_entry(ptr, type, member)      \
+	container_of(ptr, type, member)
+
+#define lockfree_list_for_each(pos, node)           \
+	for ((pos) = (node); pos; (pos) = (pos)->next)
+
+#define lockfree_list_for_each_entry(pos, node, member, opos)		\
+	for ((pos) = lockfree_list_entry((node), typeof(*(pos)), member);   \
+		&(pos)->member != NULL;                    \
+		(pos) = lockfree_list_entry((struct lockfree_list_node *)get_unmarked_ref( \
+				(long)(pos)->member.next), typeof(*(pos)), member), \
+				 (opos) = (pos)->member.next)
+
+#define lockfree_list_for_each_entry_safe(pos, n, node, member, opos)	\
+	for (pos = lockfree_list_entry((node), typeof(*pos), member);             \
+		&pos->member != NULL &&	\
+		(n = lockfree_list_entry((struct lockfree_list_node *)get_unmarked_ref( \
+				(long)pos->member.next), typeof(*n), member), true); \
+		pos = n, (opos) = (pos)->member.next)
+
+static inline bool lockfree_list_empty(const struct lockfree_list_head *head)
+{
+	return ACCESS_ONCE(head->head->next) == ACCESS_ONCE(head->tail);
+}
+
+static inline struct lockfree_list_node *lockfree_list_next(
+		struct lockfree_list_node *node)
+{
+	return node->next;
+}
+
+extern bool lockfree_list_add_batch(struct lockfree_list_node *new,
+		struct lockfree_list_head *head);
+static inline bool lockfree_list_add(struct lockfree_list_node *new,
+		struct lockfree_list_head *head)
+{
+	return lockfree_list_add_batch(new, head);
+}
+
+struct lockfree_list_node *lockfree_list_del_batch(
+		struct lockfree_list_node *node, struct lockfree_list_head *head);
+static inline struct lockfree_list_node *lockfree_list_del(
+		struct lockfree_list_node *node, struct lockfree_list_head *head)
+{
+	return lockfree_list_del_batch(node, head);
+}
+
+static inline int lockfree_list_is_singular(const struct lockfree_list_head *head)
+{
+	return !lockfree_list_empty(head) && (head->head->next->next == head->tail);
+}
+
+
+#endif /* #ifndef _LOCKFREE_LIST_H_ */
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 516e149..541c1b7 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -9,6 +9,7 @@
 #include <linux/gfp.h>
 #include <linux/bug.h>
 #include <linux/list.h>
+#include <linux/lockfree_list.h>
 #include <linux/mmzone.h>
 #include <linux/rbtree.h>
 #include <linux/atomic.h>
@@ -25,7 +26,6 @@
 
 struct mempolicy;
 struct anon_vma;
-struct anon_vma_chain;
 struct file_ra_state;
 struct user_struct;
 struct writeback_control;
@@ -1881,38 +1881,15 @@ extern int min_free_kbytes;
 extern atomic_long_t mmap_pages_allocated;
 extern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);
 
-/* interval_tree.c */
-void vma_interval_tree_insert(struct vm_area_struct *node,
-			      struct rb_root *root);
-void vma_interval_tree_insert_after(struct vm_area_struct *node,
-				    struct vm_area_struct *prev,
-				    struct rb_root *root);
-void vma_interval_tree_remove(struct vm_area_struct *node,
-			      struct rb_root *root);
-struct vm_area_struct *vma_interval_tree_iter_first(struct rb_root *root,
-				unsigned long start, unsigned long last);
-struct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,
-				unsigned long start, unsigned long last);
-
-#define vma_interval_tree_foreach(vma, root, start, last)		\
-	for (vma = vma_interval_tree_iter_first(root, start, last);	\
-	     vma; vma = vma_interval_tree_iter_next(vma, start, last))
-
-void anon_vma_interval_tree_insert(struct anon_vma_chain *node,
-				   struct rb_root *root);
-void anon_vma_interval_tree_remove(struct anon_vma_chain *node,
-				   struct rb_root *root);
-struct anon_vma_chain *anon_vma_interval_tree_iter_first(
-	struct rb_root *root, unsigned long start, unsigned long last);
-struct anon_vma_chain *anon_vma_interval_tree_iter_next(
-	struct anon_vma_chain *node, unsigned long start, unsigned long last);
-#ifdef CONFIG_DEBUG_VM_RB
-void anon_vma_interval_tree_verify(struct anon_vma_chain *node);
-#endif
-
-#define anon_vma_interval_tree_foreach(avc, root, start, last)		 \
-	for (avc = anon_vma_interval_tree_iter_first(root, start, last); \
-	     avc; avc = anon_vma_interval_tree_iter_next(avc, start, last))
+static inline void vma_linear_insert(struct vm_area_struct *vma,
+		struct lockfree_list_head  *list)
+{
+	LOCKFREE_LIST_SAVE_KEY(vma, shared.linear);
+	LOCKFREE_LIST_CLEAR_GC(vma, shared.linear);
+	//pr_info("vma_linear_insert\n");
+	if (!lockfree_list_add(&vma->shared.linear, list))
+		pr_info("vma lockfree list add fail!!!\n");
+}
 
 /* mmap.c */
 extern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 624b78b..d0b0838 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -14,6 +14,8 @@
 #include <linux/page-flags-layout.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
+#include <linux/lockfree_list.h>
+#include <linux/llist.h>
 
 #ifndef AT_VECTOR_SIZE_ARCH
 #define AT_VECTOR_SIZE_ARCH 0
@@ -321,19 +323,24 @@ struct vm_area_struct {
 	 * For areas with an address space and backing store,
 	 * linkage into the address_space->i_mmap interval tree.
 	 */
-	struct {
-		struct rb_node rb;
-		unsigned long rb_subtree_last;
+	union {
+		struct lockfree_list_node linear;
+		struct list_head nonlinear;
 	} shared;
 
+	struct llist_node llnode; /* delayed free */
+
 	/*
 	 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
 	 * list, after a COW of one of the file pages.	A MAP_SHARED vma
 	 * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack
 	 * or brk vma (with NULL file) can only be in an anon_vma list.
 	 */
-	struct list_head anon_vma_chain; /* Serialized by mmap_sem &
-					  * page_table_lock */
+	 struct lockfree_list_head anon_vma_chain; /* Serialized by mmap_sem &
+	  * page_table_lock */
+	 struct lockfree_list_node   anon_vma_chain_head_node;
+	 struct lockfree_list_node   anon_vma_chain_tail_node;
+
 	struct anon_vma *anon_vma;	/* Serialized by page_table_lock */
 
 	/* Function pointers to deal with this struct. */
diff --git a/include/linux/rmap.h b/include/linux/rmap.h
index a07f42b..903acfdd 100644
--- a/include/linux/rmap.h
+++ b/include/linux/rmap.h
@@ -5,10 +5,12 @@
  */
 
 #include <linux/list.h>
+#include <linux/llist.h>
 #include <linux/slab.h>
 #include <linux/mm.h>
 #include <linux/rwsem.h>
 #include <linux/memcontrol.h>
+#include <linux/lockfree_list.h>
 
 /*
  * The anon_vma heads a list of private "related" vmas, to scan if
@@ -42,7 +44,7 @@ struct anon_vma {
 	 * This counter is used for making decision about reusing anon_vma
 	 * instead of forking new one. See comments in function anon_vma_clone.
 	 */
-	unsigned degree;
+	atomic_t degree;
 
 	struct anon_vma *parent;	/* Parent of this anon_vma */
 
@@ -54,7 +56,10 @@ struct anon_vma {
 	 * is serialized by a system wide lock only visible to
 	 * mm_take_all_locks() (mm_all_locks_mutex).
 	 */
-	struct rb_root rb_root;	/* Interval tree of private "related" vmas */
+	struct lockfree_list_head	head;
+	struct lockfree_list_node	head_node;
+	struct lockfree_list_node	tail_node;
+
 };
 
 /*
@@ -73,9 +78,9 @@ struct anon_vma {
 struct anon_vma_chain {
 	struct vm_area_struct *vma;
 	struct anon_vma *anon_vma;
-	struct list_head same_vma;   /* locked by mmap_sem & page_table_lock */
-	struct rb_node rb;			/* locked by anon_vma->rwsem */
-	unsigned long rb_subtree_last;
+	struct llist_node llnode; /* delayed free */
+	struct lockfree_list_node same_vma; /* locked by mmap_sem & page_table_lock */
+	struct lockfree_list_node same_anon_vma; /* locked by anon_vma->mutex */
 #ifdef CONFIG_DEBUG_VM_RB
 	unsigned long cached_vma_start, cached_vma_last;
 #endif
diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c
index 0167679..7889ca0 100644
--- a/kernel/events/uprobes.c
+++ b/kernel/events/uprobes.c
@@ -38,6 +38,7 @@
 #include <linux/task_work.h>
 #include <linux/shmem_fs.h>
 
+#include <linux/lockfree_list.h>
 #include <linux/uprobes.h>
 
 #define UINSNS_PER_PAGE			(PAGE_SIZE/UPROBE_XOL_SLOT_BYTES)
@@ -711,16 +712,26 @@ static inline struct map_info *free_map_info(struct map_info *info)
 static struct map_info *
 build_map_info(struct address_space *mapping, loff_t offset, bool is_register)
 {
-	unsigned long pgoff = offset >> PAGE_SHIFT;
 	struct vm_area_struct *vma;
 	struct map_info *curr = NULL;
 	struct map_info *prev = NULL;
 	struct map_info *info;
 	int more = 0;
+	struct lockfree_list_node *node;
+	struct lockfree_list_node *onode;
+
 
  again:
 	i_mmap_lock_read(mapping);
-	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
+	node = (struct lockfree_list_node *)get_unmarked_ref((long)mapping->i_mmap_head_node.next);
+	onode = mapping->i_mmap_head_node.next;
+	pr_debug("i_mmap read lock : %s\n", __func__);
+	lockfree_list_for_each_entry(vma, node, shared.linear, onode) {
+		if (&vma->shared.linear == &mapping->i_mmap_tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
+
 		if (!valid_vma(vma, is_register))
 			continue;
 
diff --git a/kernel/fork.c b/kernel/fork.c
index 2e391c7..08ef7389 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -75,6 +75,7 @@
 #include <linux/aio.h>
 #include <linux/compiler.h>
 #include <linux/sysctl.h>
+#include <linux/lockfree_list.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -448,7 +449,8 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		if (!tmp)
 			goto fail_nomem;
 		*tmp = *mpnt;
-		INIT_LIST_HEAD(&tmp->anon_vma_chain);
+		init_lockfree_list_head(&tmp->anon_vma_chain, &tmp->anon_vma_chain_head_node,
+				&tmp->anon_vma_chain_tail_node);
 		retval = vma_dup_policy(mpnt, tmp);
 		if (retval)
 			goto fail_nomem_policy;
@@ -467,15 +469,16 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			get_file(file);
 			if (tmp->vm_flags & VM_DENYWRITE)
 				atomic_dec(&inode->i_writecount);
-			i_mmap_lock_write(mapping);
+			//i_mmap_lock_write(mapping);
 			if (tmp->vm_flags & VM_SHARED)
 				atomic_inc(&mapping->i_mmap_writable);
 			flush_dcache_mmap_lock(mapping);
+			vma_linear_insert(tmp, &mapping->i_mmap);
 			/* insert tmp into the share list, just after mpnt */
-			vma_interval_tree_insert_after(tmp, mpnt,
-					&mapping->i_mmap);
+			//vma_interval_tree_insert_after(tmp, mpnt,
+			//		&mapping->i_mmap);
 			flush_dcache_mmap_unlock(mapping);
-			i_mmap_unlock_write(mapping);
+			//i_mmap_unlock_write(mapping);
 		}
 
 		/*
diff --git a/lib/Kconfig b/lib/Kconfig
index 133ebc0..3426850 100644
--- a/lib/Kconfig
+++ b/lib/Kconfig
@@ -348,6 +348,10 @@ config TEXTSEARCH_FSM
 config BTREE
 	bool
 
+config LOCKFREE_LIST
+	boolean
+	default y
+
 config INTERVAL_TREE
 	bool
 	help
diff --git a/lib/Makefile b/lib/Makefile
index a7c26a4..0bafab3 100644
--- a/lib/Makefile
+++ b/lib/Makefile
@@ -63,6 +63,7 @@ CFLAGS_hweight.o = $(subst $(quote),,$(CONFIG_ARCH_HWEIGHT_CFLAGS))
 obj-$(CONFIG_GENERIC_HWEIGHT) += hweight.o
 
 obj-$(CONFIG_BTREE) += btree.o
+obj-$(CONFIG_LOCKFREE_LIST) += lockfree_list.o
 obj-$(CONFIG_INTERVAL_TREE) += interval_tree.o
 obj-$(CONFIG_ASSOCIATIVE_ARRAY) += assoc_array.o
 obj-$(CONFIG_DEBUG_PREEMPT) += smp_processor_id.o
diff --git a/lib/lockfree_list.c b/lib/lockfree_list.c
new file mode 100644
index 0000000..66b9ee8
--- /dev/null
+++ b/lib/lockfree_list.c
@@ -0,0 +1,160 @@
+#include <linux/kernel.h>
+#include <linux/export.h>
+#include <linux/interrupt.h>
+#include <linux/lockfree_list.h>
+
+static struct lockfree_list_node *list_search_for_add(struct lockfree_list_head *head, void *key,
+		struct lockfree_list_node **left_node)
+{
+	struct lockfree_list_node *left_node_next;
+	struct lockfree_list_node *right_node;
+
+	left_node_next = head->head;
+search_again:
+	do {
+		struct lockfree_list_node *t = head->head;
+		struct lockfree_list_node *t_next = head->head->next;
+
+		/* Find left_node and right_node */
+		do {
+			if (!is_marked_ref((long)t_next)) {
+				(*left_node) = t;
+				left_node_next = t_next;
+			}
+			t = (struct lockfree_list_node *) get_unmarked_ref((long)t_next);
+
+			if (!t->next)
+				break;
+			t_next = t->next;
+//		} while (is_marked_ref((long)t_next) || (t->key > key));
+		} while (is_marked_ref((long)t_next));
+		right_node = t;
+
+		/* Check that nodes are adjacent */
+		if (left_node_next == right_node) {
+			if ((right_node->next) && is_marked_ref((long) right_node->next)) {
+				goto search_again;
+			} else
+				return right_node;
+		}
+
+		/* Remove one or more marked nodes */
+		if (cmpxchg(&(*left_node)->next, left_node_next, right_node) == left_node_next) {
+			struct lockfree_list_node *cur = left_node_next;
+			do {
+				struct lockfree_list_node *free = cur;
+				cur = (struct lockfree_list_node *) get_unmarked_ref((long) cur->next);
+				free->garbage = 1;
+			} while (cur != right_node);
+			if ((right_node->next) && is_marked_ref((long) right_node->next)) {
+				goto search_again;
+			} else
+				return right_node;
+		}
+
+	} while (1);
+}
+
+static struct lockfree_list_node *list_search_for_del(struct lockfree_list_head *head,
+		struct lockfree_list_node *key_node,
+		struct lockfree_list_node **left_node)
+{
+	struct lockfree_list_node *left_node_next;
+	struct lockfree_list_node *right_node;
+
+	left_node_next = head->head;
+search_again:
+	do {
+		struct lockfree_list_node *t = head->head;
+		struct lockfree_list_node *t_next = head->head->next;
+
+		/* Find left_node and right_node */
+		do {
+			if (!is_marked_ref((long)t_next)) {
+				(*left_node) = t;
+				left_node_next = t_next;
+			}
+			t = (struct lockfree_list_node *) get_unmarked_ref((long)t_next);
+
+			if (!t->next)
+				break;
+			t_next = t->next;
+		} while (is_marked_ref((long)t_next) || (t->key != key_node->key));
+//		} while (is_marked_ref((long)t_next) || (t->key > key_node->key));
+		right_node = t;
+
+		/* Check that nodes are adjacent */
+		if (left_node_next == right_node) {
+			if ((right_node->next) && is_marked_ref((long) right_node->next)) {
+				goto search_again;
+		} else
+				return right_node;
+		}
+
+		/* Remove one or more marked nodes */
+		if (cmpxchg(&(*left_node)->next, left_node_next, right_node) == left_node_next) {
+			struct lockfree_list_node *cur = left_node_next;
+			do {
+				struct lockfree_list_node *free = cur;
+				cur = (struct lockfree_list_node *) get_unmarked_ref((long) cur->next);
+				free->garbage = 1;
+			} while (cur != right_node);
+			if ((right_node->next) && is_marked_ref((long) right_node->next)) {
+				goto search_again;
+			} else
+				return right_node;
+		}
+
+	} while (1);
+}
+
+bool lockfree_list_add_batch(struct lockfree_list_node *new,
+		struct lockfree_list_head *head)
+{
+	struct lockfree_list_node *right_node, *left_node;
+	left_node = head->head;
+	do {
+		right_node = list_search_for_add(head, new->key, &left_node);
+		new->next = right_node;
+		smp_mb();
+	} while (cmpxchg(&left_node->next, right_node, new) != right_node);
+
+	return true;
+}
+EXPORT_SYMBOL_GPL(lockfree_list_add_batch);
+
+struct lockfree_list_node *lockfree_list_del_batch(
+		struct lockfree_list_node *node, struct lockfree_list_head *head)
+{
+	struct lockfree_list_node *right_node, *right_node_next, *left_node;
+	left_node = head->head;
+retry:
+	do {
+		right_node = list_search_for_del(head, node, &left_node);
+		/*
+		 * harris's lockfree list sometimes meet to the searching miss becasue
+		 * of the concurrent marked reference pointer. Therefore, we search again
+		 * due to removing the object obviously. It seems to hang in the this
+		 * code. However, our lockfree list patently contain removed object in
+		 * the list, so it may not ocurr infinite loop.
+		 */
+		if ((right_node->key != node->key)) {
+			goto retry;
+		}
+		right_node_next = right_node->next;
+		if (!is_marked_ref((long) right_node_next))
+			if (cmpxchg(&right_node->next, right_node_next,
+					(struct lockfree_list_node *)get_marked_ref((long) right_node_next)) == right_node_next)
+				break;
+	} while (1);
+
+	if (cmpxchg(&left_node->next, right_node, right_node_next) != right_node) {
+		right_node = list_search_for_del(head, right_node, &left_node);
+	} else {
+		struct lockfree_list_node *free = (struct lockfree_list_node *) get_unmarked_ref((long) right_node);
+		free->garbage = 1;
+	}
+
+	return right_node;
+}
+EXPORT_SYMBOL_GPL(lockfree_list_del_batch);
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index e10a4fe..19aa116 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -28,6 +28,7 @@
 #include <linux/debugfs.h>
 #include <linux/migrate.h>
 #include <linux/hashtable.h>
+#include <linux/lockfree_list.h>
 #include <linux/userfaultfd_k.h>
 #include <linux/page_idle.h>
 
@@ -3123,13 +3124,19 @@ static void freeze_page_vma(struct vm_area_struct *vma, struct page *page,
 static void freeze_page(struct anon_vma *anon_vma, struct page *page)
 {
 	struct anon_vma_chain *avc;
-	pgoff_t pgoff = page_to_pgoff(page);
+	struct lockfree_list_node *onode = anon_vma->head_node.next;
+	struct lockfree_list_node *node = (struct lockfree_list_node *)get_unmarked_ref((long)anon_vma->head_node.next);
 
 	VM_BUG_ON_PAGE(!PageHead(page), page);
 
-	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff,
-			pgoff + HPAGE_PMD_NR - 1) {
-		unsigned long address = __vma_address(page, avc->vma);
+	lockfree_list_for_each_entry(avc, node, same_anon_vma, onode) {
+		unsigned long address;
+		if (&avc->same_anon_vma == &anon_vma->tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
+
+		address = __vma_address(page, avc->vma);
 
 		mmu_notifier_invalidate_range_start(avc->vma->vm_mm,
 				address, address + HPAGE_PMD_SIZE);
@@ -3206,12 +3213,17 @@ static void unfreeze_page_vma(struct vm_area_struct *vma, struct page *page,
 static void unfreeze_page(struct anon_vma *anon_vma, struct page *page)
 {
 	struct anon_vma_chain *avc;
-	pgoff_t pgoff = page_to_pgoff(page);
+	struct lockfree_list_node *onode = anon_vma->head_node.next;
+	struct lockfree_list_node *node = (struct lockfree_list_node *)get_unmarked_ref((long)anon_vma->head_node.next);
 
-	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root,
-			pgoff, pgoff + HPAGE_PMD_NR - 1) {
-		unsigned long address = __vma_address(page, avc->vma);
+	lockfree_list_for_each_entry(avc, node, same_anon_vma, onode) {
+		unsigned long address;
+		if (&avc->same_anon_vma == &anon_vma->tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
 
+		address = __vma_address(page, avc->vma);
 		mmu_notifier_invalidate_range_start(avc->vma->vm_mm,
 				address, address + HPAGE_PMD_SIZE);
 		unfreeze_page_vma(avc->vma, page, address);
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 01f2b48..d9935a7 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -3,6 +3,7 @@
  * (C) Nadia Yvette Chambers, April 2004
  */
 #include <linux/list.h>
+#include <linux/lockfree_list.h>
 #include <linux/init.h>
 #include <linux/mm.h>
 #include <linux/seq_file.h>
@@ -3264,6 +3265,8 @@ static void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,
 	struct vm_area_struct *iter_vma;
 	struct address_space *mapping;
 	pgoff_t pgoff;
+	struct lockfree_list_node *node;
+	struct lockfree_list_node *onode;
 
 	/*
 	 * vm_pgoff is in PAGE_SIZE units, hence the different calculation
@@ -3280,7 +3283,13 @@ static void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,
 	 * __unmap_hugepage_range() is called as the lock is already held
 	 */
 	i_mmap_lock_write(mapping);
-	vma_interval_tree_foreach(iter_vma, &mapping->i_mmap, pgoff, pgoff) {
+	node = (struct lockfree_list_node *)get_unmarked_ref((long)mapping->i_mmap_head_node.next);
+	onode = mapping->i_mmap_head_node.next;
+	lockfree_list_for_each_entry(iter_vma, node, shared.linear, onode) {
+		if (&iter_vma->shared.linear == &mapping->i_mmap_tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
 		/* Do not unmap the current VMA */
 		if (iter_vma == vma)
 			continue;
@@ -4162,12 +4171,21 @@ pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)
 	pte_t *spte = NULL;
 	pte_t *pte;
 	spinlock_t *ptl;
+	struct lockfree_list_node *node;
+	struct lockfree_list_node *onode;
 
 	if (!vma_shareable(vma, addr))
 		return (pte_t *)pmd_alloc(mm, pud, addr);
 
 	i_mmap_lock_write(mapping);
-	vma_interval_tree_foreach(svma, &mapping->i_mmap, idx, idx) {
+	node = (struct lockfree_list_node *)get_unmarked_ref((long)mapping->i_mmap_head_node.next);
+	onode = mapping->i_mmap_head_node.next;
+	lockfree_list_for_each_entry(svma, node, shared.linear, onode) {
+		if (&svma->shared.linear == &mapping->i_mmap_tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
+
 		if (svma == vma)
 			continue;
 
diff --git a/mm/interval_tree.c b/mm/interval_tree.c
index f2c2492..e53b147 100644
--- a/mm/interval_tree.c
+++ b/mm/interval_tree.c
@@ -21,92 +21,3 @@ static inline unsigned long vma_last_pgoff(struct vm_area_struct *v)
 	return v->vm_pgoff + ((v->vm_end - v->vm_start) >> PAGE_SHIFT) - 1;
 }
 
-INTERVAL_TREE_DEFINE(struct vm_area_struct, shared.rb,
-		     unsigned long, shared.rb_subtree_last,
-		     vma_start_pgoff, vma_last_pgoff,, vma_interval_tree)
-
-/* Insert node immediately after prev in the interval tree */
-void vma_interval_tree_insert_after(struct vm_area_struct *node,
-				    struct vm_area_struct *prev,
-				    struct rb_root *root)
-{
-	struct rb_node **link;
-	struct vm_area_struct *parent;
-	unsigned long last = vma_last_pgoff(node);
-
-	VM_BUG_ON_VMA(vma_start_pgoff(node) != vma_start_pgoff(prev), node);
-
-	if (!prev->shared.rb.rb_right) {
-		parent = prev;
-		link = &prev->shared.rb.rb_right;
-	} else {
-		parent = rb_entry(prev->shared.rb.rb_right,
-				  struct vm_area_struct, shared.rb);
-		if (parent->shared.rb_subtree_last < last)
-			parent->shared.rb_subtree_last = last;
-		while (parent->shared.rb.rb_left) {
-			parent = rb_entry(parent->shared.rb.rb_left,
-				struct vm_area_struct, shared.rb);
-			if (parent->shared.rb_subtree_last < last)
-				parent->shared.rb_subtree_last = last;
-		}
-		link = &parent->shared.rb.rb_left;
-	}
-
-	node->shared.rb_subtree_last = last;
-	rb_link_node(&node->shared.rb, &parent->shared.rb, link);
-	rb_insert_augmented(&node->shared.rb, root,
-			    &vma_interval_tree_augment);
-}
-
-static inline unsigned long avc_start_pgoff(struct anon_vma_chain *avc)
-{
-	return vma_start_pgoff(avc->vma);
-}
-
-static inline unsigned long avc_last_pgoff(struct anon_vma_chain *avc)
-{
-	return vma_last_pgoff(avc->vma);
-}
-
-INTERVAL_TREE_DEFINE(struct anon_vma_chain, rb, unsigned long, rb_subtree_last,
-		     avc_start_pgoff, avc_last_pgoff,
-		     static inline, __anon_vma_interval_tree)
-
-void anon_vma_interval_tree_insert(struct anon_vma_chain *node,
-				   struct rb_root *root)
-{
-#ifdef CONFIG_DEBUG_VM_RB
-	node->cached_vma_start = avc_start_pgoff(node);
-	node->cached_vma_last = avc_last_pgoff(node);
-#endif
-	__anon_vma_interval_tree_insert(node, root);
-}
-
-void anon_vma_interval_tree_remove(struct anon_vma_chain *node,
-				   struct rb_root *root)
-{
-	__anon_vma_interval_tree_remove(node, root);
-}
-
-struct anon_vma_chain *
-anon_vma_interval_tree_iter_first(struct rb_root *root,
-				  unsigned long first, unsigned long last)
-{
-	return __anon_vma_interval_tree_iter_first(root, first, last);
-}
-
-struct anon_vma_chain *
-anon_vma_interval_tree_iter_next(struct anon_vma_chain *node,
-				 unsigned long first, unsigned long last)
-{
-	return __anon_vma_interval_tree_iter_next(node, first, last);
-}
-
-#ifdef CONFIG_DEBUG_VM_RB
-void anon_vma_interval_tree_verify(struct anon_vma_chain *node)
-{
-	WARN_ON_ONCE(node->cached_vma_start != avc_start_pgoff(node));
-	WARN_ON_ONCE(node->cached_vma_last != avc_last_pgoff(node));
-}
-#endif
diff --git a/mm/ksm.c b/mm/ksm.c
index ca6d2a0..5fe3a06 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -37,6 +37,7 @@
 #include <linux/freezer.h>
 #include <linux/oom.h>
 #include <linux/numa.h>
+#include <linux/lockfree_list.h>
 
 #include <asm/tlbflush.h>
 #include "internal.h"
@@ -1897,11 +1898,18 @@ again:
 		struct anon_vma *anon_vma = rmap_item->anon_vma;
 		struct anon_vma_chain *vmac;
 		struct vm_area_struct *vma;
+		struct lockfree_list_node *node;
+		struct lockfree_list_node *onode;
 
 		cond_resched();
 		anon_vma_lock_read(anon_vma);
-		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
-					       0, ULONG_MAX) {
+		 node = (struct lockfree_list_node *)get_unmarked_ref((long)anon_vma->head_node.next);
+		 onode = anon_vma->head_node.next;
+		 lockfree_list_for_each_entry(vmac, node, same_anon_vma, onode) {
+			 if (&vmac->same_anon_vma == &anon_vma->tail_node)
+				 break;
+			 if (is_marked_ref((long)onode))
+				 continue;
 			cond_resched();
 			vma = vmac->vma;
 			if (rmap_item->address < vma->vm_start ||
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index ac595e7..be44665 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -57,6 +57,7 @@
 #include <linux/mm_inline.h>
 #include <linux/kfifo.h>
 #include <linux/ratelimit.h>
+#include <linux/lockfree_list.h>
 #include "internal.h"
 #include "ras/ras_event.h"
 
@@ -408,22 +409,26 @@ static void collect_procs_anon(struct page *page, struct list_head *to_kill,
 	struct vm_area_struct *vma;
 	struct task_struct *tsk;
 	struct anon_vma *av;
-	pgoff_t pgoff;
 
 	av = page_lock_anon_vma_read(page);
 	if (av == NULL)	/* Not actually mapped anymore */
 		return;
 
-	pgoff = page_to_pgoff(page);
 	read_lock(&tasklist_lock);
 	for_each_process (tsk) {
 		struct anon_vma_chain *vmac;
 		struct task_struct *t = task_early_kill(tsk, force_early);
+		struct lockfree_list_node *node = (struct lockfree_list_node *)get_unmarked_ref((long)av->head_node.next);
+		struct lockfree_list_node *onode = av->head_node.next;
 
 		if (!t)
 			continue;
-		anon_vma_interval_tree_foreach(vmac, &av->rb_root,
-					       pgoff, pgoff) {
+		lockfree_list_for_each_entry(vmac, node, same_anon_vma, onode) {
+			if (&vmac->same_anon_vma == &av->tail_node)
+				break;
+			if (is_marked_ref((long)onode))
+				continue;
+
 			vma = vmac->vma;
 			if (!page_mapped_in_vma(page, vma))
 				continue;
@@ -444,17 +449,25 @@ static void collect_procs_file(struct page *page, struct list_head *to_kill,
 	struct vm_area_struct *vma;
 	struct task_struct *tsk;
 	struct address_space *mapping = page->mapping;
+	struct lockfree_list_node *node;
+	struct lockfree_list_node *onode;
 
 	i_mmap_lock_read(mapping);
+	node = (struct lockfree_list_node *)get_unmarked_ref((long)mapping->i_mmap_head_node.next);
+	onode = mapping->i_mmap_head_node.next;
+
 	read_lock(&tasklist_lock);
 	for_each_process(tsk) {
-		pgoff_t pgoff = page_to_pgoff(page);
 		struct task_struct *t = task_early_kill(tsk, force_early);
 
 		if (!t)
 			continue;
-		vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff,
-				      pgoff) {
+		lockfree_list_for_each_entry(vma, node, shared.linear, onode) {
+			if (&vma->shared.linear == &mapping->i_mmap_tail_node)
+				break;
+			if (is_marked_ref((long)onode))
+				continue;
+
 			/*
 			 * Send early kill signal to tasks where a vma covers
 			 * the page but the corrupted page is not necessarily
diff --git a/mm/memory.c b/mm/memory.c
index 8132787..654862b 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -63,6 +63,7 @@
 #include <linux/dma-debug.h>
 #include <linux/debugfs.h>
 #include <linux/userfaultfd_k.h>
+#include <linux/lockfree_list.h>
 
 #include <asm/io.h>
 #include <asm/pgalloc.h>
@@ -527,6 +528,8 @@ void free_pgd_range(struct mmu_gather *tlb,
 	} while (pgd++, addr = next, addr != end);
 }
 
+extern void free_anon_vma_chain(void);
+
 void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *vma,
 		unsigned long floor, unsigned long ceiling)
 {
@@ -560,6 +563,7 @@ void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *vma,
 		}
 		vma = next;
 	}
+	free_anon_vma_chain();
 }
 
 int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,
@@ -2366,14 +2370,19 @@ static void unmap_mapping_range_vma(struct vm_area_struct *vma,
 	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
 }
 
-static inline void unmap_mapping_range_tree(struct rb_root *root,
+static inline void unmap_mapping_range_linear_list(struct lockfree_list_head *head,
 					    struct zap_details *details)
 {
 	struct vm_area_struct *vma;
 	pgoff_t vba, vea, zba, zea;
+	struct lockfree_list_node *node = (struct lockfree_list_node *)get_unmarked_ref((long)head->head->next);
+	struct lockfree_list_node *onode = head->head->next;
 
-	vma_interval_tree_foreach(vma, root,
-			details->first_index, details->last_index) {
+	lockfree_list_for_each_entry(vma, node, shared.linear, onode) {
+		if (&vma->shared.linear == head->tail)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
 
 		vba = vma->vm_pgoff;
 		vea = vba + vma_pages(vma) - 1;
@@ -2433,8 +2442,8 @@ void unmap_mapping_range(struct address_space *mapping,
 
 	/* DAX uses i_mmap_lock to serialise file truncate vs page fault */
 	i_mmap_lock_write(mapping);
-	if (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap)))
-		unmap_mapping_range_tree(&mapping->i_mmap, &details);
+	if (unlikely(!lockfree_list_empty(&mapping->i_mmap)))
+		unmap_mapping_range_linear_list(&mapping->i_mmap, &details);
 	i_mmap_unlock_write(mapping);
 }
 EXPORT_SYMBOL(unmap_mapping_range);
diff --git a/mm/mmap.c b/mm/mmap.c
index 76d1ec2..90bfa2b 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -43,6 +43,8 @@
 #include <linux/printk.h>
 #include <linux/userfaultfd_k.h>
 #include <linux/moduleparam.h>
+#include <linux/lockfree_list.h>
+#include <linux/llist.h>
 
 #include <asm/uaccess.h>
 #include <asm/cacheflush.h>
@@ -73,6 +75,9 @@ int mmap_rnd_compat_bits __read_mostly = CONFIG_ARCH_MMAP_RND_COMPAT_BITS;
 static bool ignore_rlimit_data = true;
 core_param(ignore_rlimit_data, ignore_rlimit_data, bool, 0644);
 
+static struct work_struct i_mmap_free_wq;
+static LLIST_HEAD(i_mmap_freelist);
+
 static void unmap_region(struct mm_struct *mm,
 		struct vm_area_struct *vma, struct vm_area_struct *prev,
 		unsigned long start, unsigned long end);
@@ -251,16 +256,16 @@ error:
  * Requires inode->i_mapping->i_mmap_rwsem
  */
 static void __remove_shared_vm_struct(struct vm_area_struct *vma,
-		struct file *file, struct address_space *mapping)
+		struct file *file, struct address_space *mapping, int need_lock)
 {
+	flush_dcache_mmap_lock(mapping);
+	lockfree_list_del(&vma->shared.linear, &mapping->i_mmap);
+	flush_dcache_mmap_unlock(mapping);
+
 	if (vma->vm_flags & VM_DENYWRITE)
-		atomic_inc(&file_inode(file)->i_writecount);
+		allow_write_access(file);
 	if (vma->vm_flags & VM_SHARED)
 		mapping_unmap_writable(mapping);
-
-	flush_dcache_mmap_lock(mapping);
-	vma_interval_tree_remove(vma, &mapping->i_mmap);
-	flush_dcache_mmap_unlock(mapping);
 }
 
 /*
@@ -273,12 +278,34 @@ void unlink_file_vma(struct vm_area_struct *vma)
 
 	if (file) {
 		struct address_space *mapping = file->f_mapping;
-		i_mmap_lock_write(mapping);
-		__remove_shared_vm_struct(vma, file, mapping);
-		i_mmap_unlock_write(mapping);
+		//i_mmap_lock_write(mapping);
+		//pr_info("i_mmap write lock : %s\n", __func__);
+		__remove_shared_vm_struct(vma, file, mapping, 0);
+		pr_debug("i_mmap write unlock : %s\n", __func__);
+		//i_mmap_unlock_write(mapping);
+
 	}
 }
 
+static void i_mmap_free_work_func(struct work_struct *w)
+{
+	struct llist_node *entry;
+	struct vm_area_struct *vma, *vma_next;
+
+	if (llist_empty(&i_mmap_freelist))
+		return;
+
+	entry = llist_del_all(&i_mmap_freelist);
+	llist_for_each_entry_safe(vma, vma_next, entry, llnode) {
+		if (vma->shared.linear.garbage || !vma->vm_file)
+			kmem_cache_free(vm_area_cachep, vma);
+		else
+			llist_add(&vma->llnode, &i_mmap_freelist);
+	}
+}
+
+
+
 /*
  * Close a vm structure and free it, returning the next.
  */
@@ -292,7 +319,9 @@ static struct vm_area_struct *remove_vma(struct vm_area_struct *vma)
 	if (vma->vm_file)
 		fput(vma->vm_file);
 	mpol_put(vma_policy(vma));
-	kmem_cache_free(vm_area_cachep, vma);
+	llist_add(&vma->llnode, &i_mmap_freelist);
+	//kmem_cache_free(vm_area_cachep, vma);
+
 	return next;
 }
 
@@ -537,38 +566,6 @@ static void vma_rb_erase(struct vm_area_struct *vma, struct rb_root *root)
 	rb_erase_augmented(&vma->vm_rb, root, &vma_gap_callbacks);
 }
 
-/*
- * vma has some anon_vma assigned, and is already inserted on that
- * anon_vma's interval trees.
- *
- * Before updating the vma's vm_start / vm_end / vm_pgoff fields, the
- * vma must be removed from the anon_vma's interval trees using
- * anon_vma_interval_tree_pre_update_vma().
- *
- * After the update, the vma will be reinserted using
- * anon_vma_interval_tree_post_update_vma().
- *
- * The entire update must be protected by exclusive mmap_sem and by
- * the root anon_vma's mutex.
- */
-static inline void
-anon_vma_interval_tree_pre_update_vma(struct vm_area_struct *vma)
-{
-	struct anon_vma_chain *avc;
-
-	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
-		anon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);
-}
-
-static inline void
-anon_vma_interval_tree_post_update_vma(struct vm_area_struct *vma)
-{
-	struct anon_vma_chain *avc;
-
-	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
-		anon_vma_interval_tree_insert(avc, &avc->anon_vma->rb_root);
-}
-
 static int find_vma_links(struct mm_struct *mm, unsigned long addr,
 		unsigned long end, struct vm_area_struct **pprev,
 		struct rb_node ***rb_link, struct rb_node **rb_parent)
@@ -655,7 +652,7 @@ void __vma_link_rb(struct mm_struct *mm, struct vm_area_struct *vma,
 	vma_rb_insert(vma, &mm->mm_rb);
 }
 
-static void __vma_link_file(struct vm_area_struct *vma)
+static void __vma_link_file(struct vm_area_struct *vma, int need_lock)
 {
 	struct file *file;
 
@@ -663,14 +660,15 @@ static void __vma_link_file(struct vm_area_struct *vma)
 	if (file) {
 		struct address_space *mapping = file->f_mapping;
 
+		flush_dcache_mmap_lock(mapping);
+		vma_linear_insert(vma, &mapping->i_mmap);
+		flush_dcache_mmap_unlock(mapping);
+
 		if (vma->vm_flags & VM_DENYWRITE)
-			atomic_dec(&file_inode(file)->i_writecount);
+			put_write_access(file_inode(file));
 		if (vma->vm_flags & VM_SHARED)
-			atomic_inc(&mapping->i_mmap_writable);
+			mapping_allow_writable(mapping);
 
-		flush_dcache_mmap_lock(mapping);
-		vma_interval_tree_insert(vma, &mapping->i_mmap);
-		flush_dcache_mmap_unlock(mapping);
 	}
 }
 
@@ -689,17 +687,17 @@ static void vma_link(struct mm_struct *mm, struct vm_area_struct *vma,
 {
 	struct address_space *mapping = NULL;
 
+	__vma_link(mm, vma, prev, rb_link, rb_parent);
+
 	if (vma->vm_file) {
 		mapping = vma->vm_file->f_mapping;
-		i_mmap_lock_write(mapping);
+		//i_mmap_lock_write(mapping);
+		//pr_info("i_mmap write lock : %s\n", __func__);
+		__vma_link_file(vma, 1);
+		pr_debug("i_mmap write unlock : %s\n", __func__);
+		//i_mmap_unlock_write(mapping);
 	}
 
-	__vma_link(mm, vma, prev, rb_link, rb_parent);
-	__vma_link_file(vma);
-
-	if (mapping)
-		i_mmap_unlock_write(mapping);
-
 	mm->map_count++;
 	validate_mm(mm);
 }
@@ -749,7 +747,7 @@ int vma_adjust(struct vm_area_struct *vma, unsigned long start,
 	struct vm_area_struct *next = vma->vm_next;
 	struct vm_area_struct *importer = NULL;
 	struct address_space *mapping = NULL;
-	struct rb_root *root = NULL;
+	struct lockfree_list_head *head = NULL;
 	struct anon_vma *anon_vma = NULL;
 	struct file *file = vma->vm_file;
 	bool start_changed = false, end_changed = false;
@@ -804,13 +802,13 @@ again:			remove_next = 1 + (end > next->vm_end);
 
 	if (file) {
 		mapping = file->f_mapping;
-		root = &mapping->i_mmap;
+		head = &mapping->i_mmap;
 		uprobe_munmap(vma, vma->vm_start, vma->vm_end);
 
 		if (adjust_next)
 			uprobe_munmap(next, next->vm_start, next->vm_end);
 
-		i_mmap_lock_write(mapping);
+		//i_mmap_lock_write(mapping);
 		if (insert) {
 			/*
 			 * Put into interval tree now, so instantiated pages
@@ -818,29 +816,21 @@ again:			remove_next = 1 + (end > next->vm_end);
 			 * throughout; but we cannot insert into address
 			 * space until vma start or end is updated.
 			 */
-			__vma_link_file(insert);
+			__vma_link_file(insert, 1);
 		}
 	}
 
+
 	vma_adjust_trans_huge(vma, start, end, adjust_next);
 
-	anon_vma = vma->anon_vma;
-	if (!anon_vma && adjust_next)
-		anon_vma = next->anon_vma;
-	if (anon_vma) {
+	if (vma->anon_vma && (importer || start != vma->vm_start)) {
+		anon_vma = vma->anon_vma;
 		VM_BUG_ON_VMA(adjust_next && next->anon_vma &&
 			  anon_vma != next->anon_vma, next);
+	} else if (adjust_next && next->anon_vma)
+		anon_vma = next->anon_vma;
+	if (anon_vma) {
 		anon_vma_lock_write(anon_vma);
-		anon_vma_interval_tree_pre_update_vma(vma);
-		if (adjust_next)
-			anon_vma_interval_tree_pre_update_vma(next);
-	}
-
-	if (root) {
-		flush_dcache_mmap_lock(mapping);
-		vma_interval_tree_remove(vma, root);
-		if (adjust_next)
-			vma_interval_tree_remove(next, root);
 	}
 
 	if (start != vma->vm_start) {
@@ -857,13 +847,6 @@ again:			remove_next = 1 + (end > next->vm_end);
 		next->vm_pgoff += adjust_next;
 	}
 
-	if (root) {
-		if (adjust_next)
-			vma_interval_tree_insert(next, root);
-		vma_interval_tree_insert(vma, root);
-		flush_dcache_mmap_unlock(mapping);
-	}
-
 	if (remove_next) {
 		/*
 		 * vma_merge has merged next into vma, and needs
@@ -871,7 +854,7 @@ again:			remove_next = 1 + (end > next->vm_end);
 		 */
 		__vma_unlink(mm, next, vma);
 		if (file)
-			__remove_shared_vm_struct(next, file, mapping);
+			__remove_shared_vm_struct(next, file, mapping, 1);
 	} else if (insert) {
 		/*
 		 * split_vma has split insert from vma, and needs
@@ -891,15 +874,12 @@ again:			remove_next = 1 + (end > next->vm_end);
 	}
 
 	if (anon_vma) {
-		anon_vma_interval_tree_post_update_vma(vma);
-		if (adjust_next)
-			anon_vma_interval_tree_post_update_vma(next);
 		anon_vma_unlock_write(anon_vma);
 	}
-	if (mapping)
-		i_mmap_unlock_write(mapping);
+	//if (mapping)
+	//	i_mmap_unlock_write(mapping);
 
-	if (root) {
+	if (head) {
 		uprobe_mmap(vma);
 
 		if (adjust_next)
@@ -915,7 +895,8 @@ again:			remove_next = 1 + (end > next->vm_end);
 			anon_vma_merge(vma, next);
 		mm->map_count--;
 		mpol_put(vma_policy(next));
-		kmem_cache_free(vm_area_cachep, next);
+		llist_add(&next->llnode, &i_mmap_freelist);
+		//kmem_cache_free(vm_area_cachep, next);
 		/*
 		 * In mprotect's case 6 (see comments on vma_merge),
 		 * we must remove another next too. It would clutter
@@ -973,7 +954,8 @@ static inline int is_mergeable_anon_vma(struct anon_vma *anon_vma1,
 	 * parents. This can improve scalability caused by anon_vma lock.
 	 */
 	if ((!anon_vma1 || !anon_vma2) && (!vma ||
-		list_is_singular(&vma->anon_vma_chain)))
+		//list_is_singular(&vma->anon_vma_chain)))
+		lockfree_list_is_singular(&vma->anon_vma_chain)))
 		return 1;
 	return anon_vma1 == anon_vma2;
 }
@@ -1184,7 +1166,8 @@ static struct anon_vma *reusable_anon_vma(struct vm_area_struct *old, struct vm_
 	if (anon_vma_compatible(a, b)) {
 		struct anon_vma *anon_vma = READ_ONCE(old->anon_vma);
 
-		if (anon_vma && list_is_singular(&old->anon_vma_chain))
+		//if (anon_vma && list_is_singular(&old->anon_vma_chain))
+		if (anon_vma && lockfree_list_is_singular(&old->anon_vma_chain))
 			return anon_vma;
 	}
 	return NULL;
@@ -1604,7 +1587,9 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 	vma->vm_flags = vm_flags;
 	vma->vm_page_prot = vm_get_page_prot(vm_flags);
 	vma->vm_pgoff = pgoff;
-	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	//INIT_LIST_HEAD(&vma->anon_vma_chain);
+	init_lockfree_list_head(&vma->anon_vma_chain, &vma->anon_vma_chain_head_node,
+			&vma->anon_vma_chain_tail_node);
 
 	if (file) {
 		if (vm_flags & VM_DENYWRITE) {
@@ -2197,9 +2182,7 @@ int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 				if (vma->vm_flags & VM_LOCKED)
 					mm->locked_vm += grow;
 				vm_stat_account(mm, vma->vm_flags, grow);
-				anon_vma_interval_tree_pre_update_vma(vma);
 				vma->vm_end = address;
-				anon_vma_interval_tree_post_update_vma(vma);
 				if (vma->vm_next)
 					vma_gap_update(vma->vm_next);
 				else
@@ -2268,10 +2251,8 @@ int expand_downwards(struct vm_area_struct *vma,
 				if (vma->vm_flags & VM_LOCKED)
 					mm->locked_vm += grow;
 				vm_stat_account(mm, vma->vm_flags, grow);
-				anon_vma_interval_tree_pre_update_vma(vma);
 				vma->vm_start = address;
 				vma->vm_pgoff -= grow;
-				anon_vma_interval_tree_post_update_vma(vma);
 				vma_gap_update(vma);
 				spin_unlock(&mm->page_table_lock);
 
@@ -2386,6 +2367,7 @@ static void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)
 	} while (vma);
 	vm_unacct_memory(nr_accounted);
 	validate_mm(mm);
+	schedule_work(&i_mmap_free_wq);
 }
 
 /*
@@ -2461,7 +2443,9 @@ static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 	/* most fields are the same, copy all, and then fixup */
 	*new = *vma;
 
-	INIT_LIST_HEAD(&new->anon_vma_chain);
+	//INIT_LIST_HEAD(&new->anon_vma_chain);
+	init_lockfree_list_head(&new->anon_vma_chain, &new->anon_vma_chain_head_node,
+			&new->anon_vma_chain_tail_node);
 
 	if (new_below)
 		new->vm_end = addr;
@@ -2799,7 +2783,9 @@ static unsigned long do_brk(unsigned long addr, unsigned long len)
 		return -ENOMEM;
 	}
 
-	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	//INIT_LIST_HEAD(&vma->anon_vma_chain);
+	init_lockfree_list_head(&vma->anon_vma_chain, &vma->anon_vma_chain_head_node,
+			&vma->anon_vma_chain_tail_node);
 	vma->vm_mm = mm;
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
@@ -2877,6 +2863,7 @@ void exit_mmap(struct mm_struct *mm)
 			nr_accounted += vma_pages(vma);
 		vma = remove_vma(vma);
 	}
+	schedule_work(&i_mmap_free_wq);
 	vm_unacct_memory(nr_accounted);
 }
 
@@ -2978,7 +2965,10 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 		new_vma->vm_pgoff = pgoff;
 		if (vma_dup_policy(vma, new_vma))
 			goto out_free_vma;
-		INIT_LIST_HEAD(&new_vma->anon_vma_chain);
+		//INIT_LIST_HEAD(&new_vma->anon_vma_chain);
+		init_lockfree_list_head(&new_vma->anon_vma_chain, &new_vma->anon_vma_chain_head_node,
+				&new_vma->anon_vma_chain_tail_node);
+
 		if (anon_vma_clone(new_vma, vma))
 			goto out_free_mempol;
 		if (new_vma->vm_file)
@@ -3098,7 +3088,9 @@ static struct vm_area_struct *__install_special_mapping(
 	if (unlikely(vma == NULL))
 		return ERR_PTR(-ENOMEM);
 
-	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	init_lockfree_list_head(&vma->anon_vma_chain, &vma->anon_vma_chain_head_node,
+			&vma->anon_vma_chain_tail_node);
+	//INIT_LIST_HEAD(&vma->anon_vma_chain);
 	vma->vm_mm = mm;
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
@@ -3157,7 +3149,7 @@ static DEFINE_MUTEX(mm_all_locks_mutex);
 
 static void vm_lock_anon_vma(struct mm_struct *mm, struct anon_vma *anon_vma)
 {
-	if (!test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_node)) {
+	if (!test_bit(0, (unsigned long *) &anon_vma->root->head_node.next)) {
 		/*
 		 * The LSB of head.next can't change from under us
 		 * because we hold the mm_all_locks_mutex.
@@ -3173,7 +3165,7 @@ static void vm_lock_anon_vma(struct mm_struct *mm, struct anon_vma *anon_vma)
 		 * anon_vma->root->rwsem.
 		 */
 		if (__test_and_set_bit(0, (unsigned long *)
-				       &anon_vma->root->rb_root.rb_node))
+				&anon_vma->root->head_node.next))
 			BUG();
 	}
 }
@@ -3261,9 +3253,18 @@ int mm_take_all_locks(struct mm_struct *mm)
 	for (vma = mm->mmap; vma; vma = vma->vm_next) {
 		if (signal_pending(current))
 			goto out_unlock;
-		if (vma->anon_vma)
-			list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
+		if (vma->anon_vma) {
+			struct lockfree_list_node *node = (struct lockfree_list_node *)get_unmarked_ref((long)vma->anon_vma_chain_head_node.next);
+			struct lockfree_list_node *onode = vma->anon_vma_chain_head_node.next;
+
+			lockfree_list_for_each_entry(avc, node, same_vma, onode) {
+				if (&avc->same_vma == &vma->anon_vma_chain_tail_node)
+					break;
+				if (is_marked_ref((long)onode))
+					continue;
 				vm_lock_anon_vma(mm, avc->anon_vma);
+			}
+		}
 	}
 
 	return 0;
@@ -3275,7 +3276,7 @@ out_unlock:
 
 static void vm_unlock_anon_vma(struct anon_vma *anon_vma)
 {
-	if (test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_node)) {
+	if (test_bit(0, (unsigned long *) &anon_vma->root->head_node.next)) {
 		/*
 		 * The LSB of head.next can't change to 0 from under
 		 * us because we hold the mm_all_locks_mutex.
@@ -3289,7 +3290,7 @@ static void vm_unlock_anon_vma(struct anon_vma *anon_vma)
 		 * anon_vma->root->rwsem.
 		 */
 		if (!__test_and_clear_bit(0, (unsigned long *)
-					  &anon_vma->root->rb_root.rb_node))
+				&anon_vma->root->head_node.next))
 			BUG();
 		anon_vma_unlock_write(anon_vma);
 	}
@@ -3322,9 +3323,17 @@ void mm_drop_all_locks(struct mm_struct *mm)
 	BUG_ON(!mutex_is_locked(&mm_all_locks_mutex));
 
 	for (vma = mm->mmap; vma; vma = vma->vm_next) {
-		if (vma->anon_vma)
-			list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
+		if (vma->anon_vma) {
+			struct lockfree_list_node *node = (struct lockfree_list_node *)get_unmarked_ref((long)vma->anon_vma_chain_head_node.next);
+			struct lockfree_list_node *onode = vma->anon_vma_chain_head_node.next;
+			lockfree_list_for_each_entry(avc, node, same_vma, onode) {
+				if (&avc->same_vma == &vma->anon_vma_chain_tail_node)
+					break;
+				if (is_marked_ref((long)onode))
+					continue;
 				vm_unlock_anon_vma(avc->anon_vma);
+			}
+		}
 		if (vma->vm_file && vma->vm_file->f_mapping)
 			vm_unlock_mapping(vma->vm_file->f_mapping);
 	}
@@ -3454,3 +3463,11 @@ static int __meminit init_reserve_notifier(void)
 	return 0;
 }
 subsys_initcall(init_reserve_notifier);
+
+static int __init i_mmap_init_wq(void)
+{
+	pr_info("i_mmap work queue initialize");
+	INIT_WORK(&i_mmap_free_wq, i_mmap_free_work_func);
+	return 0;
+}
+__initcall(i_mmap_init_wq);
diff --git a/mm/rmap.c b/mm/rmap.c
index 79f3bf0..c6fe43d 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -60,6 +60,10 @@
 #include <linux/migrate.h>
 #include <linux/hugetlb.h>
 #include <linux/backing-dev.h>
+#include <linux/lockfree_list.h>
+#include <linux/llist.h>
+#include <linux/workqueue.h>
+
 #include <linux/page_idle.h>
 
 #include <asm/tlbflush.h>
@@ -71,6 +75,9 @@
 static struct kmem_cache *anon_vma_cachep;
 static struct kmem_cache *anon_vma_chain_cachep;
 
+static struct work_struct anon_vma_free_wq;
+static LLIST_HEAD(anon_vma_freelist);
+
 static inline struct anon_vma *anon_vma_alloc(void)
 {
 	struct anon_vma *anon_vma;
@@ -78,7 +85,6 @@ static inline struct anon_vma *anon_vma_alloc(void)
 	anon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);
 	if (anon_vma) {
 		atomic_set(&anon_vma->refcount, 1);
-		anon_vma->degree = 1;	/* Reference for first vma */
 		anon_vma->parent = anon_vma;
 		/*
 		 * Initialise the anon_vma root to point to itself. If called
@@ -136,8 +142,12 @@ static void anon_vma_chain_link(struct vm_area_struct *vma,
 {
 	avc->vma = vma;
 	avc->anon_vma = anon_vma;
-	list_add(&avc->same_vma, &vma->anon_vma_chain);
-	anon_vma_interval_tree_insert(avc, &anon_vma->rb_root);
+	LOCKFREE_LIST_SAVE_KEY(avc, same_vma);
+	LOCKFREE_LIST_CLEAR_GC(avc, same_vma);
+	lockfree_list_add(&avc->same_vma, &vma->anon_vma_chain);
+	LOCKFREE_LIST_SAVE_KEY(avc, same_anon_vma);
+	LOCKFREE_LIST_CLEAR_GC(avc, same_anon_vma);
+	lockfree_list_add(&avc->same_anon_vma, &anon_vma->head);
 }
 
 /**
@@ -197,7 +207,6 @@ int anon_vma_prepare(struct vm_area_struct *vma)
 			vma->anon_vma = anon_vma;
 			anon_vma_chain_link(vma, avc, anon_vma);
 			/* vma reference or self-parent link for new root */
-			anon_vma->degree++;
 			allocated = NULL;
 			avc = NULL;
 		}
@@ -259,37 +268,31 @@ int anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)
 {
 	struct anon_vma_chain *avc, *pavc;
 	struct anon_vma *root = NULL;
+	struct lockfree_list_node *node = (struct lockfree_list_node *)get_unmarked_ref((long)src->anon_vma_chain_head_node.next);
+	struct lockfree_list_node *onode = src->anon_vma_chain_head_node.next;
 
-	list_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {
+	lockfree_list_for_each_entry(pavc, node, same_vma, onode) {
 		struct anon_vma *anon_vma;
+		if (&pavc->same_vma == &src->anon_vma_chain_tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
+
 
 		avc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);
 		if (unlikely(!avc)) {
-			unlock_anon_vma_root(root);
+			//unlock_anon_vma_root(root);
 			root = NULL;
 			avc = anon_vma_chain_alloc(GFP_KERNEL);
 			if (!avc)
 				goto enomem_failure;
 		}
 		anon_vma = pavc->anon_vma;
-		root = lock_anon_vma_root(root, anon_vma);
+		//root = lock_anon_vma_root(root, anon_vma);
 		anon_vma_chain_link(dst, avc, anon_vma);
 
-		/*
-		 * Reuse existing anon_vma if its degree lower than two,
-		 * that means it has no vma and only one anon_vma child.
-		 *
-		 * Do not chose parent anon_vma, otherwise first child
-		 * will always reuse it. Root anon_vma is never reused:
-		 * it has self-parent reference and at least one child.
-		 */
-		if (!dst->anon_vma && anon_vma != src->anon_vma &&
-				anon_vma->degree < 2)
-			dst->anon_vma = anon_vma;
 	}
-	if (dst->anon_vma)
-		dst->anon_vma->degree++;
-	unlock_anon_vma_root(root);
+	//unlock_anon_vma_root(root);
 	return 0;
 
  enomem_failure:
@@ -315,7 +318,6 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 	struct anon_vma *anon_vma;
 	int error;
 
-	/* Don't bother if the parent process has no anon_vma here. */
 	if (!pvma->anon_vma)
 		return 0;
 
@@ -330,10 +332,6 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 	if (error)
 		return error;
 
-	/* An existing anon_vma has been reused, all done then. */
-	if (vma->anon_vma)
-		return 0;
-
 	/* Then add our own anon_vma. */
 	anon_vma = anon_vma_alloc();
 	if (!anon_vma)
@@ -356,10 +354,9 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 	get_anon_vma(anon_vma->root);
 	/* Mark this anon_vma as the one where our new (COWed) pages go. */
 	vma->anon_vma = anon_vma;
-	anon_vma_lock_write(anon_vma);
+	//anon_vma_lock_write(anon_vma);
 	anon_vma_chain_link(vma, avc, anon_vma);
-	anon_vma->parent->degree++;
-	anon_vma_unlock_write(anon_vma);
+	//anon_vma_unlock_write(anon_vma);
 
 	return 0;
 
@@ -370,60 +367,98 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 	return -ENOMEM;
 }
 
+static void anon_vma_free_work_func(struct work_struct *w)
+{
+	struct llist_node *entry;
+	struct anon_vma_chain *avc, *avc_next;
+
+	if (llist_empty(&anon_vma_freelist))
+		return;
+
+	entry = llist_del_all(&anon_vma_freelist);
+	llist_for_each_entry_safe(avc, avc_next, entry, llnode) {
+		if (avc->same_vma.garbage && avc->same_anon_vma.garbage)
+			anon_vma_chain_free(avc);
+		else
+			llist_add(&avc->llnode, &anon_vma_freelist);
+	}
+}
+
+
 void unlink_anon_vmas(struct vm_area_struct *vma)
 {
 	struct anon_vma_chain *avc, *next;
 	struct anon_vma *root = NULL;
+	struct lockfree_list_node *node = (struct lockfree_list_node *)get_unmarked_ref((long)vma->anon_vma_chain_head_node.next);
+	struct lockfree_list_node *onode = vma->anon_vma_chain_head_node.next;
 
 	/*
 	 * Unlink each anon_vma chained to the VMA.  This list is ordered
 	 * from newest to oldest, ensuring the root anon_vma gets freed last.
 	 */
-	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
+	lockfree_list_for_each_entry_safe(avc, next, node, same_vma, onode) {
 		struct anon_vma *anon_vma = avc->anon_vma;
 
-		root = lock_anon_vma_root(root, anon_vma);
-		anon_vma_interval_tree_remove(avc, &anon_vma->rb_root);
+		if (&avc->same_vma == &vma->anon_vma_chain_tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
+
+		//root = lock_anon_vma_root(root, anon_vma);
+		lockfree_list_del(&avc->same_anon_vma, &anon_vma->head);
 
 		/*
 		 * Leave empty anon_vmas on the list - we'll need
 		 * to free them outside the lock.
 		 */
-		if (RB_EMPTY_ROOT(&anon_vma->rb_root)) {
-			anon_vma->parent->degree--;
+		if (lockfree_list_empty(&anon_vma->head)) {
 			continue;
 		}
 
-		list_del(&avc->same_vma);
-		anon_vma_chain_free(avc);
+		lockfree_list_del(&avc->same_vma, &vma->anon_vma_chain);
+		/* Need for add avc to llist */
+		/* Add global delayed free list */
+		//anon_vma_chain_free(avc);
+		llist_add(&avc->llnode, &anon_vma_freelist);
 	}
-	if (vma->anon_vma)
-		vma->anon_vma->degree--;
-	unlock_anon_vma_root(root);
 
+	// unlock_anon_vma_root(root);
+	node = (struct lockfree_list_node *)get_unmarked_ref((long)vma->anon_vma_chain_head_node.next);
+	onode = vma->anon_vma_chain_head_node.next;
 	/*
 	 * Iterate the list once more, it now only contains empty and unlinked
 	 * anon_vmas, destroy them. Could not do before due to __put_anon_vma()
 	 * needing to write-acquire the anon_vma->root->rwsem.
 	 */
-	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
-		struct anon_vma *anon_vma = avc->anon_vma;
-
-		BUG_ON(anon_vma->degree);
+	lockfree_list_for_each_entry_safe(avc, next, node, same_vma, onode) {
+		struct anon_vma *anon_vma;
+		if (&avc->same_vma == &vma->anon_vma_chain_tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
+		anon_vma = avc->anon_vma;
 		put_anon_vma(anon_vma);
 
-		list_del(&avc->same_vma);
-		anon_vma_chain_free(avc);
+		lockfree_list_del(&avc->same_vma, &vma->anon_vma_chain);
+		//anon_vma_chain_free(avc);
+		llist_add(&avc->llnode, &anon_vma_freelist);
 	}
 }
 
+void free_anon_vma_chain(void)
+{
+	/* call for delayed free */
+	schedule_work(&anon_vma_free_wq);
+}
+
 static void anon_vma_ctor(void *data)
 {
 	struct anon_vma *anon_vma = data;
 
 	init_rwsem(&anon_vma->rwsem);
 	atomic_set(&anon_vma->refcount, 0);
-	anon_vma->rb_root = RB_ROOT;
+	init_lockfree_list_head(&anon_vma->head, &anon_vma->head_node,
+			&anon_vma->tail_node);
 }
 
 void __init anon_vma_init(void)
@@ -1722,18 +1757,30 @@ static struct anon_vma *rmap_walk_anon_lock(struct page *page,
 static int rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc)
 {
 	struct anon_vma *anon_vma;
-	pgoff_t pgoff;
 	struct anon_vma_chain *avc;
 	int ret = SWAP_AGAIN;
+	struct lockfree_list_node *node;
+	struct lockfree_list_node *onode;
 
 	anon_vma = rmap_walk_anon_lock(page, rwc);
 	if (!anon_vma)
 		return ret;
 
-	pgoff = page_to_pgoff(page);
-	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
-		struct vm_area_struct *vma = avc->vma;
-		unsigned long address = vma_address(page, vma);
+	node = (struct lockfree_list_node *)get_unmarked_ref((long)anon_vma->head_node.next);
+	onode = anon_vma->head_node.next;
+
+
+	lockfree_list_for_each_entry(avc, node, same_anon_vma, onode) {
+		struct vm_area_struct *vma;
+		unsigned long address;
+
+		if (&avc->same_anon_vma == &anon_vma->tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
+
+		vma = avc->vma;
+		address = vma_address(page, vma);
 
 		cond_resched();
 
@@ -1769,6 +1816,8 @@ static int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc)
 	pgoff_t pgoff;
 	struct vm_area_struct *vma;
 	int ret = SWAP_AGAIN;
+	struct lockfree_list_node *node;
+	struct lockfree_list_node *onode;
 
 	/*
 	 * The page lock not only makes sure that page->mapping cannot
@@ -1783,8 +1832,18 @@ static int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc)
 
 	pgoff = page_to_pgoff(page);
 	i_mmap_lock_read(mapping);
-	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
-		unsigned long address = vma_address(page, vma);
+
+	node = (struct lockfree_list_node *)get_unmarked_ref((long)mapping->i_mmap_head_node.next);
+	onode = mapping->i_mmap_head_node.next;
+
+	pr_debug("i_mmap read lock : %s\n", __func__);
+	lockfree_list_for_each_entry(vma, node, shared.linear, onode) {
+		unsigned long address;
+		if (&vma->shared.linear == &mapping->i_mmap_tail_node)
+			break;
+		if (is_marked_ref((long)onode))
+			continue;
+		address = vma_address(page, vma);
 
 		cond_resched();
 
@@ -1813,6 +1872,14 @@ int rmap_walk(struct page *page, struct rmap_walk_control *rwc)
 		return rmap_walk_file(page, rwc);
 }
 
+static int __init anon_vma_init_wq(void)
+{
+	pr_info("anon_vma work queue initialize");
+	INIT_WORK(&anon_vma_free_wq, anon_vma_free_work_func);
+	return 0;
+}
+__initcall(anon_vma_init_wq);
+
 #ifdef CONFIG_HUGETLB_PAGE
 /*
  * The following three functions are for anonymous (private mapped) hugepages.
-- 
2.7.4

