From 26869bb85eed9303d1f72b5b944d2afdd4d3aa6b Mon Sep 17 00:00:00 2001
From: joohyun kyong <joohyun0115@gmail.com>
Date: Wed, 19 Oct 2016 11:47:51 +0900
Subject: [PATCH] LDU: enable per-core queue version of the LDU

---
 fs/exec.c                |   3 +-
 fs/hugetlbfs/inode.c     |   3 +
 fs/inode.c               |  21 ++-
 include/linux/fs.h       |  18 +++
 include/linux/ldu.h      |  60 +++++++
 include/linux/mm.h       |   1 +
 include/linux/mm_types.h |   4 +
 include/linux/rmap.h     |  20 ++-
 kernel/events/uprobes.c  |   7 +-
 kernel/fork.c            |  17 +-
 mm/huge_memory.c         |  12 +-
 mm/hugetlb.c             |   4 +
 mm/ksm.c                 |  13 +-
 mm/memory-failure.c      |   7 +-
 mm/memory.c              |   2 +
 mm/migrate.c             |   3 +
 mm/mlock.c               |   2 +-
 mm/mmap.c                | 409 ++++++++++++++++++++++++++++++++++++++++-------
 mm/mremap.c              |  17 +-
 mm/rmap.c                | 345 +++++++++++++++++++++++++++++++--------
 20 files changed, 807 insertions(+), 161 deletions(-)
 create mode 100644 include/linux/ldu.h

diff --git a/fs/exec.c b/fs/exec.c
index dcd4ac7..dd5a46a 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -277,6 +277,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 	vma->vm_flags = VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP;
 	vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	memset(&vma->dnode, 0, sizeof(vma->dnode));
 
 	err = insert_vm_struct(mm, vma);
 	if (err)
@@ -290,7 +291,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 err:
 	up_write(&mm->mmap_sem);
 	bprm->vma = NULL;
-	kmem_cache_free(vm_area_cachep, vma);
+	free_vma(vma);
 	return err;
 }
 
diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c
index e1f465a..24b7f40 100644
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@ -446,6 +446,7 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,
 				BUG_ON(truncate_op);
 
 				i_mmap_lock_write(mapping);
+				synchronize_ldu_i_mmap(mapping);
 				hugetlb_vmdelete_list(&mapping->i_mmap,
 					next * pages_per_huge_page(h),
 					(next + 1) * pages_per_huge_page(h));
@@ -507,6 +508,7 @@ static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)
 
 	i_size_write(inode, offset);
 	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	if (!RB_EMPTY_ROOT(&mapping->i_mmap))
 		hugetlb_vmdelete_list(&mapping->i_mmap, pgoff, 0);
 	i_mmap_unlock_write(mapping);
@@ -532,6 +534,7 @@ static long hugetlbfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 
 		inode_lock(inode);
 		i_mmap_lock_write(mapping);
+		synchronize_ldu_i_mmap(mapping);
 		if (!RB_EMPTY_ROOT(&mapping->i_mmap))
 			hugetlb_vmdelete_list(&mapping->i_mmap,
 						hole_start >> PAGE_SHIFT,
diff --git a/fs/inode.c b/fs/inode.c
index 69b8b52..08ebbed 100644
--- a/fs/inode.c
+++ b/fs/inode.c
@@ -253,10 +253,23 @@ static void i_callback(struct rcu_head *head)
 	kmem_cache_free(inode_cachep, inode);
 }
 
+extern void clean_percore_mapping(struct address_space *mapping);
+
 static void destroy_inode(struct inode *inode)
 {
 	BUG_ON(!list_empty(&inode->i_lru));
-	__destroy_inode(inode);
+	if (inode->i_mapping) {
+		clean_percore_mapping(inode->i_mapping);
+		__destroy_inode(inode);
+		might_sleep();
+		if (rwsem_is_locked(&inode->i_mapping->i_mmap_rwsem)) {
+			down_write(&inode->i_mapping->i_mmap_rwsem);
+			up_write(&inode->i_mapping->i_mmap_rwsem);
+		}
+	} else {
+		__destroy_inode(inode);
+	}
+
 	if (inode->i_sb->s_op->destroy_inode)
 		inode->i_sb->s_op->destroy_inode(inode);
 	else
@@ -341,6 +354,8 @@ void inc_nlink(struct inode *inode)
 }
 EXPORT_SYMBOL(inc_nlink);
 
+extern void i_mmap_free_work_func(struct work_struct *w);
+
 void address_space_init_once(struct address_space *mapping)
 {
 	memset(mapping, 0, sizeof(*mapping));
@@ -350,6 +365,7 @@ void address_space_init_once(struct address_space *mapping)
 	INIT_LIST_HEAD(&mapping->private_list);
 	spin_lock_init(&mapping->private_lock);
 	mapping->i_mmap = RB_ROOT;
+	i_mmap_init_ldu_head(&mapping->lduh);
 }
 EXPORT_SYMBOL(address_space_init_once);
 
@@ -1909,6 +1925,9 @@ void __init inode_init(void)
 
 	for (loop = 0; loop < (1U << i_hash_shift); loop++)
 		INIT_HLIST_HEAD(&inode_hashtable[loop]);
+
+
+
 }
 
 void init_special_inode(struct inode *inode, umode_t mode, dev_t rdev)
diff --git a/include/linux/fs.h b/include/linux/fs.h
index ae68100..221dd44 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -35,6 +35,7 @@
 
 #include <asm/byteorder.h>
 #include <uapi/linux/fs.h>
+#include <linux/ldu.h>
 
 struct backing_dev_info;
 struct bdi_writeback;
@@ -430,6 +431,8 @@ struct address_space {
 	spinlock_t		tree_lock;	/* and lock protecting it */
 	atomic_t		i_mmap_writable;/* count VM_SHARED mappings */
 	struct rb_root		i_mmap;		/* tree of private and shared mappings */
+	struct ldu_head  lduh;
+	struct llist_head llclean;
 	struct rw_semaphore	i_mmap_rwsem;	/* protect tree, count, list */
 	/* Protected by tree_lock together with the radix tree */
 	unsigned long		nrpages;	/* number of total pages */
@@ -516,11 +519,21 @@ static inline void i_mmap_unlock_read(struct address_space *mapping)
 	up_read(&mapping->i_mmap_rwsem);
 }
 
+void synchronize_ldu_i_mmap(struct address_space *mapping);
+bool ldu_logical_update(struct address_space *mapping, struct ldu_node *dnode);
+bool ldu_logical_insert(struct vm_area_struct *vma, struct address_space *mapping);
+bool ldu_logical_remove(struct vm_area_struct *vma, struct address_space *mapping);
+
+
 /*
  * Might pages of this file be mapped into userspace?
  */
 static inline int mapping_mapped(struct address_space *mapping)
 {
+	down_write(&mapping->i_mmap_rwsem);
+	synchronize_ldu_i_mmap(mapping);
+	up_write(&mapping->i_mmap_rwsem);
+
 	return	!RB_EMPTY_ROOT(&mapping->i_mmap);
 }
 
@@ -679,6 +692,7 @@ struct inode {
 #endif
 
 	void			*i_private; /* fs or device private pointer */
+	struct llist_node llist;
 };
 
 static inline int inode_unhashed(struct inode *inode)
@@ -2533,6 +2547,10 @@ static inline void file_end_write(struct file *file)
  */
 static inline int get_write_access(struct inode *inode)
 {
+	down_write(&inode->i_mapping->i_mmap_rwsem);
+	synchronize_ldu_i_mmap(inode->i_mapping);
+	up_write(&inode->i_mapping->i_mmap_rwsem);
+
 	return atomic_inc_unless_negative(&inode->i_writecount) ? 0 : -ETXTBSY;
 }
 static inline int deny_write_access(struct file *file)
diff --git a/include/linux/ldu.h b/include/linux/ldu.h
new file mode 100644
index 0000000..ead0bd4
--- /dev/null
+++ b/include/linux/ldu.h
@@ -0,0 +1,60 @@
+#ifndef __LINUX_DEFERABLE_UPDATE
+#define __LINUX_DEFERABLE_UPDATE
+
+#include <linux/llist.h>
+#include <linux/mutex.h>
+
+#define LDU_LINKED_LIST     0
+#define LDU_INTERVAL_TREE   1
+
+#define LDU_OP_ADD 1
+#define LDU_OP_DEL 2
+
+struct ldu_head {
+	struct delayed_work sync;
+	struct llist_head ll_head;
+};
+
+struct ldu_node {
+	void *key;
+	void *key2;
+	int mark;
+	int op_num;
+	struct rb_root *root;
+	struct llist_node ll_node;
+};
+
+struct ldu_anon_node {
+	unsigned long used;
+	struct ldu_node node[2]; /* 0 : add op, 1 : del op */
+};
+
+struct ldu_i_mmap_node {
+	unsigned long used;
+	struct ldu_node node[2]; /* 0 : add op, 1 : del op */
+};
+
+
+struct pldu_deferred {
+	struct llist_head list;
+	struct delayed_work wq;
+};
+
+
+
+
+
+static inline void anon_vma_init_ldu_head(struct ldu_head *dp)
+{
+	init_llist_head(&dp->ll_head);
+}
+
+
+void i_mmap_ldu_physical_update(int op, struct vm_area_struct *vma, struct rb_root *root);
+static inline void i_mmap_init_ldu_head(struct ldu_head *dp)
+{
+	init_llist_head(&dp->ll_head);
+}
+
+
+#endif /* __LINUX_DEFERABLE_UPDATE */
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 516e149..d743269 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -112,6 +112,7 @@ extern int overcommit_kbytes_handler(struct ctl_table *, int, void __user *,
  */
 
 extern struct kmem_cache *vm_area_cachep;
+extern void free_vma(struct vm_area_struct *vma);
 
 #ifndef CONFIG_MMU
 extern struct rb_root nommu_region_tree;
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 624b78b..fc88178 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -14,6 +14,7 @@
 #include <linux/page-flags-layout.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
+#include <linux/ldu.h>
 
 #ifndef AT_VECTOR_SIZE_ARCH
 #define AT_VECTOR_SIZE_ARCH 0
@@ -326,6 +327,9 @@ struct vm_area_struct {
 		unsigned long rb_subtree_last;
 	} shared;
 
+	struct ldu_i_mmap_node dnode;
+	struct llist_node llist;
+
 	/*
 	 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
 	 * list, after a COW of one of the file pages.	A MAP_SHARED vma
diff --git a/include/linux/rmap.h b/include/linux/rmap.h
index a07f42b..a3abe4e 100644
--- a/include/linux/rmap.h
+++ b/include/linux/rmap.h
@@ -9,6 +9,7 @@
 #include <linux/mm.h>
 #include <linux/rwsem.h>
 #include <linux/memcontrol.h>
+#include <linux/ldu.h>
 
 /*
  * The anon_vma heads a list of private "related" vmas, to scan if
@@ -35,14 +36,7 @@ struct anon_vma {
 	 * anon_vma if they are the last user on release
 	 */
 	atomic_t refcount;
-
-	/*
-	 * Count of child anon_vmas and VMAs which points to this anon_vma.
-	 *
-	 * This counter is used for making decision about reusing anon_vma
-	 * instead of forking new one. See comments in function anon_vma_clone.
-	 */
-	unsigned degree;
+	atomic_t refcount_free;
 
 	struct anon_vma *parent;	/* Parent of this anon_vma */
 
@@ -55,6 +49,9 @@ struct anon_vma {
 	 * mm_take_all_locks() (mm_all_locks_mutex).
 	 */
 	struct rb_root rb_root;	/* Interval tree of private "related" vmas */
+	struct ldu_head  lduh;
+	struct llist_node llist;
+	struct llist_head llclean;
 };
 
 /*
@@ -75,7 +72,10 @@ struct anon_vma_chain {
 	struct anon_vma *anon_vma;
 	struct list_head same_vma;   /* locked by mmap_sem & page_table_lock */
 	struct rb_node rb;			/* locked by anon_vma->rwsem */
+	struct ldu_anon_node dnode;
+	struct llist_node llist;
 	unsigned long rb_subtree_last;
+	unsigned int garbage;
 #ifdef CONFIG_DEBUG_VM_RB
 	unsigned long cached_vma_start, cached_vma_last;
 #endif
@@ -95,6 +95,10 @@ enum ttu_flags {
 					 * do a final flush if necessary */
 };
 
+void synchronize_ldu_anon(struct anon_vma *anon_vma);
+void anon_vma_global_lock(void);
+void anon_vma_global_unlock(void);
+
 #ifdef CONFIG_MMU
 static inline void get_anon_vma(struct anon_vma *anon_vma)
 {
diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c
index 0167679..1bc4ea4 100644
--- a/kernel/events/uprobes.c
+++ b/kernel/events/uprobes.c
@@ -37,6 +37,7 @@
 #include <linux/percpu-rwsem.h>
 #include <linux/task_work.h>
 #include <linux/shmem_fs.h>
+#include <linux/ldu.h>
 
 #include <linux/uprobes.h>
 
@@ -719,7 +720,9 @@ build_map_info(struct address_space *mapping, loff_t offset, bool is_register)
 	int more = 0;
 
  again:
-	i_mmap_lock_read(mapping);
+	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
+
 	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
 		if (!valid_vma(vma, is_register))
 			continue;
@@ -750,7 +753,7 @@ build_map_info(struct address_space *mapping, loff_t offset, bool is_register)
 		info->mm = vma->vm_mm;
 		info->vaddr = offset_to_vaddr(vma, offset);
 	}
-	i_mmap_unlock_read(mapping);
+	i_mmap_unlock_write(mapping);
 
 	if (!more)
 		goto out;
diff --git a/kernel/fork.c b/kernel/fork.c
index 2e391c7..b9bb53d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -394,6 +394,10 @@ free_tsk:
 }
 
 #ifdef CONFIG_MMU
+extern void free_vma(struct vm_area_struct *vma);
+bool i_mmap_ldu_logical_insert(struct vm_area_struct *vma,
+		struct address_space *mapping);
+
 static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 {
 	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
@@ -460,6 +464,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		tmp->vm_next = tmp->vm_prev = NULL;
 		tmp->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
 		file = tmp->vm_file;
+		memset(&tmp->dnode, 0, sizeof(tmp->dnode));
 		if (file) {
 			struct inode *inode = file_inode(file);
 			struct address_space *mapping = file->f_mapping;
@@ -467,15 +472,17 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			get_file(file);
 			if (tmp->vm_flags & VM_DENYWRITE)
 				atomic_dec(&inode->i_writecount);
-			i_mmap_lock_write(mapping);
+			//i_mmap_lock_write(mapping);
 			if (tmp->vm_flags & VM_SHARED)
 				atomic_inc(&mapping->i_mmap_writable);
 			flush_dcache_mmap_lock(mapping);
 			/* insert tmp into the share list, just after mpnt */
-			vma_interval_tree_insert_after(tmp, mpnt,
-					&mapping->i_mmap);
+			//vma_interval_tree_insert_after(tmp, mpnt,
+			//      &mapping->i_mmap);
+			i_mmap_ldu_logical_insert(tmp, mapping);
+
 			flush_dcache_mmap_unlock(mapping);
-			i_mmap_unlock_write(mapping);
+			//i_mmap_unlock_write(mapping);
 		}
 
 		/*
@@ -519,7 +526,7 @@ out:
 fail_nomem_anon_vma_fork:
 	mpol_put(vma_policy(tmp));
 fail_nomem_policy:
-	kmem_cache_free(vm_area_cachep, tmp);
+	free_vma(tmp);
 fail_nomem:
 	retval = -ENOMEM;
 	vm_unacct_memory(charge);
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index e10a4fe..47aeb5d 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1496,6 +1496,8 @@ int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	get_page(page);
 	spin_unlock(ptl);
 	anon_vma = page_lock_anon_vma_read(page);
+	if (anon_vma)
+		synchronize_ldu_anon(anon_vma);
 
 	/* Confirm the PMD did not change while page_table_lock was released */
 	spin_lock(ptl);
@@ -1541,8 +1543,10 @@ out_unlock:
 	spin_unlock(ptl);
 
 out:
-	if (anon_vma)
+	if (anon_vma) {
+		anon_vma_global_unlock();
 		page_unlock_anon_vma_read(anon_vma);
+	}
 
 	if (page_nid != -1)
 		task_numa_fault(last_cpupid, page_nid, HPAGE_PMD_NR, flags);
@@ -2383,6 +2387,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 	}
 
 	anon_vma_lock_write(vma->anon_vma);
+	synchronize_ldu_anon(vma->anon_vma);
 
 	pte = pte_offset_map(pmd, address);
 	pte_ptl = pte_lockptr(mm, pmd);
@@ -2416,6 +2421,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 		 */
 		pmd_populate(mm, pmd, pmd_pgtable(_pmd));
 		spin_unlock(pmd_ptl);
+		anon_vma_global_unlock();
 		anon_vma_unlock_write(vma->anon_vma);
 		result = SCAN_FAIL;
 		goto out;
@@ -2425,6 +2431,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 	 * All pages are isolated and locked so anon_vma rmap
 	 * can't run anymore.
 	 */
+	anon_vma_global_unlock();
 	anon_vma_unlock_write(vma->anon_vma);
 
 	__collapse_huge_page_copy(pte, new_page, vma, address, pte_ptl);
@@ -3385,7 +3392,9 @@ int split_huge_page_to_list(struct page *page, struct list_head *list)
 		ret = -EBUSY;
 		goto out;
 	}
+
 	anon_vma_lock_write(anon_vma);
+	synchronize_ldu_anon(anon_vma);
 
 	/*
 	 * Racy check if we can split the page, before freeze_page() will
@@ -3431,6 +3440,7 @@ int split_huge_page_to_list(struct page *page, struct list_head *list)
 	}
 
 out_unlock:
+	anon_vma_global_unlock();
 	anon_vma_unlock_write(anon_vma);
 	put_anon_vma(anon_vma);
 out:
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 01f2b48..d49c77c 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -23,6 +23,7 @@
 #include <linux/swapops.h>
 #include <linux/page-isolation.h>
 #include <linux/jhash.h>
+#include <linux/ldu.h>
 
 #include <asm/page.h>
 #include <asm/pgtable.h>
@@ -3280,6 +3281,7 @@ static void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,
 	 * __unmap_hugepage_range() is called as the lock is already held
 	 */
 	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	vma_interval_tree_foreach(iter_vma, &mapping->i_mmap, pgoff, pgoff) {
 		/* Do not unmap the current VMA */
 		if (iter_vma == vma)
@@ -3909,6 +3911,7 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 
 	mmu_notifier_invalidate_range_start(mm, start, end);
 	i_mmap_lock_write(vma->vm_file->f_mapping);
+	//synchronize_ldu_i_mmap(vma->vm_file->f_mapping);
 	for (; address < end; address += huge_page_size(h)) {
 		spinlock_t *ptl;
 		ptep = huge_pte_offset(mm, address);
@@ -4167,6 +4170,7 @@ pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)
 		return (pte_t *)pmd_alloc(mm, pud, addr);
 
 	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	vma_interval_tree_foreach(svma, &mapping->i_mmap, idx, idx) {
 		if (svma == vma)
 			continue;
diff --git a/mm/ksm.c b/mm/ksm.c
index ca6d2a0..e05edb4 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -37,6 +37,7 @@
 #include <linux/freezer.h>
 #include <linux/oom.h>
 #include <linux/numa.h>
+#include <linux/ldu.h>
 
 #include <asm/tlbflush.h>
 #include "internal.h"
@@ -1899,7 +1900,8 @@ again:
 		struct vm_area_struct *vma;
 
 		cond_resched();
-		anon_vma_lock_read(anon_vma);
+		anon_vma_lock_write(anon_vma);
+		synchronize_ldu_anon(anon_vma);
 		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
 					       0, ULONG_MAX) {
 			cond_resched();
@@ -1922,15 +1924,18 @@ again:
 			ret = rwc->rmap_one(page, vma,
 					rmap_item->address, rwc->arg);
 			if (ret != SWAP_AGAIN) {
-				anon_vma_unlock_read(anon_vma);
+				anon_vma_global_unlock();
+				anon_vma_unlock_write(anon_vma);
 				goto out;
 			}
 			if (rwc->done && rwc->done(page)) {
-				anon_vma_unlock_read(anon_vma);
+				anon_vma_global_unlock();
+				anon_vma_unlock_write(anon_vma);
 				goto out;
 			}
 		}
-		anon_vma_unlock_read(anon_vma);
+		anon_vma_global_unlock();
+		anon_vma_unlock_write(anon_vma);
 	}
 	if (!search_new_forks++)
 		goto again;
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index ac595e7..113d626 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -413,6 +413,7 @@ static void collect_procs_anon(struct page *page, struct list_head *to_kill,
 	av = page_lock_anon_vma_read(page);
 	if (av == NULL)	/* Not actually mapped anymore */
 		return;
+	synchronize_ldu_anon(av);
 
 	pgoff = page_to_pgoff(page);
 	read_lock(&tasklist_lock);
@@ -432,6 +433,7 @@ static void collect_procs_anon(struct page *page, struct list_head *to_kill,
 		}
 	}
 	read_unlock(&tasklist_lock);
+	anon_vma_global_unlock();
 	page_unlock_anon_vma_read(av);
 }
 
@@ -445,8 +447,9 @@ static void collect_procs_file(struct page *page, struct list_head *to_kill,
 	struct task_struct *tsk;
 	struct address_space *mapping = page->mapping;
 
-	i_mmap_lock_read(mapping);
+	i_mmap_lock_write(mapping);
 	read_lock(&tasklist_lock);
+	synchronize_ldu_i_mmap(mapping);
 	for_each_process(tsk) {
 		pgoff_t pgoff = page_to_pgoff(page);
 		struct task_struct *t = task_early_kill(tsk, force_early);
@@ -467,7 +470,7 @@ static void collect_procs_file(struct page *page, struct list_head *to_kill,
 		}
 	}
 	read_unlock(&tasklist_lock);
-	i_mmap_unlock_read(mapping);
+	i_mmap_unlock_write(mapping);
 }
 
 /*
diff --git a/mm/memory.c b/mm/memory.c
index 8132787..142693b 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1288,6 +1288,7 @@ static void unmap_single_vma(struct mmu_gather *tlb,
 			 */
 			if (vma->vm_file) {
 				i_mmap_lock_write(vma->vm_file->f_mapping);
+				synchronize_ldu_i_mmap(vma->vm_file->f_mapping);
 				__unmap_hugepage_range_final(tlb, vma, start, end, NULL);
 				i_mmap_unlock_write(vma->vm_file->f_mapping);
 			}
@@ -2433,6 +2434,7 @@ void unmap_mapping_range(struct address_space *mapping,
 
 	/* DAX uses i_mmap_lock to serialise file truncate vs page fault */
 	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	if (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap)))
 		unmap_mapping_range_tree(&mapping->i_mmap, &details);
 	i_mmap_unlock_write(mapping);
diff --git a/mm/migrate.c b/mm/migrate.c
index 3ad0fea..7afc650 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1684,6 +1684,9 @@ int migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,
 	int nr_remaining;
 	LIST_HEAD(migratepages);
 
+	if (PageAnon(page))
+		goto out;
+
 	/*
 	 * Don't migrate file pages that are mapped in multiple processes
 	 * with execute permissions as they are probably shared libraries.
diff --git a/mm/mlock.c b/mm/mlock.c
index 96f0010..184c655 100644
--- a/mm/mlock.c
+++ b/mm/mlock.c
@@ -128,7 +128,7 @@ static void __munlock_isolated_page(struct page *page)
 	 * Optimization: if the page was mapped just once, that's our mapping
 	 * and we don't need to check all the other vmas.
 	 */
-	if (page_mapcount(page) > 1)
+	if (page_mapcount(page) > 1 && (PageAnon(page) || PageKsm(page)))
 		ret = try_to_munlock(page);
 
 	/* Did try_to_unlock() succeed or punt? */
diff --git a/mm/mmap.c b/mm/mmap.c
index 76d1ec2..32f1366 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -41,6 +41,7 @@
 #include <linux/notifier.h>
 #include <linux/memory.h>
 #include <linux/printk.h>
+#include <linux/kthread.h>
 #include <linux/userfaultfd_k.h>
 #include <linux/moduleparam.h>
 
@@ -77,6 +78,266 @@ static void unmap_region(struct mm_struct *mm,
 		struct vm_area_struct *vma, struct vm_area_struct *prev,
 		unsigned long start, unsigned long end);
 
+#define LDU_UPDATE_RATE 1 /* 1 sec */
+static struct workqueue_struct *i_mmap_wq;
+static struct task_struct *free_vma_task;
+
+#define I_MMAP_HASH_ORDER	11
+#define I_MMAP_HASH_SIZE (1 << I_MMAP_HASH_ORDER)
+
+struct pldu_deferred_i_mmap {
+	struct llist_head list;
+};
+
+struct i_mmap_slot {
+	struct pldu_deferred_i_mmap mapping[1 << I_MMAP_HASH_ORDER];
+};
+
+static DEFINE_PER_CPU(struct i_mmap_slot, i_mmap_slot);
+static DEFINE_PER_CPU(struct llist_head, pldu_vma_clean);
+
+
+void synchronize_ldu_i_mmap_internal(struct llist_node *entry);
+void synchronize_ldu_i_mmap(struct address_space *mapping);
+
+void i_mmap_free_work_func(struct work_struct *work);
+
+bool i_mmap_ldu_logical_update(struct address_space *mapping,
+		struct ldu_node *dnode)
+{
+	struct pldu_deferred_i_mmap *p;
+	struct i_mmap_slot *slot;
+	struct llist_node *first;
+	struct llist_node *entry;
+	struct ldu_node *ldu;
+	struct address_space *locked_mapping = NULL;
+
+	slot = &get_cpu_var(i_mmap_slot);
+	p = &slot->mapping[hash_ptr(mapping, I_MMAP_HASH_ORDER)];
+	first = READ_ONCE(p->list.first);
+	if (first) {
+		ldu = llist_entry(first, struct ldu_node, ll_node);
+		if (READ_ONCE(ldu->root) != READ_ONCE(dnode->root)) {
+			locked_mapping = READ_ONCE(ldu->key2);
+			entry = llist_del_all(&p->list);
+			llist_add(&dnode->ll_node, &p->list);
+			put_cpu_var(i_mmap_slot);
+			down_write(&locked_mapping->i_mmap_rwsem);
+			if (entry) {
+				synchronize_ldu_i_mmap_internal(entry);
+			}
+			up_write(&locked_mapping->i_mmap_rwsem);
+			goto out;
+		}
+	}
+
+	llist_add(&dnode->ll_node, &p->list);
+	put_cpu_var(i_mmap_slot);
+
+out:
+	return true;
+}
+
+bool i_mmap_ldu_logical_insert(struct vm_area_struct *obj,
+		struct address_space *head)
+{
+	struct ldu_node *add_dnode = &obj->dnode.node[0];
+	struct ldu_node *del_dnode = &obj->dnode.node[1];
+
+	/* Phase 1 : update-side removing logs */
+	if (!xchg(&del_dnode->mark, 0)) {
+		BUG_ON(add_dnode->mark);
+		WRITE_ONCE(add_dnode->mark, 1);
+		/* Phase 2 : reusing garbage log */
+		if (!test_and_set_bit(LDU_OP_ADD, &obj->dnode.used)) {
+			add_dnode->op_num = LDU_OP_ADD;
+			add_dnode->key = obj;
+			add_dnode->key2 = head;
+			add_dnode->root = &head->i_mmap;
+			/* Phase 3(slow-path): insert log to queue */
+			i_mmap_ldu_logical_update(head, add_dnode);
+		}
+	}
+
+	return true;
+}
+
+bool i_mmap_ldu_logical_remove(struct vm_area_struct *obj,
+		struct address_space *head)
+{
+	struct ldu_node *add_dnode = &obj->dnode.node[0];
+	struct ldu_node *del_dnode = &obj->dnode.node[1];
+
+	/* Phase 1 : update-side removing logs */
+	if (!xchg(&add_dnode->mark, 0)) {
+		BUG_ON(del_dnode->mark);
+		WRITE_ONCE(del_dnode->mark, 1);
+		/* Phase 2 : reusing garbage log */
+		if (!test_and_set_bit(LDU_OP_DEL, &obj->dnode.used)) {
+			del_dnode->op_num = LDU_OP_DEL;
+			del_dnode->key = obj;
+			del_dnode->key2 = head;
+			del_dnode->root = &head->i_mmap;
+			/* Phase 3(slow-path): insert log to queue */
+			i_mmap_ldu_logical_update(head, del_dnode);
+		}
+	}
+
+	return true;
+}
+
+void i_mmap_ldu_physical_update(int op, struct vm_area_struct *vma,
+		struct rb_root *root)
+{
+	if (!root)
+		return;
+
+	if (op == LDU_OP_ADD)
+		vma_interval_tree_insert(vma, root);
+	else
+		vma_interval_tree_remove(vma, root);
+}
+
+void synchronize_ldu_i_mmap_internal(struct llist_node *entry)
+{
+	struct ldu_node *dnode;
+	struct vm_area_struct *vma;
+
+	/* iteration all logs */
+	llist_for_each_entry(dnode, entry, ll_node) {
+		vma = READ_ONCE(dnode->key);
+		/* atomic swap due to update-side removing */
+		if (xchg(&dnode->mark, 0)) {
+			i_mmap_ldu_physical_update(dnode->op_num, vma,
+					READ_ONCE(dnode->root));
+		}
+		clear_bit(dnode->op_num, &vma->dnode.used);
+		/* once again check due to reusing garbage logs */
+		if (xchg(&dnode->mark, 0)) {
+			i_mmap_ldu_physical_update(dnode->op_num, vma,
+					READ_ONCE(dnode->root));
+		}
+	}
+}
+
+void synchronize_ldu_i_mmap(struct address_space *mapping)
+{
+	int cpu;
+	struct i_mmap_slot *slot;
+	struct pldu_deferred_i_mmap *p;
+	struct llist_node *entry;
+	struct llist_node *first;
+	struct ldu_node *ldu;
+
+	for_each_possible_cpu(cpu) {
+		slot = &per_cpu(i_mmap_slot, cpu);
+		p = &slot->mapping[hash_ptr(mapping, I_MMAP_HASH_ORDER)];
+		first = READ_ONCE(p->list.first);
+		if (first) {
+			ldu = llist_entry(first, struct ldu_node, ll_node);
+			if (ldu && ldu->root != &mapping->i_mmap) {
+				continue;
+			}
+		} else
+			continue;
+
+		entry = llist_del_all(&p->list);
+		if (!entry)
+			continue;
+
+		synchronize_ldu_i_mmap_internal(entry);
+	}
+}
+EXPORT_SYMBOL_GPL(synchronize_ldu_i_mmap);
+
+void free_vma(struct vm_area_struct *vma)
+{
+	struct file *file = vma->vm_file;
+
+	if (file && READ_ONCE(vma->dnode.used)) {
+		struct llist_head *ll = this_cpu_ptr(&pldu_vma_clean);
+		llist_add(&vma->llist, ll);
+		return;
+	}
+	kmem_cache_free(vm_area_cachep, vma);
+}
+
+void clean_percore_mapping(struct address_space *mapping)
+{
+	int cpu;
+	struct i_mmap_slot *slot;
+	struct pldu_deferred_i_mmap *p;
+	struct llist_node *entry;
+	struct llist_node *first;
+	struct ldu_node *ldu;
+
+	for_each_possible_cpu(cpu) {
+		slot = &per_cpu(i_mmap_slot, cpu);
+		p = &slot->mapping[hash_ptr(mapping, I_MMAP_HASH_ORDER)];
+		first = READ_ONCE(p->list.first);
+		if (first) {
+			ldu = llist_entry(first, struct ldu_node, ll_node);
+			if (ldu && ldu->root != &mapping->i_mmap)
+				continue;
+		} else
+			continue;
+		entry = llist_del_all(&p->list);
+		if (!entry)
+			continue;
+
+		synchronize_ldu_i_mmap_internal(entry);
+	}
+}
+
+static struct llist_node *free_entry[NR_CPUS];
+
+static int free_vma_thread(void *dummy)
+{
+	struct vm_area_struct *vnode, *vnext;
+	int cpu, i;
+	struct i_mmap_slot *slot;
+	struct llist_node *entry;
+	struct llist_head *ll;
+	struct address_space *locked_mapping = NULL;
+	struct ldu_node *ldu;
+
+	while (!kthread_should_stop()) {
+		schedule_timeout_interruptible(HZ);
+
+		for_each_possible_cpu(cpu) {
+			ll = &per_cpu(pldu_vma_clean, cpu);
+			free_entry[cpu] = llist_del_all(ll);
+		}
+
+		for_each_possible_cpu(cpu) {
+			for (i = 0; i < I_MMAP_HASH_SIZE; i++) {
+				slot = &per_cpu(i_mmap_slot, cpu);
+				entry = llist_del_all(&slot->mapping[i].list);
+				if (!entry)
+					continue;
+				ldu = llist_entry(entry, struct ldu_node, ll_node);
+				locked_mapping = READ_ONCE(ldu->key2);
+				down_write(&locked_mapping->i_mmap_rwsem);
+				synchronize_ldu_i_mmap_internal(entry);
+				up_write(&locked_mapping->i_mmap_rwsem);
+			}
+		}
+
+		for_each_possible_cpu(cpu) {
+			ll = &per_cpu(pldu_vma_clean, cpu);
+			llist_for_each_entry_safe(vnode, vnext, free_entry[cpu], llist) {
+				if (!READ_ONCE(vnode->dnode.used)) {
+					kmem_cache_free(vm_area_cachep, vnode);
+				} else {
+					ll = this_cpu_ptr(&pldu_vma_clean);
+					llist_add(&vnode->llist, ll);
+				}
+			}
+		}
+	}
+	return 0;
+}
+
 /* description of effects of mapping type and prot in current implementation.
  * this is due to the limited x86 page protection hardware.  The expected
  * behavior is in parens:
@@ -253,14 +514,15 @@ error:
 static void __remove_shared_vm_struct(struct vm_area_struct *vma,
 		struct file *file, struct address_space *mapping)
 {
+	flush_dcache_mmap_lock(mapping);
+	i_mmap_ldu_logical_remove(vma, mapping);
+	//vma_interval_tree_remove(vma, &mapping->i_mmap);
+	flush_dcache_mmap_unlock(mapping);
+
 	if (vma->vm_flags & VM_DENYWRITE)
 		atomic_inc(&file_inode(file)->i_writecount);
 	if (vma->vm_flags & VM_SHARED)
 		mapping_unmap_writable(mapping);
-
-	flush_dcache_mmap_lock(mapping);
-	vma_interval_tree_remove(vma, &mapping->i_mmap);
-	flush_dcache_mmap_unlock(mapping);
 }
 
 /*
@@ -273,9 +535,9 @@ void unlink_file_vma(struct vm_area_struct *vma)
 
 	if (file) {
 		struct address_space *mapping = file->f_mapping;
-		i_mmap_lock_write(mapping);
+		//i_mmap_lock_write(mapping);
 		__remove_shared_vm_struct(vma, file, mapping);
-		i_mmap_unlock_write(mapping);
+		//i_mmap_unlock_write(mapping);
 	}
 }
 
@@ -292,7 +554,7 @@ static struct vm_area_struct *remove_vma(struct vm_area_struct *vma)
 	if (vma->vm_file)
 		fput(vma->vm_file);
 	mpol_put(vma_policy(vma));
-	kmem_cache_free(vm_area_cachep, vma);
+	free_vma(vma);
 	return next;
 }
 
@@ -463,10 +725,11 @@ static void validate_mm(struct mm_struct *mm)
 		struct anon_vma_chain *avc;
 
 		if (anon_vma) {
-			anon_vma_lock_read(anon_vma);
+			anon_vma_lock_write(anon_vma);
+			synchronize_ldu_anon(anon_vma);
 			list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
 				anon_vma_interval_tree_verify(avc);
-			anon_vma_unlock_read(anon_vma);
+			anon_vma_unlock_write(anon_vma);
 		}
 
 		highest_address = vma->vm_end;
@@ -537,6 +800,10 @@ static void vma_rb_erase(struct vm_area_struct *vma, struct rb_root *root)
 	rb_erase_augmented(&vma->vm_rb, root, &vma_gap_callbacks);
 }
 
+
+#include <linux/ldu.h>
+bool anon_vma_ldu_logical_insert(struct anon_vma_chain *avc, struct anon_vma *anon);
+bool anon_vma_ldu_logical_remove(struct anon_vma_chain *avc, struct anon_vma *anon);
 /*
  * vma has some anon_vma assigned, and is already inserted on that
  * anon_vma's interval trees.
@@ -557,7 +824,8 @@ anon_vma_interval_tree_pre_update_vma(struct vm_area_struct *vma)
 	struct anon_vma_chain *avc;
 
 	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
-		anon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);
+		anon_vma_ldu_logical_remove(avc, avc->anon_vma);
+//		anon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);
 }
 
 static inline void
@@ -566,7 +834,8 @@ anon_vma_interval_tree_post_update_vma(struct vm_area_struct *vma)
 	struct anon_vma_chain *avc;
 
 	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
-		anon_vma_interval_tree_insert(avc, &avc->anon_vma->rb_root);
+		anon_vma_ldu_logical_insert(avc, avc->anon_vma);
+//		anon_vma_interval_tree_insert(avc, &avc->anon_vma->rb_root);
 }
 
 static int find_vma_links(struct mm_struct *mm, unsigned long addr,
@@ -663,14 +932,17 @@ static void __vma_link_file(struct vm_area_struct *vma)
 	if (file) {
 		struct address_space *mapping = file->f_mapping;
 
+		flush_dcache_mmap_lock(mapping);
+		i_mmap_ldu_logical_insert(vma, mapping);
+		//pr_info("mapping : %lx\n", (long)mapping);
+		//vma_interval_tree_insert(vma, &mapping->i_mmap);
+		flush_dcache_mmap_unlock(mapping);
+
 		if (vma->vm_flags & VM_DENYWRITE)
 			atomic_dec(&file_inode(file)->i_writecount);
 		if (vma->vm_flags & VM_SHARED)
 			atomic_inc(&mapping->i_mmap_writable);
 
-		flush_dcache_mmap_lock(mapping);
-		vma_interval_tree_insert(vma, &mapping->i_mmap);
-		flush_dcache_mmap_unlock(mapping);
 	}
 }
 
@@ -689,16 +961,13 @@ static void vma_link(struct mm_struct *mm, struct vm_area_struct *vma,
 {
 	struct address_space *mapping = NULL;
 
-	if (vma->vm_file) {
+	if (vma->vm_file)
 		mapping = vma->vm_file->f_mapping;
-		i_mmap_lock_write(mapping);
-	}
 
 	__vma_link(mm, vma, prev, rb_link, rb_parent);
-	__vma_link_file(vma);
 
 	if (mapping)
-		i_mmap_unlock_write(mapping);
+		__vma_link_file(vma);
 
 	mm->map_count++;
 	validate_mm(mm);
@@ -735,6 +1004,7 @@ __vma_unlink(struct mm_struct *mm, struct vm_area_struct *vma,
 	vmacache_invalidate(mm);
 }
 
+
 /*
  * We cannot adjust vm_start, vm_end, vm_pgoff fields of a vma that
  * is already present in an i_mmap tree without adjusting the tree.
@@ -810,7 +1080,6 @@ again:			remove_next = 1 + (end > next->vm_end);
 		if (adjust_next)
 			uprobe_munmap(next, next->vm_start, next->vm_end);
 
-		i_mmap_lock_write(mapping);
 		if (insert) {
 			/*
 			 * Put into interval tree now, so instantiated pages
@@ -830,17 +1099,12 @@ again:			remove_next = 1 + (end > next->vm_end);
 	if (anon_vma) {
 		VM_BUG_ON_VMA(adjust_next && next->anon_vma &&
 			  anon_vma != next->anon_vma, next);
-		anon_vma_lock_write(anon_vma);
+		//anon_vma_lock_write(anon_vma);
+#if 1
 		anon_vma_interval_tree_pre_update_vma(vma);
 		if (adjust_next)
 			anon_vma_interval_tree_pre_update_vma(next);
-	}
-
-	if (root) {
-		flush_dcache_mmap_lock(mapping);
-		vma_interval_tree_remove(vma, root);
-		if (adjust_next)
-			vma_interval_tree_remove(next, root);
+#endif
 	}
 
 	if (start != vma->vm_start) {
@@ -857,13 +1121,6 @@ again:			remove_next = 1 + (end > next->vm_end);
 		next->vm_pgoff += adjust_next;
 	}
 
-	if (root) {
-		if (adjust_next)
-			vma_interval_tree_insert(next, root);
-		vma_interval_tree_insert(vma, root);
-		flush_dcache_mmap_unlock(mapping);
-	}
-
 	if (remove_next) {
 		/*
 		 * vma_merge has merged next into vma, and needs
@@ -891,13 +1148,13 @@ again:			remove_next = 1 + (end > next->vm_end);
 	}
 
 	if (anon_vma) {
+#if 1
 		anon_vma_interval_tree_post_update_vma(vma);
 		if (adjust_next)
 			anon_vma_interval_tree_post_update_vma(next);
-		anon_vma_unlock_write(anon_vma);
+#endif
+		//anon_vma_unlock_write(anon_vma);
 	}
-	if (mapping)
-		i_mmap_unlock_write(mapping);
 
 	if (root) {
 		uprobe_mmap(vma);
@@ -915,7 +1172,7 @@ again:			remove_next = 1 + (end > next->vm_end);
 			anon_vma_merge(vma, next);
 		mm->map_count--;
 		mpol_put(vma_policy(next));
-		kmem_cache_free(vm_area_cachep, next);
+		free_vma(next);
 		/*
 		 * In mprotect's case 6 (see comments on vma_merge),
 		 * we must remove another next too. It would clutter
@@ -1605,6 +1862,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 	vma->vm_page_prot = vm_get_page_prot(vm_flags);
 	vma->vm_pgoff = pgoff;
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	memset(&vma->dnode, 0, sizeof(vma->dnode));
 
 	if (file) {
 		if (vm_flags & VM_DENYWRITE) {
@@ -1695,7 +1953,7 @@ allow_write_and_free_vma:
 	if (vm_flags & VM_DENYWRITE)
 		allow_write_access(file);
 free_vma:
-	kmem_cache_free(vm_area_cachep, vma);
+	free_vma(vma);
 unacct_error:
 	if (charged)
 		vm_unacct_memory(charged);
@@ -2238,9 +2496,10 @@ int expand_downwards(struct vm_area_struct *vma,
 	/*
 	 * vma->vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
-	 * anon_vma lock to serialize against concurrent expand_stacks.
+	 * page_table_lock lock to serialize against concurrent expand_stacks.
 	 */
-	anon_vma_lock_write(vma->anon_vma);
+	//anon_vma_lock_write(vma->anon_vma);
+	spin_lock(&mm->page_table_lock);
 
 	/* Somebody else might have raced and expanded it already */
 	if (address < vma->vm_start) {
@@ -2253,18 +2512,6 @@ int expand_downwards(struct vm_area_struct *vma,
 		if (grow <= vma->vm_pgoff) {
 			error = acct_stack_growth(vma, size, grow);
 			if (!error) {
-				/*
-				 * vma_gap_update() doesn't support concurrent
-				 * updates, but we only hold a shared mmap_sem
-				 * lock here, so we need to protect against
-				 * concurrent vma expansions.
-				 * anon_vma_lock_write() doesn't help here, as
-				 * we don't guarantee that all growable vmas
-				 * in a mm share the same root anon vma.
-				 * So, we reuse mm->page_table_lock to guard
-				 * against concurrent vma expansions.
-				 */
-				spin_lock(&mm->page_table_lock);
 				if (vma->vm_flags & VM_LOCKED)
 					mm->locked_vm += grow;
 				vm_stat_account(mm, vma->vm_flags, grow);
@@ -2273,13 +2520,13 @@ int expand_downwards(struct vm_area_struct *vma,
 				vma->vm_pgoff -= grow;
 				anon_vma_interval_tree_post_update_vma(vma);
 				vma_gap_update(vma);
-				spin_unlock(&mm->page_table_lock);
 
 				perf_event_mmap(vma);
 			}
 		}
 	}
-	anon_vma_unlock_write(vma->anon_vma);
+	spin_unlock(&mm->page_table_lock);
+	//anon_vma_unlock_write(vma->anon_vma);
 	khugepaged_enter_vma_merge(vma, vma->vm_flags);
 	validate_mm(mm);
 	return error;
@@ -2454,7 +2701,7 @@ static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 					~(huge_page_mask(hstate_vma(vma)))))
 		return -EINVAL;
 
-	new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+	new = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 	if (!new)
 		return -ENOMEM;
 
@@ -2462,6 +2709,7 @@ static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 	*new = *vma;
 
 	INIT_LIST_HEAD(&new->anon_vma_chain);
+	memset(&new->dnode, 0, sizeof(new->dnode));
 
 	if (new_below)
 		new->vm_end = addr;
@@ -2503,7 +2751,7 @@ static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
  out_free_mpol:
 	mpol_put(vma_policy(new));
  out_free_vma:
-	kmem_cache_free(vm_area_cachep, new);
+	free_vma(new);
 	return err;
 }
 
@@ -2800,6 +3048,7 @@ static unsigned long do_brk(unsigned long addr, unsigned long len)
 	}
 
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	memset(&vma->dnode, 0, sizeof(vma->dnode));
 	vma->vm_mm = mm;
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
@@ -2969,7 +3218,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 		}
 		*need_rmap_locks = (new_vma->vm_pgoff <= vma->vm_pgoff);
 	} else {
-		new_vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+		new_vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 		if (!new_vma)
 			goto out;
 		*new_vma = *vma;
@@ -2979,6 +3228,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 		if (vma_dup_policy(vma, new_vma))
 			goto out_free_vma;
 		INIT_LIST_HEAD(&new_vma->anon_vma_chain);
+		memset(&new_vma->dnode, 0, sizeof(new_vma->dnode));
 		if (anon_vma_clone(new_vma, vma))
 			goto out_free_mempol;
 		if (new_vma->vm_file)
@@ -2993,7 +3243,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 out_free_mempol:
 	mpol_put(vma_policy(new_vma));
 out_free_vma:
-	kmem_cache_free(vm_area_cachep, new_vma);
+	free_vma(new_vma);
 out:
 	return NULL;
 }
@@ -3099,6 +3349,7 @@ static struct vm_area_struct *__install_special_mapping(
 		return ERR_PTR(-ENOMEM);
 
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	memset(&vma->dnode, 0, sizeof(vma->dnode));
 	vma->vm_mm = mm;
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
@@ -3120,7 +3371,7 @@ static struct vm_area_struct *__install_special_mapping(
 	return vma;
 
 out:
-	kmem_cache_free(vm_area_cachep, vma);
+	free_vma(vma);
 	return ERR_PTR(ret);
 }
 
@@ -3265,7 +3516,7 @@ int mm_take_all_locks(struct mm_struct *mm)
 			list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
 				vm_lock_anon_vma(mm, avc->anon_vma);
 	}
-
+	anon_vma_global_lock();
 	return 0;
 
 out_unlock:
@@ -3321,6 +3572,7 @@ void mm_drop_all_locks(struct mm_struct *mm)
 	BUG_ON(down_read_trylock(&mm->mmap_sem));
 	BUG_ON(!mutex_is_locked(&mm_all_locks_mutex));
 
+	anon_vma_global_unlock();
 	for (vma = mm->mmap; vma; vma = vma->vm_next) {
 		if (vma->anon_vma)
 			list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
@@ -3454,3 +3706,36 @@ static int __meminit init_reserve_notifier(void)
 	return 0;
 }
 subsys_initcall(init_reserve_notifier);
+
+static int __init i_mmap_init_wq(void)
+{
+	int i, j;
+
+	i_mmap_wq = create_singlethread_workqueue("i_mmap");
+	WARN(!i_mmap_wq, "failed to create i_mmap workqueue\n");
+
+	pr_info("i_mmap work queue initialize");
+
+	free_vma_task = kthread_create(free_vma_thread, NULL, "mm_free_vma");
+	BUG_ON(IS_ERR(free_vma_task));
+	wake_up_process(free_vma_task);
+
+	for_each_possible_cpu(i) {
+		struct i_mmap_slot *slot;
+		struct llist_head *ll;
+
+		slot = &per_cpu(i_mmap_slot, i);
+
+		for (j = 0; j < I_MMAP_HASH_SIZE; j++) {
+			struct pldu_deferred_i_mmap *pldu = &slot->mapping[j];
+			init_llist_head(&pldu->list);
+		}
+
+		ll = &per_cpu(pldu_vma_clean, i);
+		init_llist_head(ll);
+
+	}
+
+	return 0;
+}
+__initcall(i_mmap_init_wq);
diff --git a/mm/mremap.c b/mm/mremap.c
index 8eeba02..b024d6c 100644
--- a/mm/mremap.c
+++ b/mm/mremap.c
@@ -119,11 +119,15 @@ static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,
 		if (vma->vm_file) {
 			mapping = vma->vm_file->f_mapping;
 			i_mmap_lock_write(mapping);
+			pr_info("move pte i_mmap\n");
 		}
 		if (vma->anon_vma) {
 			anon_vma = vma->anon_vma;
 			anon_vma_lock_write(anon_vma);
+			synchronize_ldu_anon(anon_vma);
+			pr_info("move pte anon_vma\n");
 		}
+
 	}
 
 	/*
@@ -152,8 +156,10 @@ static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,
 		spin_unlock(new_ptl);
 	pte_unmap(new_pte - 1);
 	pte_unmap_unlock(old_pte - 1, old_ptl);
-	if (anon_vma)
+	if (anon_vma) {
+		anon_vma_global_unlock();
 		anon_vma_unlock_write(anon_vma);
+	}
 	if (mapping)
 		i_mmap_unlock_write(mapping);
 }
@@ -197,13 +203,18 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
 				VM_BUG_ON_VMA(vma->vm_file || !vma->anon_vma,
 					      vma);
 				/* See comment in move_ptes() */
-				if (need_rmap_locks)
+				if (need_rmap_locks) {
 					anon_vma_lock_write(vma->anon_vma);
+					synchronize_ldu_anon(vma->anon_vma);
+					pr_info("move pte move_huge_pmd anon_vma\n");
+				}
 				moved = move_huge_pmd(vma, new_vma, old_addr,
 						    new_addr, old_end,
 						    old_pmd, new_pmd);
-				if (need_rmap_locks)
+				if (need_rmap_locks) {
+					anon_vma_global_unlock();
 					anon_vma_unlock_write(vma->anon_vma);
+				}
 				if (moved) {
 					need_flush = true;
 					continue;
diff --git a/mm/rmap.c b/mm/rmap.c
index 79f3bf0..e98dfaf 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -61,6 +61,8 @@
 #include <linux/hugetlb.h>
 #include <linux/backing-dev.h>
 #include <linux/page_idle.h>
+#include <linux/ldu.h>
+#include <linux/kthread.h>
 
 #include <asm/tlbflush.h>
 
@@ -71,6 +73,13 @@
 static struct kmem_cache *anon_vma_cachep;
 static struct kmem_cache *anon_vma_chain_cachep;
 
+static struct task_struct *free_avc_task;
+static struct workqueue_struct *avc_wq;
+
+static DEFINE_PER_CPU(struct pldu_deferred, pldu_anon_vma_deferred);
+static DEFINE_PER_CPU(struct llist_head, pldu_avc_clean);
+static DEFINE_MUTEX(anon_vma_mutex);
+
 static inline struct anon_vma *anon_vma_alloc(void)
 {
 	struct anon_vma *anon_vma;
@@ -78,8 +87,9 @@ static inline struct anon_vma *anon_vma_alloc(void)
 	anon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);
 	if (anon_vma) {
 		atomic_set(&anon_vma->refcount, 1);
-		anon_vma->degree = 1;	/* Reference for first vma */
-		anon_vma->parent = anon_vma;
+		atomic_set(&anon_vma->refcount_free, 0);
+
+		anon_vma->parent = 0;
 		/*
 		 * Initialise the anon_vma root to point to itself. If called
 		 * from fork, the root will be reset to the parents anon_vma.
@@ -112,22 +122,209 @@ static inline void anon_vma_free(struct anon_vma *anon_vma)
 	 * happen _before_ what follows.
 	 */
 	might_sleep();
+
 	if (rwsem_is_locked(&anon_vma->root->rwsem)) {
 		anon_vma_lock_write(anon_vma);
 		anon_vma_unlock_write(anon_vma);
 	}
-
 	kmem_cache_free(anon_vma_cachep, anon_vma);
 }
 
 static inline struct anon_vma_chain *anon_vma_chain_alloc(gfp_t gfp)
 {
-	return kmem_cache_alloc(anon_vma_chain_cachep, gfp);
+	return kmem_cache_zalloc(anon_vma_chain_cachep, gfp);
+}
+
+void anon_vma_global_lock(void)
+{
+	mutex_lock(&anon_vma_mutex);
+}
+
+void anon_vma_global_unlock(void)
+{
+	mutex_unlock(&anon_vma_mutex);
 }
 
 static void anon_vma_chain_free(struct anon_vma_chain *anon_vma_chain)
 {
+	struct anon_vma *anon_vma = anon_vma_chain->anon_vma;
+
+	if (READ_ONCE(anon_vma_chain->dnode.used)) {
+		struct llist_head *ll = this_cpu_ptr(&pldu_avc_clean);
+		llist_add(&anon_vma_chain->llist, ll);
+		return;
+	}
+
 	kmem_cache_free(anon_vma_chain_cachep, anon_vma_chain);
+	if (anon_vma)
+		atomic_dec(&anon_vma->refcount_free);
+}
+
+bool anon_vma_ldu_logical_update(struct anon_vma *anon, struct ldu_node *dnode)
+{
+	struct pldu_deferred *p = this_cpu_ptr(&pldu_anon_vma_deferred);
+
+	llist_add(&dnode->ll_node, &p->list);
+
+	return true;
+}
+
+bool anon_vma_ldu_logical_insert(struct anon_vma_chain *obj, struct anon_vma *head)
+{
+	struct ldu_node *add_dnode = &obj->dnode.node[0];
+	struct ldu_node *del_dnode = &obj->dnode.node[1];
+
+	if (!head && !head->root)
+		return false;
+	/* Phase 1 : update-side removing logs */
+	if (!xchg(&del_dnode->mark, 0)) {
+		BUG_ON(add_dnode->mark);
+		WRITE_ONCE(add_dnode->mark, 1);
+		/* Phase 2 : reusing garbage log */
+		if (!test_and_set_bit(LDU_OP_ADD, &obj->dnode.used)) {
+			add_dnode->op_num = LDU_OP_ADD;
+			add_dnode->key = obj;
+			add_dnode->key2= head->root;
+			add_dnode->root = &head->rb_root;
+			/* Phase 3(slow-path): insert log to queue */
+			anon_vma_ldu_logical_update(head, add_dnode);
+		}
+	}
+
+	return true;
+}
+
+bool anon_vma_ldu_logical_remove(struct anon_vma_chain *obj, struct anon_vma *head)
+{
+	struct ldu_node *add_dnode = &obj->dnode.node[0];
+	struct ldu_node *del_dnode = &obj->dnode.node[1];
+
+	if (!head && !head->root)
+		return false;
+
+	/* Phase 1 : update-side removing logs */
+	if (!xchg(&add_dnode->mark, 0)) {
+		BUG_ON(del_dnode->mark);
+		WRITE_ONCE(del_dnode->mark, 1);
+		/* Phase 2 : reusing garbage log */
+		if (!test_and_set_bit(LDU_OP_DEL, &obj->dnode.used)) {
+			del_dnode->op_num = LDU_OP_DEL;
+			del_dnode->key = obj;
+			del_dnode->key2= head->root;
+			del_dnode->root = &head->rb_root;
+			/* Phase 3(slow-path): insert log to queue */
+			anon_vma_ldu_logical_update(head, del_dnode);
+		}
+	}
+
+	return true;
+}
+
+void anon_vma_ldu_physical_update(int op, struct anon_vma_chain *avc,
+		struct rb_root *root)
+{
+	BUG_ON(!root);
+	BUG_ON(!avc);
+	if (op == LDU_OP_ADD)
+		anon_vma_interval_tree_insert(avc, root);
+	else
+		anon_vma_interval_tree_remove(avc, root);
+}
+
+void synchronize_ldu_anon_internal(struct llist_head *head)
+{
+	struct llist_node *entry;
+	struct ldu_node *dnode;
+
+	entry = llist_del_all(head);
+	/* iteration all logs */
+	llist_for_each_entry(dnode, entry, ll_node) {
+		struct anon_vma_chain *avc = READ_ONCE(dnode->key);
+		/* atomic swap due to update-side removing */
+		if (xchg(&dnode->mark, 0)) {
+			anon_vma_ldu_physical_update(dnode->op_num, avc,
+					READ_ONCE(dnode->root));
+		}
+		clear_bit(dnode->op_num, &avc->dnode.used);
+		/* once again check due to reusing garbage logs */
+		if (xchg(&dnode->mark, 0)) {
+			anon_vma_ldu_physical_update(dnode->op_num, avc,
+					READ_ONCE(dnode->root));
+		}
+	}
+}
+
+void synchronize_ldu_anon(struct anon_vma *anon_vma)
+{
+	int cpu;
+
+	anon_vma_global_lock();
+	for_each_possible_cpu(cpu) {
+		struct pldu_deferred *pd;
+		pd = &per_cpu(pldu_anon_vma_deferred, cpu);
+		synchronize_ldu_anon_internal(&pd->list);
+	}
+
+	return;
+}
+EXPORT_SYMBOL_GPL(synchronize_ldu_anon);
+
+
+void synchronize_ldu_anon_no_lock(struct anon_vma *anon_vma)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		struct pldu_deferred *pd;
+		pd = &per_cpu(pldu_anon_vma_deferred, cpu);
+		synchronize_ldu_anon_internal(&pd->list);
+	}
+
+	return;
+}
+EXPORT_SYMBOL_GPL(synchronize_ldu_anon_no_lock);
+
+
+
+static struct llist_node *free_entry[NR_CPUS];
+
+static int free_avc_thread(void *dummy)
+{
+	struct anon_vma_chain *anode, *anext;
+	int cpu;
+	struct llist_head *ll;
+
+	while (!kthread_should_stop()) {
+		schedule_timeout_interruptible(HZ);
+		for_each_possible_cpu(cpu) {
+			ll = &per_cpu(pldu_avc_clean, cpu);
+			free_entry[cpu] = llist_del_all(ll);
+		}
+
+		synchronize_ldu_anon(NULL);
+		anon_vma_global_unlock();
+
+		for_each_possible_cpu(cpu) {
+			ll = &per_cpu(pldu_avc_clean, cpu);
+			llist_for_each_entry_safe(anode, anext, free_entry[cpu], llist) {
+				struct anon_vma *anon = anode->anon_vma;
+				if (!READ_ONCE(anode->dnode.used)) {
+					kmem_cache_free(anon_vma_chain_cachep, anode);
+					if (atomic_dec_and_test(&anon->refcount_free) &&
+							RB_EMPTY_ROOT(&anon->rb_root)) {
+						struct anon_vma *root = anon->root;
+						kmem_cache_free(anon_vma_cachep, anon);
+						if (root != anon && atomic_dec_and_test(&root->refcount))
+							kmem_cache_free(anon_vma_cachep, root);
+					}
+				} else {
+					ll = this_cpu_ptr(&pldu_avc_clean);
+					llist_add(&anode->llist, ll);
+				}
+			}
+		}
+	}
+	return 0;
 }
 
 static void anon_vma_chain_link(struct vm_area_struct *vma,
@@ -136,8 +333,10 @@ static void anon_vma_chain_link(struct vm_area_struct *vma,
 {
 	avc->vma = vma;
 	avc->anon_vma = anon_vma;
+	atomic_inc(&anon_vma->refcount_free);
 	list_add(&avc->same_vma, &vma->anon_vma_chain);
-	anon_vma_interval_tree_insert(avc, &anon_vma->rb_root);
+	//anon_vma_interval_tree_insert(avc, &anon_vma->rb_root);
+	anon_vma_ldu_logical_insert(avc, anon_vma);
 }
 
 /**
@@ -190,19 +389,18 @@ int anon_vma_prepare(struct vm_area_struct *vma)
 			allocated = anon_vma;
 		}
 
-		anon_vma_lock_write(anon_vma);
+		//anon_vma_lock_write(anon_vma);
 		/* page_table_lock to protect against threads */
 		spin_lock(&mm->page_table_lock);
 		if (likely(!vma->anon_vma)) {
 			vma->anon_vma = anon_vma;
 			anon_vma_chain_link(vma, avc, anon_vma);
 			/* vma reference or self-parent link for new root */
-			anon_vma->degree++;
 			allocated = NULL;
 			avc = NULL;
 		}
 		spin_unlock(&mm->page_table_lock);
-		anon_vma_unlock_write(anon_vma);
+		//anon_vma_unlock_write(anon_vma);
 
 		if (unlikely(allocated))
 			put_anon_vma(allocated);
@@ -265,31 +463,19 @@ int anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)
 
 		avc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);
 		if (unlikely(!avc)) {
-			unlock_anon_vma_root(root);
+			//unlock_anon_vma_root(root);
 			root = NULL;
 			avc = anon_vma_chain_alloc(GFP_KERNEL);
 			if (!avc)
 				goto enomem_failure;
 		}
 		anon_vma = pavc->anon_vma;
-		root = lock_anon_vma_root(root, anon_vma);
+		//root = lock_anon_vma_root(root, anon_vma);
 		anon_vma_chain_link(dst, avc, anon_vma);
 
-		/*
-		 * Reuse existing anon_vma if its degree lower than two,
-		 * that means it has no vma and only one anon_vma child.
-		 *
-		 * Do not chose parent anon_vma, otherwise first child
-		 * will always reuse it. Root anon_vma is never reused:
-		 * it has self-parent reference and at least one child.
-		 */
-		if (!dst->anon_vma && anon_vma != src->anon_vma &&
-				anon_vma->degree < 2)
-			dst->anon_vma = anon_vma;
 	}
-	if (dst->anon_vma)
-		dst->anon_vma->degree++;
-	unlock_anon_vma_root(root);
+
+	//unlock_anon_vma_root(root);
 	return 0;
 
  enomem_failure:
@@ -347,19 +533,22 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 	 * lock any of the anon_vmas in this anon_vma tree.
 	 */
 	anon_vma->root = pvma->anon_vma->root;
-	anon_vma->parent = pvma->anon_vma;
+	//anon_vma->parent = pvma->anon_vma;
+
 	/*
 	 * With refcounts, an anon_vma can stay around longer than the
 	 * process it belongs to. The root anon_vma needs to be pinned until
 	 * this anon_vma is freed, because the lock lives in the root.
 	 */
 	get_anon_vma(anon_vma->root);
+	//pr_info("get_anon_vma\n");
+	//pr_info("anon_vma root ref count %d\n", atomic_read(&anon_vma->root->refcount));
+
 	/* Mark this anon_vma as the one where our new (COWed) pages go. */
 	vma->anon_vma = anon_vma;
-	anon_vma_lock_write(anon_vma);
+	//anon_vma_lock_write(anon_vma);
 	anon_vma_chain_link(vma, avc, anon_vma);
-	anon_vma->parent->degree++;
-	anon_vma_unlock_write(anon_vma);
+	//anon_vma_unlock_write(anon_vma);
 
 	return 0;
 
@@ -373,7 +562,6 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 void unlink_anon_vmas(struct vm_area_struct *vma)
 {
 	struct anon_vma_chain *avc, *next;
-	struct anon_vma *root = NULL;
 
 	/*
 	 * Unlink each anon_vma chained to the VMA.  This list is ordered
@@ -382,39 +570,13 @@ void unlink_anon_vmas(struct vm_area_struct *vma)
 	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
 		struct anon_vma *anon_vma = avc->anon_vma;
 
-		root = lock_anon_vma_root(root, anon_vma);
-		anon_vma_interval_tree_remove(avc, &anon_vma->rb_root);
-
-		/*
-		 * Leave empty anon_vmas on the list - we'll need
-		 * to free them outside the lock.
-		 */
-		if (RB_EMPTY_ROOT(&anon_vma->rb_root)) {
-			anon_vma->parent->degree--;
-			continue;
-		}
-
-		list_del(&avc->same_vma);
-		anon_vma_chain_free(avc);
-	}
-	if (vma->anon_vma)
-		vma->anon_vma->degree--;
-	unlock_anon_vma_root(root);
-
-	/*
-	 * Iterate the list once more, it now only contains empty and unlinked
-	 * anon_vmas, destroy them. Could not do before due to __put_anon_vma()
-	 * needing to write-acquire the anon_vma->root->rwsem.
-	 */
-	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
-		struct anon_vma *anon_vma = avc->anon_vma;
-
-		BUG_ON(anon_vma->degree);
-		put_anon_vma(anon_vma);
-
+		//root = lock_anon_vma_root(root, anon_vma);
+		//anon_vma_interval_tree_remove(avc, &anon_vma->rb_root);
+		anon_vma_ldu_logical_remove(avc, anon_vma);
 		list_del(&avc->same_vma);
 		anon_vma_chain_free(avc);
 	}
+	//unlock_anon_vma_root(root);
 }
 
 static void anon_vma_ctor(void *data)
@@ -423,6 +585,8 @@ static void anon_vma_ctor(void *data)
 
 	init_rwsem(&anon_vma->rwsem);
 	atomic_set(&anon_vma->refcount, 0);
+	atomic_set(&anon_vma->refcount_free, 0);
+	anon_vma->parent = 0;
 	anon_vma->rb_root = RB_ROOT;
 }
 
@@ -491,6 +655,7 @@ struct anon_vma *page_get_anon_vma(struct page *page)
 out:
 	rcu_read_unlock();
 
+
 	return anon_vma;
 }
 
@@ -516,14 +681,14 @@ struct anon_vma *page_lock_anon_vma_read(struct page *page)
 
 	anon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);
 	root_anon_vma = READ_ONCE(anon_vma->root);
-	if (down_read_trylock(&root_anon_vma->rwsem)) {
+	if (down_write_trylock(&root_anon_vma->rwsem)) {
 		/*
 		 * If the page is still mapped, then this anon_vma is still
 		 * its anon_vma, and holding the mutex ensures that it will
 		 * not go away, see anon_vma_free().
 		 */
 		if (!page_mapped(page)) {
-			up_read(&root_anon_vma->rwsem);
+			up_write(&root_anon_vma->rwsem);
 			anon_vma = NULL;
 		}
 		goto out;
@@ -543,7 +708,8 @@ struct anon_vma *page_lock_anon_vma_read(struct page *page)
 
 	/* we pinned the anon_vma, its safe to sleep */
 	rcu_read_unlock();
-	anon_vma_lock_read(anon_vma);
+	anon_vma_lock_write(anon_vma);
+
 
 	if (atomic_dec_and_test(&anon_vma->refcount)) {
 		/*
@@ -551,7 +717,7 @@ struct anon_vma *page_lock_anon_vma_read(struct page *page)
 		 * and bail -- can't simply use put_anon_vma() because
 		 * we'll deadlock on the anon_vma_lock_write() recursion.
 		 */
-		anon_vma_unlock_read(anon_vma);
+		anon_vma_unlock_write(anon_vma);
 		__put_anon_vma(anon_vma);
 		anon_vma = NULL;
 	}
@@ -565,7 +731,7 @@ out:
 
 void page_unlock_anon_vma_read(struct anon_vma *anon_vma)
 {
-	anon_vma_unlock_read(anon_vma);
+	anon_vma_unlock_write(anon_vma);
 }
 
 #ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
@@ -1679,8 +1845,9 @@ void __put_anon_vma(struct anon_vma *anon_vma)
 	struct anon_vma *root = anon_vma->root;
 
 	anon_vma_free(anon_vma);
-	if (root != anon_vma && atomic_dec_and_test(&root->refcount))
+	if (root != anon_vma && atomic_dec_and_test(&root->refcount)) {
 		anon_vma_free(root);
+	}
 }
 
 static struct anon_vma *rmap_walk_anon_lock(struct page *page,
@@ -1701,7 +1868,7 @@ static struct anon_vma *rmap_walk_anon_lock(struct page *page,
 	if (!anon_vma)
 		return NULL;
 
-	anon_vma_lock_read(anon_vma);
+	anon_vma_lock_write(anon_vma);
 	return anon_vma;
 }
 
@@ -1727,10 +1894,12 @@ static int rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc)
 	int ret = SWAP_AGAIN;
 
 	anon_vma = rmap_walk_anon_lock(page, rwc);
-	if (!anon_vma)
+	if (!anon_vma) {
 		return ret;
+	}
 
 	pgoff = page_to_pgoff(page);
+	synchronize_ldu_anon(anon_vma);
 	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
 		struct vm_area_struct *vma = avc->vma;
 		unsigned long address = vma_address(page, vma);
@@ -1746,7 +1915,8 @@ static int rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc)
 		if (rwc->done && rwc->done(page))
 			break;
 	}
-	anon_vma_unlock_read(anon_vma);
+	anon_vma_global_unlock();
+	anon_vma_unlock_write(anon_vma);
 	return ret;
 }
 
@@ -1781,8 +1951,9 @@ static int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc)
 	if (!mapping)
 		return ret;
 
+	i_mmap_lock_write(mapping);
+	synchronize_ldu_i_mmap(mapping);
 	pgoff = page_to_pgoff(page);
-	i_mmap_lock_read(mapping);
 	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
 		unsigned long address = vma_address(page, vma);
 
@@ -1799,7 +1970,7 @@ static int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc)
 	}
 
 done:
-	i_mmap_unlock_read(mapping);
+	i_mmap_unlock_write(mapping);
 	return ret;
 }
 
@@ -1834,6 +2005,8 @@ static void __hugepage_set_anon_rmap(struct page *page,
 	anon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;
 	page->mapping = (struct address_space *) anon_vma;
 	page->index = linear_page_index(vma, address);
+
+
 }
 
 void hugepage_add_anon_rmap(struct page *page,
@@ -1846,8 +2019,10 @@ void hugepage_add_anon_rmap(struct page *page,
 	BUG_ON(!anon_vma);
 	/* address might be in next vma when migration races vma_adjust */
 	first = atomic_inc_and_test(compound_mapcount_ptr(page));
-	if (first)
+	if (first) {
+		synchronize_ldu_anon_no_lock(NULL);
 		__hugepage_set_anon_rmap(page, vma, address, 0);
+	}
 }
 
 void hugepage_add_new_anon_rmap(struct page *page,
@@ -1858,3 +2033,31 @@ void hugepage_add_new_anon_rmap(struct page *page,
 	__hugepage_set_anon_rmap(page, vma, address, 1);
 }
 #endif /* CONFIG_HUGETLB_PAGE */
+
+static int __init avc_init_wq(void)
+{
+	int i;
+
+	avc_wq = create_singlethread_workqueue("avc");
+	WARN(!avc_wq, "failed to create avc workqueue\n");
+	pr_info("avc work queue initialize");
+
+	for_each_possible_cpu(i) {
+		struct pldu_deferred *p;
+		struct llist_head *ll;
+
+		p = &per_cpu(pldu_anon_vma_deferred, i);
+		init_llist_head(&p->list);
+
+		ll = &per_cpu(pldu_avc_clean, i);
+		init_llist_head(ll);
+	}
+
+	free_avc_task = kthread_create(free_avc_thread, NULL, "mm_free_avc");
+	BUG_ON(IS_ERR(free_avc_task));
+	wake_up_process(free_avc_task);
+	pr_info("avc work queue initialize");
+
+	return 0;
+}
+__initcall(avc_init_wq);
-- 
2.7.4

